\documentclass[11pt]{article}
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,color,array,eurosym,multirow}
\usepackage[sort&compress]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.3}
\newcommand{\R}{\mathbb{R}}

\definecolor{darkblue}{rgb}{0,0,.8}
\definecolor{darkgreen}{rgb}{0,0.5,0.1}
\newcommand{\matlabfunc}[1]{\textcolor{darkblue}{#1}}
\newcommand{\matlabcomment}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\christian}[1]{\textcolor{blue}{\textbf{CH}: #1}}
\newcommand{\alex}[1]{\textcolor{red}{\textbf{AL}: #1}}

%% Adding shortcut commands to refer to our figures %%
\newcommand{\FigEvoProc}{{\bf Fig.~1}}
\newcommand{\FigInvAnalysis}{{\bf Fig.~2}}
\newcommand{\FigResultsOverPara}{{\bf Fig.~3}}


\titleformat{\section}{\sffamily \fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sffamily \fontsize{11.5}{11.5}\bfseries}{\thesubsection}{1em}{}


\newtheoremstyle{plainCl1}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newtheoremstyle{plainCl2}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{$'$.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec


\theoremstyle{plainCl1}
\newtheorem{Claim}{Claim}
\newtheorem{Thm}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem*{Lem}{Lemma}
\newtheorem{Cor}{Corollary}
\newtheorem*{Def}{Definition}

\theoremstyle{plainCl2}
\newtheorem{Claim2}{Claim}

\title{
\bf  \sffamily \LARGE Prisoner's dilemma with stochastic payoffs\\}
\date{}
\author{Christian Hilbe, Nikoleta E. Glynatsi}

\begin{document}
\maketitle

\section{Introduction}

Evolutionary game theory is a mathematical framework for modeling evolution in
biological, social and economical systems. %ToDo references
The success of individuals is not constant, but depends on the composition of
the population. Individuals interact according to rules of the game and their
payoff is translated into reproductive fitness.

Traditionally, evolutionary game studies work with idealized assumptions when
calculating fitness. As stated in~\cite{Roca2006}, it is commonly assumed that
individuals play many times and with all other players before reproduction takes
place, so that payoffs, equivalently fitness, are given by the mean distribution
of types in the population. Initially, this implies that selection occurs much
more slowly than the interaction between individuals, even experimental studies
show that this may not always be the case in biology. Based on %ToDo references
it is clear that in cultural evolution or social learning the time scale of
selection is much closer to the time scale of interaction. The work
of~\cite{Roca2006} relaxed this assumption and showed that rapid selection
affects evolutionary dynamics in such a dramatic way that for some games it even
changes the stability of equilibria.

Furthermore, the second assumption implies that individuals have the time to
interact with with all other players. Moreover, this implies that individuals
when update their strategies over time, individuals are assumed to have perfect
memory. However, when modeling how individuals make decisions in each round,
these models assume that players only remember the last
round~\cite{Traulsen2007, Baek2016, Ye2017}. The aim of this work is too relax
this assumption, and to further the work of~\cite{Roca2006} by also considering
strategies that interact to the past interactions.

In literature, when individuals update their strategies over time they interact
with all other players so that the are given by the mean distribution of types
in the population. These payoffs are referred to as the \textit{expected}
payoffs of individuals. We contrast this standard scenario with an alternative
scenario where individuals update their strategies when remembering several of
their past interactions but not all of them. We refer to these as the \textit{stochastic}
payoffs of individuals.

Evolutionary game dynamics have extensively been studied from mostly
deterministic models based on rate equations to stochastic individual-based
models. These more sophisticated models make use of different rules to determine
how successful strategies spread. As stated in~\cite{Wu2015} there are two
classes have been used extensively, \textit{fitness-based processes} in which an
individual chosen proportional to fitness reproduces and the offspring replaces
a randomly chosen individual and \textit{pairwise comparison} processes in which
a pair of individuals is chosen, and where subsequently one of these individuals
may adopt the strategy of the other. This adoption occurs with a probability
that depends on the payoff of both individuals, such that better players are
more likely to be imitated than those who do worse.

In this we work we follow a pairwise comparison processes. We consider a well
mixed population of $N$ individuals. In our model mutation happens rarely and
such each individual can be of one of the two types, either of the
\textit{resident} or either of the \textit{mutant}.

Regarding the game, we the important case of symmetric \(2 \times 2\) games,
in which the payoffs are given by the following matrix:

\begin{equation}\label{eq:game}
    \begin{pmatrix}
        R & S  \\
        T & P
    \end{pmatrix}
\end{equation}

These cases include the Snowdrift~\cite{sugden2004economics}, Stag Hunt~\cite{skyrms2001stag},
Harmony~\cite{Licht1999} and the Prisoner's Dilemma games. These results are 
presented in Section~\ref{section:2_by_2_games}.

Moreover, we will specifically consider the donation game where each player can
cooperate by providing a benefit \(b\) to the other player at their cost \(c\),
with \(0 < c < b\). Then, \(T=b, R=b-c, S=-c, P=0\), and matrix~(\ref{eq:games})
is give by:

\begin{equation}\label{eq:donation_normal_form}
    \begin{pmatrix}
        b-c & -c  \\
        b & 0
    \end{pmatrix}
\end{equation}

These results are presented in Section~\ref{section:donation}. The relative
influence of the game is controlled by an external parameter, the so called
intensity of selection $\beta$.

The space of strategies in repeated is infinite. To continue with our
evolutionary analysis, we assume herein that individuals at most make use of
simple \textit{reactive strategies}. Reactive strategy are a set of memory-one
strategies that only take into account the previous action of the opponent. An
example of a reactive strategy is Tit For Tat. Reactive strategies can be
written explicitly as a vector \(\in \R_{3}\). More specifically a reactive
strategy \(s\) is given by \(s=(y, p, q)\) where \(y\) is the probability that
the strategy opens with a cooperation and \(p, q\) are the probabilities that
the strategy cooperates given that the opponent cooperated and defected
equivalently. The individuals of our population do not interact for a given
number of turns, on the contrast, we consider that each match has a probability
$\delta$ to continue following each turn.


\section{Expected and stochastic payoffs in the donation game}\label{section:donation}

Figure~\ref{fig:expected_and_stochastic_for_donation} shows simulation results
for the above described process. Figure~\ref{fig:expected_and_stochastic_for_donation}~depicts the evolving conditional
cooperation probabilities $p$ and $q$ (assuming that the discount
factor~$\delta$ and the benefit $b$ are comparably high). The left panel
corresponds to the standard scenario considered in the previous literature. It
considers players who use expected payoffs to update their
strategies. The right panel shows the scenario considered herein, in which
players update their strategies based on their last round's payoff. The figure
suggests that when updating is based on expected payoffs, players tend to be
more generous (their $q$-values are higher on average). In addition, players are
generally more cooperative.

\begin{figure}[!htbp]
    \includegraphics[width=\textwidth]{static/expected_and_stochastic_for_donation_game.pdf}
    \caption{{\bf Evolutionary dynamics under expected payoffs and stochastic payoffs.} 
    We have run two simulations of the evolutionary process described in
    Section~\ref{section:methods} for $T\!=\!10^7$ time steps. For each time step,
    we have recorded the current resident population ($y,p,q$). Since simulations
    are run for a relatively high continuation probability of $\delta\!=\!0.999$, we
    do not report the players' initial cooperation probability $y$. The graphs show
    how often the resident population chooses each combination ($p,q$) of
    conditional cooperation probabilities in the subsequent rounds. ({\bf A}) If
    players update based on their expected payoffs, the resident population
    typically applies a strategy for which $p\!\approx\!1$ and
    $q\!\le\!1\!-\!c/b\!=\!0.9$. The cooperation rate within the resident population
    (averaged over all games and over all time steps) is close to 100\%. ({\bf B})
    When players update their strategies based on their realized payoffs in the last
    round, there are two different predominant behaviors. The resident population
    either consists of defectors (with $p\!\approx\!q\!\approx\!0$) or of
    conditional cooperators. In the latter case, the maximum level of $q$ consistent
    with stable cooperation is somewhat smaller compared to the expected-payoff
    setting, $q\!<\!0.5$. Also the resulting cooperation rate is smaller. On
    average, players cooperate roughly in half of all rounds. Parameters:
    $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.999$.}
    \label{fig:expected_and_stochastic_for_donation}
\end{figure}

The difference in the cooperation rates becomes more and more obvious as the
temptation to defect increases, Figure~\ref{fig:expected_and_stochastic_for_donation_b_10}.

\begin{figure}[!htbp]
    \includegraphics[width=\textwidth]{static/expected_and_stochastic_for_donation_game_b_10.pdf}
    \caption{{\bf Evolutionary dynamics under expected payoffs and stochastic payoffs.} 
    Parameters:
    $N\!=\!100$, $b\!=\!10$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.999$.}
    \label{fig:expected_and_stochastic_for_donation_b_10}
\end{figure}

We explored how the evolving cooperation rates change as we vary the benefit $b$
and the selection strength $\beta$
(Figure~\ref{fig:cooration_under_parameters}). In all cases, we find that the
two scenarios yield similar cooperation rates when the respective parameters are
small. Once there is a high benefit to cooperation, strong selection, or a high
expected number of rounds, updating based on expected payoffs yields higher
cooperation rates.

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{static/cooperation_rate_over_b.pdf}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{static/cooperation_rate_over_betas.pdf}
    \end{subfigure}
\caption{{\bf The evolution of cooperation for different parameter values.} 
While the previous figures depict the evolutionary outcome for fixed parameter
values, here we vary the benefit of cooperation $b$ and the strength of
selection $\beta$. In all cases, stochastic payoff evaluation tends to reduce
the evolving cooperation rates. Unless explicitly varied, the parameters of the
simulation are $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$,
$\delta\!=\!0.99$. Simulations are run for $T\!=\!5\times 10^6$ time steps for
each parameter combination.}\label{fig:cooration_under_parameters}
\end{figure}

\section{Expected and stochastic payoffs in $2 \times 2$ games}\label{section:2_by_2_games}

\section{Methods}\label{section:methods}

In evolution context we consider a population of \(N\) players,
where \(N\) is even, and where mutations are sufficiently rare. At any point
in time the there are at most two different strategies are present in
the population. Suppose there are \(N - k\) players who use the strategy \(s_{1}=(y_{1},
p_{1}, q_{1})\), whereas \(k\) players use the strategy \(s_{2}=(y_{2}, p_{2},
q_{2})\). We refer to these two player types as ‘residents’ and ‘mutants’,
respectively.

Each step of the evolutionary process consists of two stages, a game stage and
an updating stage.

\begin{enumerate}
    \item In the game stage, each player is randomly matched with some
    other player in the population to interact in one instance of the IPD.
    \item In the updating stage, two players are randomly drawn from
    the population, a `learner' and a `exemplar'. Given that the learner's payoff
    in the last round is $u_L\!\in\! \mathcal{U}$ and that the exemplar's last
    round's payoff $u_E\!\in\! \mathcal{U}$, we assume the learner adopts the role
    model's strategy with probability 

    \begin{equation} \label{Eq:rho}
    \rho(u_L, u_E) = \frac{1}{1\!+\!\exp\big[ \!-\!\beta (u_E\!-\!u_L) \big]}. 
    \end{equation}

    where $\beta\!\ge\!0$ corresponds to the strength of selection.
\end{enumerate}

We iterate this basic evolutionary step until either the mutant strategy goes
extinct, or until it fixes in the population (in which case the mutant strategy
becomes the new resident strategy). After either outcome, we introduce a new
mutant strategy $s'_2\!=\!(y_2',p_2',q_2')$ (uniformly chosen from all reactive
strategies at random), and we set the number of mutants to $k\!=\!1$. This
process of mutation and fixation/extinction is then iterated many times.

We compare this process for what we defined as {\bf stochastic payoff
evaluation} with the analogous process where players update their strategies
with respect to their {\bf expected} payoffs,

\begin{equation} \label{eq:ExpPay}
\begin{array}{lcrcr}
\pi_1 &= &\displaystyle \frac{N\!-\!k\!-\!1}{N-1}\cdot \langle\mathbf{v}(s_1,s_1),\mathbf{U}\rangle	& + &\displaystyle\frac{k}{N-1}\cdot \langle\mathbf{v}(s_1,s_2),\mathbf{U}\rangle,\\[0.5cm]
\pi_2 &= &\displaystyle\frac{N-k}{N-1}\cdot \langle\mathbf{v}(s_2,s_1),\mathbf{U}\rangle & + &\displaystyle\frac{k-1}{N-1}\cdot \langle\mathbf{v}(s_2,s_2),\mathbf{U}\rangle.\\
\end{array}
\end{equation}

In the limit of no discounting, $\delta\!\rightarrow\! 1$, this process based on
expected payoffs has been considered in~\cite{imhof2010stochastic}.

We define {\bf stochastic payoff} as the average payoff $u\!\in\! \mathcal{U}$ a player
receives in the last \(n\) rounds of the game given that they interact with
\(m\) players.

\subsubsection*{Case \(n=m=1\).}

Initially, consider the situation where \(n=m=1\). The player's stochastic
payoff is what they receive in the last round against a single opponent. There
only four possible outcomes for the last round, those are \(CC, CD, DC, DD\).
Consider two players with reactive strategies $S_1\!=\!(y_1, p_1, q_1$) and
$S_2\!=\!(y_2,p_2,q_2)$ who interact in a repeated prisoner's dilemma with
continuation probability $\delta$, the probability that are in each of the
four possible states in the last round is given by:

\begin{equation}
    \mathbf{v}(s_1,s_2)\!=\!\Big(\mathbf{v}_{R}(s_1,s_2),\mathbf{v}_{S}(s_1,s_2),\mathbf{v}_{T}(s_1,s_2),\mathbf{v}_{P}(s_1,s_2)\Big).
\end{equation}

where,

\begin{equation} \label{Eq:LastRound}
    \setlength{\arraycolsep}{1pt}
    \begin{array}{rcl}

    \mathbf{v}_{R}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
    {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

    \mathbf{v}_{S}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
    {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

    \mathbf{v}_{T}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
    {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

    \mathbf{v}_{P}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
    {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)}.
    \end{array}
\end{equation}

\begin{proof}

Assume a repeated prisoner's dilemma between two reactive strategies. Given the
continuation probability $\delta$, probability that the game ends in the after
the first round $(1 - \delta)$ and the expected distribution of the four
outcomes in the very first round is $\mathbf{v_0}$ defined as. Following the
first round the, the outcome of the next rounds with a probability $\delta$
is \(M\) such that,

\[\dots\]

It can shown that, \(!(1\!-\!\delta)\mathbf{v_0}(I_4-\delta M)^{-1}\) and with
some algebraic manipulation we derive to Equation~\ref{Eq:LastRound}.

\end{proof}

% \section{Evolutionary dynamics in finite population}

% The Prisoner's Dilemma (PD) is a two player no-cooperative game. The players
% simultaneously and independently make a decision to Cooperate (C) or to defect
% (D). The payoffs for both players depend on their actions and the actions of
% their opponents. More specifically, the payoffs are given by:

% \begin{equation}\label{eq:pd_normal_form}
%     \begin{pmatrix}
%         R & S  \\
%         T & P
%     \end{pmatrix}
% \end{equation}

% The payoffs, \((R, P, S, T)\), are constrained by \(T > R > P > S\) and \(2R > T + S\).

% A special case is the donation game,” where each player can cooperate by providing 
% a benefit \(b\) to the other player at their cost \(c\), with \(0 < c < b\).
% Then, \(T=b, R=b-c, S=-c, P=0\), and matrix~(\ref{eq:pd_normal_form}) is give by:

% \begin{equation}\label{eq:donation_normal_form}
%     \begin{pmatrix}
%         b-c & -c  \\
%         b & 0
%     \end{pmatrix}
% \end{equation}

% where \(c\) is the cost inflicted to an individual for cooperating and \(b\) is
% the benefit.

% The questions of evolution become more interesting when repetition is considered,
% and the history can be accessed when making decisions. The iterated form of the
% game is called the Iterated Prisoner's Dilemma (IPD). There are several two
% player repeated games such as the Snowdrift game~\cite{sugden2004economics},
% Harmony and the Stag Hunt~\cite{skyrms2001stag} game. We present results in all
% four games.

% The space of strategies in the IPD is infinite. To continue with our
% evolutionary analysis, we assume herein that individuals at most make use of
% simple \textbf{reactive strategies}. Reactive strategy are a set of memory-one
% strategies that only take into account the previous action of the opponent. An
% example of a reactive strategy is Tit For Tat. Reactive strategies can be
% written explicitly as a vector \(\in \R_{3}\). More specifically a reactive
% strategy \(s\) is given by \(s=(y, p, q)\) where \(y\) is the probability that
% the strategy opens with a cooperation and \(p, q\) are the probabilities that
% the strategy cooperates given that the opponent cooperated and defected
% equivalently.

% A match between two reactive strategies takes the form of a stochastic process,
% and more specifically, of a Markov chain with four possible states \(CC, CD, DC,
% DD\) (the possible outcomes of each round). We assume a match between two
% reactive strategies \(s_{1}=(y_{1}, p_{1}, q_{1})\) and \(s_{2}=(y_{2}, p_{2},
% q_{2})\), the Markov process is described by the transition matrix \(M\):

% \begin{equation}
%     \input{static/matrix}
% \end{equation}

% The long run steady state probability vector \(\mathbf{v}\), which is the
% solution to \(\mathbf{v} M = \mathbf{v}\), can be combined with the payoff of
% matrix (\ref{eq:pd_normal_form}) (denoted as \(U\)) to give the expected payoffs
% for each player. More specifically, the payoffs for a reactive strategy \(s_1\)
% against an opponent \(s_2\) is:

% \begin{equation}
%     \mathbf{v}(s_1, s_2) \cdot U
% \end{equation}

% where

% \begin{equation}
%     U = \{R, S, T, P\}.
% \end{equation}

% \subsection{Evolutionary Dynamics}

% In evolution context we consider a population of \(N\) players,
% where \(N\) is even, and where mutations are sufficiently rare. At any point
% in time the there are at most two different strategies are present in
% the population. Suppose there are \(N - k\) players who use the strategy \(s_{1}=(y_{1},
% p_{1}, q_{1})\), whereas \(k\) players use the strategy \(s_{2}=(y_{2}, p_{2},
% q_{2})\). We refer to these two player types as ‘residents’ and ‘mutants’,
% respectively.

% Each step of the evolutionary process consists of two stages, a game stage and
% an updating stage.

% \begin{enumerate}
%     \item In the game stage, each player is randomly matched with some
%     other player in the population to interact in one instance of the IPD.
%     \item In the updating stage, two players are randomly drawn from
%     the population, a `learner' and a `exemplar'. Given that the learner's payoff
%     in the last round is $u_L\!\in\! \mathcal{U}$ and that the exemplar's last
%     round's payoff $u_E\!\in\! \mathcal{U}$, we assume the learner adopts the role
%     model's strategy with probability 

%     \begin{equation} \label{Eq:rho}
%     \rho(u_L, u_E) = \frac{1}{1\!+\!\exp\big[ \!-\!\beta (u_E\!-\!u_L) \big]}. 
%     \end{equation}

%     where $\beta\!\ge\!0$ corresponds to the strength of selection.
% \end{enumerate}

% We iterate this basic evolutionary step until either the mutant strategy goes
% extinct, or until it fixes in the population (in which case the mutant strategy
% becomes the new resident strategy). After either outcome, we introduce a new
% mutant strategy $s'_2\!=\!(y_2',p_2',q_2')$ (uniformly chosen from all reactive
% strategies at random), and we set the number of mutants to $k\!=\!1$. This
% process of mutation and fixation/extinction is then iterated many times.

% We compare this process for what we defined as {\bf stochastic payoff
% evaluation} with the analogous process where players update their strategies
% with respect to their {\bf expected} payoffs,

% \begin{equation} \label{eq:ExpPay}
% \begin{array}{lcrcr}
% \pi_1 &= &\displaystyle \frac{N\!-\!k\!-\!1}{N-1}\cdot \langle\mathbf{v}(s_1,s_1),\mathbf{U}\rangle	& + &\displaystyle\frac{k}{N-1}\cdot \langle\mathbf{v}(s_1,s_2),\mathbf{U}\rangle,\\[0.5cm]
% \pi_2 &= &\displaystyle\frac{N-k}{N-1}\cdot \langle\mathbf{v}(s_2,s_1),\mathbf{U}\rangle & + &\displaystyle\frac{k-1}{N-1}\cdot \langle\mathbf{v}(s_2,s_2),\mathbf{U}\rangle.\\
% \end{array}
% \end{equation}

% In the limit of no discounting, $\delta\!\rightarrow\! 1$, this process based on
% expected payoffs has been considered in~\cite{imhof2010stochastic}.

% \subsection{Stochastic payoff evaluation}

% We define {\bf stochastic payoff} as the average payoff $u\!\in\! \mathcal{U}$ a player
% receives in the last \(n\) rounds of the game given that they interact with
% \(m\) players.

% \subsubsection*{Case \(n=m=1\).}

% Initially, consider the situation where \(n=m=1\). The player's stochastic
% payoff is what they receive in the last round against a single opponent. There
% only four possible outcomes for the last round, those are \(CC, CD, DC, DD\).
% Consider two players with reactive strategies $S_1\!=\!(y_1, p_1, q_1$) and
% $S_2\!=\!(y_2,p_2,q_2)$ who interact in a repeated prisoner's dilemma with
% continuation probability $\delta$, the probability that are in each of the
% four possible states in the last round is given by:

% \begin{equation}
%     \mathbf{v}(s_1,s_2)\!=\!\Big(\mathbf{v}_{R}(s_1,s_2),\mathbf{v}_{S}(s_1,s_2),\mathbf{v}_{T}(s_1,s_2),\mathbf{v}_{P}(s_1,s_2)\Big).
% \end{equation}

% where,

% \begin{equation} \label{Eq:LastRound}
%     \setlength{\arraycolsep}{1pt}
%     \begin{array}{rcl}

%     \mathbf{v}_{R}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
%     {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

%     \mathbf{v}_{S}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
%     {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

%     \mathbf{v}_{T}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
%     {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

%     \mathbf{v}_{P}(S_1,S_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
%     {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)}.
%     \end{array}
% \end{equation}

% \begin{proof}

% Assume a repeated prisoner's dilemma between two reactive strategies. Given the
% continuation probability $\delta$, probability that the game ends in the after
% the first round $(1 - \delta)$ and the expected distribution of the four
% outcomes in the very first round is $\mathbf{v_0}$ defined as. Following the
% first round the, the outcome of the next rounds with a probability $\delta$
% is \(M\) such that,

% \[\dots\]

% It can shown that, \(!(1\!-\!\delta)\mathbf{v_0}(I_4-\delta M)^{-1}\) and with
% some algebraic manipulation we derive to Equation~\ref{Eq:LastRound}.

% \end{proof}

% % \begin{Prop}
% %     Consider a repeated prisoner's dilemma, with
% %     continuation probability $\delta$, between players with reactive strategies
% %     $s_1\!=\!(y_1, p_1, q_1$)  and $s_2\!=\!(y_2,p_2,q_2)$ respectively. Then the
% %     probability that the $s_1$ player receives the payoff $u\!\in\! \mathcal{U}$ in
% %     the very last round of the game is given by $v_{u}(S_1,S_2)$, as given by
% %     Eq.~(\ref{Eq:LastRound}).
% % \end{Prop}

% is given by $v_{u}(S_1,S_2)$

% Consider two players with reactive strategies $S_1\!=\!(y_1, p_1, q_1$) and $S_2\!=\!(y_2,p_2,q_2)$ who interact in a repeated prisoner's dilemma with continuation probability $\delta$. We consider the vector 

% \begin{equation}
% \mathbf{v}(S_1,S_2)\!=\!\Big(v_{R}(S_1,S_2),v_{S}(S_1,S_2),v_{T}(S_1,S_2),v_{P}(S_1,S_2)\Big)\!:=\!(1\!-\!\delta)\mathbf{v_0}(I_4-\delta M)^{-1}.
% \end{equation}

% Here, $\mathbf{v_0}$ denotes the expected distribution of the four outcomes in the very first round, $I_4$ is the $4\!\times\!4$ identity matrix, and $M$ is the transition matrix of the Markov chain. 
% The entries of $\mathbf{v}$ can be calculated explicitly,

% \begin{equation} \label{Eq:LastRound}
% \setlength{\arraycolsep}{1pt}
% \begin{array}{rcl}

% v_{R}(S_1,S_2)	&=	&\displaystyle (1\!-\!\delta)\frac{y_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
% {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

% v_{S}(S_1,S_2)	&=	&\displaystyle (1\!-\!\delta)\frac{y_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
% {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

% v_{T}(S_1,S_2)	&=	&\displaystyle (1\!-\!\delta)\frac{\bar{y}_1y_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
% {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)},\\[1cm]

% v_{P}(S_1,S_2)	&=	&\displaystyle (1\!-\!\delta)\frac{\bar{y}_1\bar{y}_2}{1\!-\!\delta^2 r_1 r_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
% {\displaystyle(1\!-\!\delta r_1r_2)(1\!-\!\delta^2 r_1 r_2)}.

% %x_1(S_1,S_2)&=&\displaystyle\frac{(1\!-\!\delta)y_1+\delta\Big(q_1+r_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big)}{1-\delta^2r_1r_2}\\[0.7cm]
% %x_2(S_1,S_2)&=&\displaystyle\frac{(1\!-\!\delta)y_2+\delta\Big(q_2+r_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}{1-\delta^2r_1r_2}.\\
% \end{array}
% \end{equation}

% In these expressions, we have used the notation $r_i:=p_i\!-\!q_i$, $\bar{y}_i\!=\!1\!-\!y_i$, $\bar{q}_i:=1\!-\!q_i$, and $\bar{r}_i:=\bar{p}_i\!-\!\bar{q}_i=-r_i$ for $i\!\in\!\{1,2\}$. Let $\mathcal{U}=\{R,S,T,P\}$ denote the set of feasible payoffs in each round, and let $\mathbf{u}\!=\!(R,S,T,P)$ be the corresponding payoff vector. Then one can show the following result.

% \includestandalone[width=0.35\textwidth]{static/expected_stochastic}

% \includestandalone{static/stochastic}

% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=\textwidth]{static/expected_and_stochastic_for_donation_game.pdf}
%     \caption{Blah blah.}
% \end{figure}



% \section{Pairwise imitation dynamics under stochastic payoff evaluation}

% \subsection{Basic setup} \label{Sec:BasicSetup}
% In the following we consider a population of size $N$, where $N$ is even. We assume mutations are sufficiently rare such that at any point in time at most two different strategies are present in the population. Suppose there are $N\!-\!k$ players who use the strategy $S_1\!=\!(y_1,p_1,q_1)$, whereas $k$ players use the strategy $S_2\!=\!(y_2,p_2,q_2)$. We refer to these two player types as `residents' and `mutants', respectively. 

% Each step of the evolutionary process consists of two stages, a game stage and an updating stage. In the game stage, each player is randomly matched with some other player in the population to interact in one instance of the repeated prisoner's dilemma. 
% In the updating stage, two players are randomly drawn from the population, a `learner' and a `role model'. Given that the learner's payoff in the last round is $u_L\!\in\! \mathcal{U}$ and that the role model's last round's payoff $u_M\!\in\! \mathcal{U}$, we assume the learner adopts the role model's strategy with probability 
% \begin{equation} \label{Eq:rho}
% \rho(u_L, u_M) = \frac{1}{1\!+\!\exp\big[ \!-\!\beta (u_M\!-\!u_L) \big]}. 
% \end{equation}
% The parameter $\beta\!\ge\!0$ corresponds to the strength of selection.

% We iterate this basic evolutionary step until either the mutant strategy goes extinct, or until it fixes in the population (in which case the mutant strategy becomes the new resident strategy). After either outcome, we introduce a new mutant strategy $S'_2\!=\!(y_2',p_2',q_2')$ (uniformly chosen from all reactive strategies at random), and we set the number of mutants to $k\!=\!1$. This process of mutation and fixation/extinction is then iterated many times. 

% We compare this process for stochastic payoff evaluation with the analogous process where players update their strategies with respect to their {\it expected} payoffs,
% \begin{equation} \label{Eq:ExpPay}
% \begin{array}{lcrcr}
% \displaystyle \pi_1	&=	&\displaystyle \frac{N\!-\!k\!-\!1}{N-1}\cdot \langle\mathbf{v}(S_1,S_1),\mathbf{u}\rangle	&+	&\displaystyle\frac{k}{N-1}\cdot \langle\mathbf{v}(S_1,S_2),\mathbf{u}\rangle,\\[0.5cm]
% \displaystyle \pi_2	&=	&\displaystyle\frac{N-k}{N-1}\cdot \langle\mathbf{v}(S_2,S_1),\mathbf{u}\rangle&+	&\displaystyle\frac{k-1}{N-1}\cdot \langle\mathbf{v}(S_2,S_2),\mathbf{u}\rangle.\\
% \end{array}
% \end{equation}
% In the limit of no discounting, $\delta\!\rightarrow\! 1$, this process based on expected payoffs has been considered in~\cite{imhof2010stochastic}. 

% \subsection{Fixation probabilities under stochastic payoff evaluation}
% Given that $N\!-\!k$ players use the resident strategy $S_1\!=\!(y_1,p_1,q_1)$ and that the remaining $k$ players use the mutant strategy $S_2\!=\!(y_2,p_2,q_2)$, the probability that the number of mutants increases by one in one step of the evolutionary process can be written as
% \begin{equation}
% \lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_1,u_2\in\mathcal{U}} x(u_1,u_2)\cdot \rho(u_1,u_2).
% \end{equation}
% In this expression, $(N\!-\!k)/N$ is the probability that the randomly chosen learner is a resident, and $k/N$ is the probability that the role model is a mutant. The sum corresponds to the total probability that the learner adopts the role model's strategy over all possible payoffs $u_1$ and $u_2$ that the two player may have received in their respective last rounds. We use $x(u_1,u_2)$ to denote the probability that the randomly chosen resident obtained a payoff of $u_1$ in the last round of his respective game, and that the mutant obtained a payoff of $u_2$. Given that the payoffs are $u_1$ and $u_2$, the imitation probability is then given by $\rho(u_1,u_2)$, as specified by Eq.~(\ref{Eq:rho}). The probability that the respective payoffs of the players are given by $u_1$ and $u_2$ can be calculated as
% \begin{equation}
% \setlength{\arraycolsep}{1pt}
% \begin{array}{llrl}
% x(u_1,u_2)	 &=&\displaystyle \frac{1}{N\!-\!1}\cdot  &v_{u_1}(S_1,S_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F}\\[0.5cm]
% &+	
% &\displaystyle \left(1\!-\!\frac{1}{N\!-\!1}\right)  
% &\left[ \frac{k\!-\!1}{N\!-\!2}\frac{k\!-\!2}{N\!-\!3} v_{u_1}(S_1,S_2) v_{u_2}(S_2,S_2) + 
%  \frac{k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!1}{N\!-\!3} v_{u_1}(S_1,S_2) v_{u_2}(S_2,S_1)\right.\\[0.5cm]
% &&&\left. +\frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{k\!-\!1}{N\!-\!3} v_{u_1}(S_1,S_1) v_{u_2}(S_2,S_2) + 
%  \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!2}{N\!-\!3} v_{u_1}(S_1,S_1) v_{u_2}(S_2,S_1)\right].
% \end{array}
% \end{equation}
% The first term on the right side corresponds to the case that the learner and the role model happened to be matched during the game stage, which happens with probability $1/(N\!-\!1)$. In that case, we note that only those payoff pairs can occur that are feasible in a direct interaction, $(u_1,u_2)\in \mathcal{U}^2_F:=\big\{ (R,R), (S,T), (T,S), (P,P) \big\}$, as represented by the respective indicator function. Otherwise, if the learner and the role model did not interact directly, we need to distinguish four different cases, depending on whether the learner was matched with a resident or a mutant, and depending on whether the role model was matched with a resident or a mutant. 

% Analogously, we can calculate the probability that the number of mutants decreases by one in one step of the evolutionary process. This probability is
% \begin{equation}
% \lambda^-_k=\frac{N\!-\!k}{N}\cdot\frac{k}{N} \sum_{u_1,u_2\in\mathcal{U}} x(u_1,u_2)\cdot \rho(u_2,u_1).
% \end{equation}
% The fixation probability of the mutant strategy then takes the standard form \cite{nowak2004emergence},
% \begin{equation}
% \varphi = \frac{1}{1+\sum_{i=1}^{N-1}\prod_k^i \frac{\lambda^-_k}{\lambda^+_k}}.
% \end{equation}


% \subsection{Invasion analysis of ALLD into GTFT}

% In the following, we apply the above formalism to calculate how easily a single ALLD mutant can invade into a resident population with strategy GTFT. In that case, $S_1=(1,1,q)$, $S_2\!=\!(0,0,0)$, and $k\!=\!1$. When two GTFT players interact in the game, their respective probabilities for each of the four outcomes in the last round simplify to
% \begin{equation}
% \begin{array}{cc}
% v_R(GTFT,GTFT)=1, &v_S(GTFT,GTFT)=0,\\
% v_T(GTFT,GTFT)=0, &v_P(GTFT,GTFT)=0.
% \end{array}
% \end{equation}
% On the other hand, if an ALLD player interacts with a GTFT player, the respective probabilities according to Eq.~(\ref{Eq:LastRound}) become
% \begin{equation}
% \begin{array}{ll}
% v_R(ALLD,GTFT)=0,	&v_S(ALLD,GTFT)=0,\\
% v_T(ALLD,GTFT)=1\!-\!\delta+\delta q,~~~ 	&v_P(ALLD,GTFT)=\delta(1\!-\!q).
% \end{array}
% \end{equation}
% As a consequence, we obtain the following probabilities $x(u_1,u_2)$ that the payoff of a randomly chosen GTFT player is $u_1$ and that the payoff of the ALLD player is $u_2$,
% \begin{equation}
% \begin{array}{l}
% \displaystyle x(R,T)=\frac{N-2}{N-1} \cdot (1\!-\!\delta\!+\!\delta q),\\[0.5cm]
% \displaystyle x(R,P)=\frac{N-2}{N-1} \cdot \delta(1\!-\!q),\\[0.5cm]
% \displaystyle x(S,T)=\frac{1}{N-1} \cdot (1\!-\!\delta\!+\!\delta q), \\[0.5cm]
% \displaystyle x(P,P)=\frac{1}{N-1} \cdot \delta(1\!-\!q). \\[0.5cm]
% \displaystyle x(u_1,u_2)=0	~~~\text{for all other payoff pairs}~(u_1,u_2).
% \end{array}
% \end{equation}
% As a consequence, we can calculate the ratio of transition probabilities as 
% \begin{equation}
% \frac{\lambda_1^+}{\lambda_1^-}=\displaystyle \frac{\displaystyle \frac{N-2}{N-1} \cdot \left(\frac{1\!-\!\delta\!+\!\delta q}{1\!+\!\exp[-\beta(T\!-\!R)]}
% \!+\! \frac{\delta(1-q)}{1\!+\!\exp[-\beta(P\!-\!R)]}\right)
% + \frac{1}{N\!-\!1} \left(\frac{1-\delta+\delta q}{1+\exp[-\beta (T\!-\!S)]}
% \!+\! \frac{\delta(1-q)}{2}\right)}
% {\displaystyle \frac{N-2}{N-1} \cdot \left(\frac{1\!-\!\delta\!+\!\delta q}{1\!+\!\exp[-\beta(R\!-\!T)]}
% \!+\! \frac{\delta(1-q)}{1\!+\!\exp[-\beta(R\!-\!P)]}\right)
% + \frac{1}{N\!-\!1} \left(\frac{1-\delta+\delta q}{1+\exp[-\beta (S\!-\!T)]}
% \!+\! \frac{\delta(1-q)}{2}\right)}.
% \end{equation}

% \noindent
% In particular, in the limit of strong selection $\beta \rightarrow \infty$ and large populations $N\!\rightarrow \infty$, we obtain
% \begin{equation}
% \frac{\lambda_1^+}{\lambda_1^-}=
% \frac{ 1\!-\!\delta+\delta q}{\delta (1-q)}.
% \end{equation}
% This ratio is smaller than 1 (such that ALLD is disfavored to invade) if $q<1\!-\!1/(2\delta)$. For infinitely repeated games, $\delta\!\rightarrow\!1$, this condition becomes $q\!<\!1/2$ (for $q\!=\!1/2$, the payoff of the ALLD player is $T\!>\!R$ for half of the time, and it is $P\!<\!R$ for the other half. The probability that the number of mutants increase by one equals the probability that the mutant goes extinct). 


% % \subsection{Simulation results}

% % \FigEvoProc -- \FigResultsOverPara~show simulation results for the above described process. \FigEvoProc~depicts the evolving conditional cooperation probabilities $p$ and $q$ (assuming that the discount factor~$\delta$ and the benefit $b$ are comparably high). The left panel corresponds to the standard scenario considered in the previous literature. It considers players who use expected payoffs (\ref{Eq:ExpPay}) to update their strategies. The right panel shows the scenario considered herein, in which players update their strategies based on their last round's payoff. The figure suggests that when updating is based on expected payoffs, players tend to be more generous (their $q$-values are higher on average). In addition, players are generally more cooperative. 

% % \begin{figure}[t!]
% % \centering
% % \includegraphics[width=0.75\textwidth]{Fig1} 
% % \caption{{\bf Evolutionary dynamics under expected payoffs (left) and stochastic payoffs (right).} 
% % We have run two simulations of the evolutionary process described in Section~\ref{Sec:BasicSetup} for $T\!=\!10^7$ time steps. For each time step, we have recorded the current resident population ($y,p,q$). Since simulations are run for a relatively high continuation probability of $\delta\!=\!0.999$, we do not report the players' initial cooperation probability $y$. The graphs show how often the resident population chooses each combination ($p,q$) of conditional cooperation probabilities in the subsequent rounds. ({\bf A}) If players update based on their expected payoffs, the resident population typically applies a strategy for which $p\!\approx\!1$ and $q\!\le\!1\!-\!c/b\!=\!0.9$. The cooperation rate within the resident population (averaged over all games and over all time steps) is close to 100\%. ({\bf B}) When players update their strategies based on their realized payoffs in the last round, there are two different predominant behaviors. The resident population either consists of defectors (with $p\!\approx\!q\!\approx\!0$) or of conditional cooperators. In the latter case, the maximum level of $q$ consistent with stable cooperation is somewhat smaller compared to the expected-payoff setting, $q\!<\!0.5$. Also the resulting cooperation rate is smaller. On average, players cooperate roughly in half of all rounds. Parameters: $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.999$.}
% % \end{figure}

% % To obtain some intuition for this result, we have recorded which mutant strategies typically invade into an ALLD population, and which mutants invade into a GTFT population, for each of the two scenarios (\FigInvAnalysis). Compared to expected-payoff updating, the stochastic scenario drastically reduces the stability of GTFT. On average, it takes fewer mutants until a GTFT population is successfully invaded. Moreover, successful mutants  tend to be less cooperative. 

% % \begin{figure}[t!]
% % \centering
% % \includegraphics[width=0.75\textwidth]{Fig2} ~\\[-0.6cm]
% % \caption{{\bf Invasion into ALLD and into GTFT for the two evolutionary scenarios.} 
% % For this figure, we either consider a resident population of ALLD players (top) or of GTFT players (bottom). In each case, we have run 1,000 independent simulations of the evolutionary process described in Section~\ref{Sec:BasicSetup}. The colored dots within the four panels depict which mutant strategies successfully invaded into the respective resident population. The black arrow indicates the average over all successful mutant strategies. In addition, we have recorded how many mutant strategies need to be introduced into the population until the resident becomes replaced. Stochastic payoffs tend to increase the time until ALLD is invaded, and they reduce the time until GTFT is invaded. Parameters are the same as in \FigEvoProc. Strategies are defined as $ALLD\!=\!(0,0,0)$ and $GTFT=(1,1,1/3)$.}
% % ~\\[0.4cm]
% % \includegraphics[width=0.8\textwidth]{Fig3} 
% % \caption{{\bf The evolution of cooperation for different parameter values.} 
% % While the previous figures depict the evolutionary outcome for fixed parameter values, here we vary the benefit of cooperation $b$, the strength of selection $\beta$, and the expected number of rounds, $1/(1\!-\!\delta)$. In all cases, stochastic payoff evaluation tends to reduce the evolving cooperation rates. Unless explicitly varied, the parameters of the simulation are $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.99$. Simulations are run for $T\!=\!5\times 10^6$ time steps for each parameter combination.}
% % \end{figure}

% % Finally, we have also explored how the evolving cooperation rates change as we vary the benefit $b$, the selection strength $\beta$, and the discount factor $\delta$ (\FigResultsOverPara). In all three cases, we find that the two scenarios yield similar cooperation rates when the respective parameters are small. Once there is a high benefit to cooperation, strong selection, or a high expected number of rounds, updating based on expected payoffs yields higher cooperation rates.

\clearpage
\newpage


\bibliographystyle{unsrt}
\bibliography{bibliography.bib}



\end{document}
