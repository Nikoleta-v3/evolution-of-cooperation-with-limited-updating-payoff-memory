\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,array,eurosym,multirow}
\usepackage[sort&compress]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage[ruled,vlined]{algorithm2e}
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.3}
\newcommand{\R}{\mathbb{R}}

\usetikzlibrary{decorations.pathreplacing, backgrounds, fit, calc, matrix, positioning}

\definecolor{darkblue}{rgb}{0,0,.8}
\definecolor{darkgreen}{rgb}{0,0.5,0.1}
\definecolor{lightapricot}{rgb}{0.99, 0.84, 0.69}
\newcommand{\matlabfunc}[1]{\textcolor{darkblue}{#1}}
\newcommand{\matlabcomment}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\christian}[1]{\textcolor{blue}{\textbf{CH}: #1}}
\newcommand{\alex}[1]{\textcolor{red}{\textbf{AL}: #1}}

\definecolor{expectedcolor}{rgb}{0.1791464821222607, 0.49287197231833907, 0.7354248366013072}
\definecolor{oneone}{rgb}{0.8503344867358708, 0.14686658977316416, 0.13633217993079583}
\definecolor{onetwo}{rgb}{0.18246828143021915, 0.5933256439830834, 0.3067589388696655}
\definecolor{twoone}{rgb}{0.4488427527873895, 0.3839600153787005, 0.6738792772010764}
\definecolor{twotwo}{rgb}{0.8871510957324106, 0.3320876585928489, 0.03104959630911188}


%% Adding shortcut commands to refer to our figures %%
\newcommand{\FigEvoProc}{{\bf Fig.~1}} \newcommand{\FigInvAnalysis}{{\bfFig.~2}} \newcommand{\FigResultsOverPara}{{\bf Fig.~3}}


\titleformat{\section}{\sffamily \fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sffamily
\fontsize{11.5}{11.5}\bfseries}{\thesubsection}{1em}{}

\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{treenode/.style = {align=center, inner sep=0pt, text centered,
  font=\sffamily}, arn_n/.style = {treenode, circle, white,
  font=\sffamily\bfseries, draw=black, inner sep=-6pt, fill=black, text
  width=1.5em},% arbre rouge noir, noeud noir
  arn_r/.style = {treenode, circle, red, text width=1.5em, very thick, inner
    sep=4pt},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black, minimum width=0.5em, minimum
    height=0.5em}% arbre rouge noir, nil
}

\newtheoremstyle{plainCl1}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newtheoremstyle{plainCl2}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{$'$.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newcommand{\splitatcommas}[1]{%
  \begingroup
  \begingroup\lccode`~=`, \lowercase{\endgroup \edef~{\mathchar\the\mathcode`,
    \penalty0 \noexpand\hspace{0pt plus 1em}}%
  }\mathcode`,="8000 #1%
  \endgroup
}

\theoremstyle{plainCl1}
\newtheorem{Claim}{Claim}
\newtheorem{Thm}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem*{Lem}{Lemma}
\newtheorem{Cor}{Corollary}
\newtheorem*{Def}{Definition}

\theoremstyle{plainCl2}
\newtheorem{Claim2}{Claim}

\title{\bf  \sffamily \LARGE Supplementary Information: Evolution of cooperation among individuals with
limited updating payoff memory\\}
\date{}
\author{Nikoleta E. Glynatsi, Christian Hilbe, Alex McAvoy}

\begin{document}
\maketitle

Section~\ref{section:pairwise_comparison} gives a brief overview of the pairwise
comparison process. The pairwise comparison process consists of three phases;
(1) the mutation phase (2) the game phase and (3) the update phase. In the
update phase an individual adopts the strategy of another individual based on
their ``updating payoffs''. Here we study different methods for calculating
updating payoffs and we compare their results using as an example the prisoner's
dilemma. In Section~\ref{section:perfect_memory} we describe the conventional
method used in the literature and in Section~\ref{section:limited_memory} we
present a new method which we refer to as 
limited payoff memory. In the limited payoff memory method we assume that
individuals update based on their last round payoff against one other member of
the population. The framework can easily be extended to consider more rounds,
more interactions or both. In
Sections~\ref{section:m_one_n_two}-\ref{section:m_two_n_two} we discuss each of
these extensions and for each extension we present a special case.
In Section~\ref{section:simulation_results} we present numerical results of the
pairwise comparison process using the different updating payoff methods.
In Sections~\ref{section:memory_one} and~\ref{section:mutation} we
verify the main result of this work when we no longer assume (i) that
individuals use reactive strategies but instead memory one strategies (ii) low
mutation.

\section{Pairwise comparison process}\label{section:pairwise_comparison}

Pairwise comparison process is a stochastic process for modelling the evolution
of a finite population. The process starts with assigning all individuals of the
population the same strategy. A strategy is a set of rules of how an individual
should behave in an interaction with another individual. Each elementary time
step of the process consists of three phases; (1) the \textbf{mutation phase}
(2) the \textbf{game phase} and (3) the \textbf{update phase}. These are summarised
in Figure~\ref{fig:pairwise_phases}.

In the \textbf{mutation phase} one individual is
chosen to switch to a new mutant strategy with a probability \(\mu\).
In the \textbf{game phase} individuals are randomly matched with other
individuals in the population and they engage in a repeated game where each
subsequent turn occurs with a fixed probability \(\delta\). At each turn the
individuals decide on an action based on their strategies.
In repeated games there are infinitely many strategies, however, it is commonly
assumed that individuals can only choose strategies from a restricted set. One
such set is that of reactive strategies. A reactive
strategy considers only the previous action of the other player, and thus, a
reactive strategy \(s\) can be written as a three-dimensional vector \(s=(y, p,
q)\). The parameter \(y\) is the probability that the strategy opens with a
cooperation and \(p\), \(q\) are the probabilities that the strategy cooperates
given that the opponent cooperated and defected equivalently.

In the \textbf{update stage} two individuals are randomly selected. From
the two individuals, one  serves as the `learner' and the other as the `role
model'. The learner adopts the role model's strategy with a probability \(\rho\)
given by,

\begin{equation} \label{Eq:rho}
    \rho(\pi_{L}, \pi_{RM}) = \frac{1}{1\!+\! e^{\!-\!\beta (\pi_{RM}\!-\! \pi_{L})}}.
\end{equation}

\(\pi_{L}\) and \(\pi_{RM}\) are the updating payoffs of the learner and the
role model respectively. The updating payoffs are a measure of how successful
individuals are in the current standing of the population. The parameter
\(\beta\) is known as the selection strength, namely, it shows how important the
payoff difference is when the learner is considering adopting the strategy of
the role model.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=\textwidth]{static/pairwise_comparison_process_diagram}
    \caption{\textbf{Pairwise comparison process phases.} \textbf{A) Initialisation.} The process begins
    with a finite population where each member is assigned a given strategy.
    Each color represents a different strategy, and the members are labelled by
    letters. \textbf{B) Mutation phase.} An individual is selected (in the
    example individual A) and with a given probability \(\mu\) that individual
    adopts a new strategy. \textbf{C) Game phase.} Individuals are
    selected to interact in a repeated social dilemma with other individuals. We
    demonstrate the case where individuals B and F have been selected to
    interact. They use the reactive strategies \(s_{B} = (y_B, p_B, q_B)\) and
    \(s_{F} = (y_F, p_F, q_F)\) respectively. The opening moves depend on their
    \(y_i\) probability. In turn 1, individual B cooperated,
    thus, F cooperates with a probability \(p_{F}\) in turn 2. On the opposite,
    individual F defected in turn 1, and so B cooperates in the next turn with
    a probability \(q_{B}\). At each turn there is a probability \(\delta\) that a
    subsequent turn will occur, and with a probability (\(1- \delta\)) the interaction ends.
    \textbf{D) Update phase.} At the updating phase two individuals are chosen;
    one serves as the role of the learner and the other one as the 
    role model. In our example C adopts D's strategy with a probability
    \(\rho(\pi_{C}, \pi_{D})\) where \(\pi_{C}, \pi_{D}\) denote the updating
    payoffs of the individuals.}\label{fig:pairwise_phases}
\end{figure}

This elementary step of the process (mutation, game and update phases) is
repeated for a large number of time steps, and at each time step we record the
state of the population.

\subsection{Low mutation \(\mu \rightarrow 0\)}

In the case of low mutation (\(\mu \rightarrow 0\)) we assume that mutations are
rare. In fact, so rare that only two different strategies can be present in the
population at any given time. The case of low mutation is vastly adopted because
it allows us to explicitly calculate the fixation probability of a newly
introduced mutant.

More specifically, the process again starts with a population where all members are of the same
strategy. At each step one individual adopts a mutant strategy randomly selected
from the set of feasible strategies. The fixation probability \(\phi_{M}\) of the
mutant strategy can be calculated explicitly,

\begin{equation}\label{eq:appendix_fixation_probability}
    \varphi_{M} = \frac{1}{1+\sum\limits_{i=1}^{N-1}\prod\limits_k^i \frac{\lambda^-_k}{\lambda^+_k}},
\end{equation}

where \(\lambda^-_k, \lambda^+_k\) are the probabilities that the number of
mutants decreases and increases respectively, \(N\) is the size of the
population, and \(k\) is the number of mutants. The probabilities \(\lambda^-_k
\text{ and } \lambda^+_k\) depend on the updating payoffs of the mutant and the
resident strategies. Depending on the fixation probability \(\phi_{M}\) the
mutant either fixes (becomes the new resident) or goes extinct. Regardless, in
the elementary time step another mutant strategy is introduced to the
population. We iterate this elementary population updating process for a large
number of mutant strategies and we record the resident strategies at each time
step. The process is summarised by Algorithm~\ref{algorithm:pairwise_comparison}.

\begin{algorithm}[!htbp]
  \SetAlgoLined
  $N \leftarrow$ population size\;
  $k \leftarrow 1$\; resident
   $\leftarrow$ starting resident\;
   \While{ $t <$ maximum number of steps}
   {mutant $\leftarrow$ random strategy\;
   fixation probability $\leftarrow \varphi_M $\;
   \If{$\varphi_M >$ random: $i \rightarrow [0,1]$}
   {resident $\leftarrow$ mutant;}}
   \caption{Evolutionary process}\label{algorithm:pairwise_comparison}
\end{algorithm}

Most of the results we present in this work consider the case of low mutation,
however, we have also verified that the main result holds in the case of
high mutation rates (Section~\ref{section:mutation}).

\subsection{Updating Payoffs}\label{section:updating_payoffs}

The updating payoffs depend on the interactions of the individuals at the game
phase. In this work we assume that in the game phase individuals are matched in
pairs and that they participate in a repeated 2 person donation game. In the
donation game there are two actions: cooperation (\(C\)) and defection (\(D\)).
By cooperating a player provides a benefit \(b\) to the other player at their
cost \(c\), with \(0 < c < b\). Thus the payoffs for a player in each turn are,

\begin{equation}\label{eq:donation}
    \begin{blockarray}{ccc}
        & \text{cooperate} & \text{defect} \\
        \begin{block}{c(cc)}
            \text{cooperate} & b - c & -c \\
            \text{defect} & b & 0 \\
        \end{block}
    \end{blockarray}.
\end{equation}

Let \(\mathbf{u} = (b-c, -c, b, 0)\) be Eq.~\ref{eq:donation} in a vector format, and let \(\mathcal{U}
= \{r, s, t, p\}\) denote the set of feasible payoffs, where \(r\) denotes the
payoff of mutual cooperation, \(s\) the sucker's payoff, \(t\) the temptation to
defect payoff, and \(p\) the punishment payoff.

In the following sections we present several methods for calculating the
updating payoffs. Initially, we discuss the conventional way of the
expected payoffs and afterwards we present our newly introduced methods.

\section{Updating Payoffs based on Expected Payoffs (perfect payoff memory)}\label{section:perfect_memory}

The expected payoffs are the conventional payoffs used in the updating stage.
The expected payoffs are defined as the mean payoff of an individual in a well-mixed population that
engages in an infinitely repeated games with all other population members.
The game between two reactive strategies $s_1\!=\!(y_1, p_1, q_1$) and $s_2\!=\!(y_2,p_2,q_2)$
can be described by a Markovian process~\cite{nowak:APC:1989} with the
transition matrix \(M\),

\begin{equation}\label{eq:transition_matrix}
  \input{static/matrix.tex}.
\end{equation}

The stationary vector \(\mathbf{v}(s_1,s_2)\) is the solution to
\(\mathbf{v}(s_1,s_2) M = \mathbf{v}(s_1,s_2)\). In an infinitely repeated game
the long-term average payoff of \(s_1\) and \(s_2\) are,

\[\langle\mathbf{v}(s_1,s_2),\mathbf{u}\rangle \quad  \text{ and } \quad  \langle\mathbf{v}(s_2,s_1), \mathbf{u}\rangle.\]

In the case of low mutation there can be only one type of mutant strategy in the
population. So in a population of size \(N\), there will be \(k\) mutants and
\(N - k\) residents, whose strategies we denote respectively as \(s_M =(y_M,
p_M, q_M)\) and \(s_R = (y_R, p_R, q_R)\).
The expected payoffs of a resident (\(\pi_R\)) and for a
mutant (\(\pi_M\)) are give by,

\begin{equation} \label{Eq:ExpPay}
  \begin{array}{lcrcr}
  \displaystyle \pi_R & = &\displaystyle \frac{N\!-\!k\!-\!1}{N-1}\cdot \langle\mathbf{v}(s_R,s_R),\mathbf{u}\rangle	&+	&\displaystyle\frac{k}{N-1}\cdot \langle\mathbf{v}(s_R,s_M),\mathbf{u}\rangle,\\[0.5cm]
  \displaystyle \pi_M & = &\displaystyle\frac{N-k}{N-1}\cdot \langle\mathbf{v}(s_M,s_R),\mathbf{u}\rangle&+	&\displaystyle\frac{k-1}{N-1}\cdot \langle\mathbf{v}(s_M,s_M),\mathbf{u}\rangle.\\
  \end{array}
\end{equation}

\(\frac{N\!-\!k\!-\!1}{N-1}\) is the probability that a resident meets another
resident and \(\frac{k}{N-1}\) is the probability that a resident meets a
mutant. Likewise, \(\frac{N-k}{N-1}\) is the probability that a mutant interacts
with a resident and \(\frac{k-1}{N-1}\) the probability that the mutants
interacts with a mutant.

The number of mutants in the population increases if a resident adopts the strategy
of a mutant, and decreases if a mutant adopts the strategy of a resident. The
probabilities that the number of mutants decreases and increases,
\(\lambda^-_k\) and \(\lambda^+_k\), are now explicitly defined as,

\begin{align*}
  \lambda^-_k \!=\!\rho(\pi_M, \pi_R) \quad \text{ and } \quad \lambda^+_k \!=\!\rho(\pi_R, \pi_M).
\end{align*}

\subsubsection*{Invasion Analysis}

Let's assume that individuals use their expected payoffs at the updating phase.
We can calculate how easily a single defecting mutant (ALLD) can invade into a
resident population of conditional cooperators, otherwise known as generous tit
for tat (GTFT). Let GTFT\(= (1, 1, q)\), ALLD\(= (0, 0, 0)\), and \(k =
1\). When two GTFT players interact in an infinitely repeated game, the
stationary distribution \(\mathbf{v}(\text{GTFT}, \text{ALLD})\) 
simplifies to,

\begin{align*}
    \mathbf{v}(\text{GTFT}, \text{ALLD}) = (1, 0, 0, 0).
\end{align*}

On the other hand, if an ALLD player interacts with a GTFT player, the
respective probabilities become,

\begin{align*}
  \mathbf{v}(\text{ALLD}, \text{GTFT}) = (0, q, 0, (1 - q)).
\end{align*}

Using the above we can define the payoffs of a GTFT individual (resident)
and of the ALLD individual (mutant) follows,

\begin{align*}
  \displaystyle \pi_{\text{GTFT}} & = \displaystyle \frac{N\!-\!2}{N-1} (b - c)  -	\displaystyle\frac{q c}{N-1} \quad \text{ and } \quad \displaystyle \pi_{\text{ALLD}}  = \displaystyle b q.
\end{align*}

As a consequence, we can calculate the ratio of transition probabilities as,

\begin{align*}
    \frac{\lambda^{+}}{\lambda^{-}} & = \frac{\rho(\pi_{\text{GTFT}}, \pi_{\text{ALLD}})}{\rho(\pi_{\text{ALLD}}, \pi_{\text{GTFT}})}  = \frac{e^{-\beta \left(\frac{(N-2) (b-c)}{N-1} - q (b + \frac{c}{N-1})\right)}+1}
    {e^{-\beta\left(b q - \frac{b (N - 2)}{N-1} - \frac{c (N - 2 - q)}{N-1}\right)}+1}
\end{align*}

In particular, in the limit of strong selection \(\beta \rightarrow \infty\)
and large populations \(N \rightarrow \infty\), we obtain that the ratio is than
smaller to 1 if \(q \leq 1 - \frac{c}{b}\). Thus, ALLD is disfavored to invade
if \(q \leq 1 - \frac{c}{b}\). For \(q = 1 -\frac{c}{b}\) the probability that
the number of mutants increase by one equals the probability that the mutant
goes extinct.

This result will become important when we discuss the numerical results of the
evolutionary process (Section~\ref{section:simulation_results}). We repeat this
``invasion analysis'' for the rest of the updating payoffs cases.

\section{Updating Payoffs based on the Last Round Payoff of One Interaction (limited payoff memory)}\label{section:limited_memory}

In the expected payoffs case the payoff of a pair depends on the average payoff
they received over an infinite number of turns. In the limited payoff memory,
the payoff of a pair depends on the average payoffs they received in the last
turn. Moreover, in expected payoffs it is assumed that a player interacts with
every member of the population whereas in the limited payoff memory approach a
player has one interaction.

Initially, we define the probability that a reactive strategy receives the
payoff $u\!\in\! \mathcal{U}$ in the very last round of the game against another
reactive strategy (Proposition~\ref{proposition:last_round}).

\begin{Prop}\label{proposition:last_round} Consider a repeated game, with
    continuation probability $\delta$, between players with reactive strategies
    $s_1\!=\!(y_1, p_1, q_1$)  and $s_2\!=\!(y_2,p_2,q_2)$ respectively. Then
    the probability that the $s_1$ player receives the payoff $u\!\in\!
    \mathcal{U}$ in the very last round of the game is given by
    $v_{u}(s_1,s_2)$, as given by Eq.~(\ref{Eq:LastRound}).

    \begin{equation} \label{Eq:LastRound}
      \setlength{\arraycolsep}{1pt}
      \begin{array}{rcl}
    
      v_{r}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1y_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(q_1+l_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(q_2+l_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)},\\[1cm]
    
      v_{s}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1\bar{y}_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(q_1+l_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)},\\[1cm]
    
      v_{t}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1y_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(q_2+l_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)},\\[1cm]
    
      v_{p}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1\bar{y}_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)}.
      \end{array}
    \end{equation}

In these expressions, we have used the notation $l_i:=p_i\!-\!q_i$,
$\bar{y}_i\!=\!1\!-\!y_i$, $\bar{q}_i:=1\!-\!q_i$, and
$\bar{l}_i:=\bar{p}_i\!-\!\bar{q}_i=-l_i$ for $i\!\in\!\{1,2\}$.
\end{Prop}

Note that in the proposition we here we focus on the case of the donation game/prisoner's
dilemma but the result applies to any \(2 \times 2\) symmetric game.

\begin{proof}
Given a play between two reactive strategies with continuation probability
$\delta$. The outcome at turn \(t\) is given by,

\begin{equation}\label{eq:}
  (1 - \delta) \mathbf{v_0} \sum \delta^{t} M^{(t)},
\end{equation}

where $\mathbf{v_0}$ denotes the expected distribution of the four outcomes in
the very first round, and \(1- \delta\) the probability that the game ends. It
can be shown that,

\begin{align*}
  (1 - \delta) \mathbf{v_0} \sum \delta^{t} M^{(t)} & = (1 - \delta)(\mathbf{v_0} + \delta \mathbf{v_0} M + \delta^{2}\mathbf{v_0} M ^{2} + \dots )\\ 
   & = (1 - \delta)\mathbf{v_0} (1 + \delta M + \delta^{2}M ^{2} + \dots ) \text{ using standard formula for geometric series}\\ 
   & = (1 - \delta)\mathbf{v_0}(I_4 - \delta M)^{-1}
\end{align*}

where \((1 - \delta)\mathbf{v_0}(I_4 - \delta M)^{-1}\) is vector \(\in R^{4}\)
and it the probabilities for being in any of the outcomes \(CC, CD, DC, DD\) in
the last round. Combining this with the payoff vector \(u\) and some algebraic
manipulation we derive to the Equation~\ref{Eq:LastRound}.
\end{proof}

At each step of the evolutionary process we choose a role model and a learner to
update the population. In this case both the role model and the learner estimate
their fitness after interacting with a single member of the population, and so
there are five possible pairings at each step. They interact with each other with a
probability \(\frac{1}{N - 1}\), and they do not interact with other with a
probability \(1 - \frac{1}{N - 1}\). In the latter case, each of them can
interact with either a mutant or a resident. Both of them interact with a mutant
with a probability $\frac{(k-1)(k-2)}{(N-2)(N-3)}$ and both interact with a
resident with a probability $\frac{(N-k-1)(N-k-2)}{(N-2)(N-3)}$. The last two
possible pairings are that either of them interacts with a resident whilst the
other interacts with a mutant, and this happens with a probability
$\frac{(N-k-1)(k-1)}{(N-2)(N-3)}$. Given the possible pairings and
Proposition~\ref{proposition:last_round}, we define the probability that the
respective last round payoffs of two players \(s_1, s_2\) are given by $u_1$ and
$u_2$ as,

\begin{equation}\label{eq:Chi}
\setlength{\arraycolsep}{1pt}
\begin{array}{llrl}
x(u_1,u_2)	 &=&\displaystyle \frac{1}{N\!-\!1}\cdot  &v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}_F}\\[0.5cm]
&+	
&\displaystyle \left(1\!-\!\frac{1}{N\!-\!1}\right)  
&\left[ \frac{k\!-\!1}{N\!-\!2}\frac{k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) + 
 \frac{k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1)\right.\\[0.5cm]
&&&\left. +\frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) + 
 \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1)\right].
\end{array}
\end{equation}

The first term on the right side corresponds to the case that the learner and
the role model happened to be matched during the game stage, which happens with
probability $\frac{1}{(N\!-\!1)}$. In that case, we note that only those payoff
pairs can occur that are feasible in a direct interaction,
$\splitatcommas{(u_1,u_2)\in \mathcal{U}_F:=\big\{ (r, r), (s, t), (t, s), (p,
p) \big\}}$, as represented by the respective indicator function. Otherwise, if
the learner and the role model did not interact directly, we need to distinguish
four different cases, depending on whether the learner was matched with a
resident or a mutant, and depending on whether the role model was matched with a
resident or a mutant.

The probability that the number of mutants increases, and decreases respectively,
by one is now given by,

\begin{align}
\lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} x(u_{R},u_{M})\cdot \rho(u_{R},u_{M}), \\
\lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} x(u_{R},u_{M})\cdot \rho(u_{M},u_{R}).
\end{align}

In this expression, $\frac{(N\!-\!k)}{N}$ is the probability that the randomly
chosen learner is a resident, and $\frac{k}{N}$ is the probability that the role
model is a mutant. The sum corresponds to the total probability that the learner
adopts the role model's strategy over all possible payoffs $u_R$ and $u_M$ that
the two players may have received in their respective last rounds. We use
$x(u_R,u_M)$ to denote the probability that the randomly chosen resident
obtained a payoff of $u_R$ in the last round of his respective game, and that
the mutant obtained a payoff of $u_M$.


\subsubsection*{Invasion Analysis}

We once again calculate how easily a single ALLD mutant can invade into a
resident population of GTFT player. When two GTFT players interact in the game,
their respective probabilities for each of the four outcomes in the last round
simplify to,

\begin{align*}
    v_{r}(GTFT,GTFT) = 1, & \quad v_{t}(GTFT,GTFT) = 0, \\
    v_{s}(GTFT,GTFT) = 0, & \quad v_{p}(GTFT,GTFT) = 0.
\end{align*}

On the other hand, if an ALLD player interacts with a GTFT player, the
respective probabilities according to Eq.~\ref{Eq:LastRound} become

\begin{align*}
    v_{r}(ALLD,GTFT) & = 0, &  v_{s}(ALLD,GTFT) & = 0, \\
    v_{t} (ALLD, GTFT ) & = 1 - \delta + \delta q, &  v_{p} (ALLD, GTFT) & = \delta(1 - q).
\end{align*}

As a consequence, we obtain the following probabilities \(x(u_1, u_2)\) that the
payoff of a randomly chosen GTFT player is \(u_1\) and that the payoff of the
ALLD player is \(u_2\),

\begin{align*}
  x(r, t) = & \frac{N - 2}{N - 1} \cdot (1 - \delta + \delta q)\\
  x(r, r) = & \frac{N - 2}{N - 1} \cdot \delta (1 - q) \\
  x(s, r) = & \frac{1}{N - 1} \cdot (1 - \delta + \delta q) \\
  x(p, p) = & \frac{1}{N - 1} \cdot \delta (1 - q) \\
  x(u_1, u_2) = &  0 \text{ for all other payoff pairs } (u_1, u_2).
\end{align*}

We now calculate the ratio of transition probabilities as

\begin{equation*}
\frac{\lambda^{+}}{\lambda^{-}} = \frac{ \frac{N - 2}{N - 1}  \left( \frac{ \delta  \left(1 - q \right)}{1
+ e^{-  \beta  \left(b - c \right)}} +  \frac{ \delta q -  \delta + 1}{e^{ \beta
c} + 1} \right)  +  \frac{1}{N-1}  \left(\frac{ \delta  \left(1 - q \right)}{2} +
 \frac{ \delta q -  \delta + 1}{1 + e^{-  \beta  \left(- b - c \right)}}\right)}
 { \frac{N - 2}{N - 1}  \left( \frac{ \delta  \left(1 - q \right)}{1 +
e^{-  \beta  \left(- b + c \right)}} +  \frac{ \delta q -  \delta + 1}{1 + e^{-
 \beta c}} \right) +  \frac{1}{N -1} \left(\frac{ \delta  \left(1 - q \right)}{2} +
 \frac{ \delta q -  \delta + 1}{1 + e^{-  \beta  \left(b + c \right)}}\right)}
\end{equation*}

In particular, in the limit of strong selection \(\beta \rightarrow \infty\) and
large populations \(N \rightarrow \infty \), we obtain

\begin{equation*}
    \frac{\lambda^{+}}{\lambda^{-}} = \frac{1 - \delta + \delta q}{\delta(1 - q)}
\end{equation*}

This ratio is smaller than 1 (such that ALLD is disfavored to invade) if \(q <
1- 1/(2 \delta)\). For infinitely repeated games, \(\delta \rightarrow 1\), this
condition becomes \(q < 1/2\) (for \(q = 1/2\), the payoff of the ALLD player is
\(r > r\) for half of the time, and it is \(p < r\) for the other half. The
probability that the number of mutants increase by one equals the probability
that the mutant goes extinct).

\section{Updating Payoffs based on the last round payoff of $n$ interactions}\label{section:m_one_n_two}

The framework of the limited memory can be generalised such that an individual
considers \(m\) rounds and of \(n\) interactions. Here we discuss the case that
the update depends on the last round of \(n\) interactions.

At each step of the evolutionary process the role model and the learner now
participate in \(n\) matches. We need to define the probability that for each of
the matches they are paired with a mutant, with a resident or with each other.
We assume that each pair is unique, so for example the resident and role model
can be matched together only once at each step. A representation of the process
is given in Figure~\ref{fig:matching_tree}. In the case of \(n=1\) there are
five possible pairs, however, the number of possible pairs increases non
linearly as we increase the number of possible interactions. We demonstrate this
case for \(n=2\), namely, for when the role model and the learner have two
interactions.


\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=\textwidth]{static/matching_tree.pdf}
  \caption{\textbf{Tree diagram of the possible pairs when the learner and the role
  model have $n$ interactions.} In the diagram \((s_i, s_j)\) represent a possible
  pair. We denote the role model as $s_{RM}$ and the learner as $s_{L}$, and
  a member of the population that plays a mutant strategy is denoted as $s_{M}$ and
  a resident strategy as $s_{R}$. The role model and
  the learner need to be paired with $n$ other members of the population
  (including each other). We break down the process into stages. For the first
  pair (stage one) the role model and the mutant can be paired together (with
  probability $\frac{1}{N-1}$) or not (with probability $1 -
  \left(\frac{1}{N-1}\right)$). In the case that they are not paired together,
  both can be paired with a mutant $\frac{(k-1)(k-2)}{(N-2)(N-3)}$, with a
  resident $\frac{(N-k-1)(N-k-2)}{(N-2)(N-3)}$, or one is paired with a mutant
  whilst the other is paired with a resident $\frac{(N-k-1)(k-1)}{(N-2)(N-3)}$.
  There are five possible pairs for stage one as we have already discussed
  in Section~\ref{section:limited_memory}. In
  the second stage we need to consider if the role model and the learner
  interacted with each other. If yes, then they can not interact with each other
  again, and at each stage there are at most four possible pairs; they interact with
  mutants, with residents, or the one interacts with a mutant whilst the second
  interacts with a resident. In the case that they were not paired in stage 1
  there are five possible pairs. These pairs are the same as in stage 1,
  however now the probabilities differ. Namely, let's assume that in the
  first stage both were matched with a mutant. They interact with each
  other with a probability $\frac{1}{N-2}$ or not with a probability
  $1 - \left(\frac{1}{N-2}\right)$. In the latter case there are four
  possible pairs.
  Both can be paired with a mutant $\frac{(k-1)(k-2)}{(N-3)(N-4)}$, with a
  resident $\frac{(N-k-1)(N-k-2)}{(N-3)(N-4)}$, or one is paired with a mutant
  whilst the other is paired with a resident $\frac{(N-k-1)(k-2)}{(N-3)(N-4)}$.
  The process continues following the same logic until it reaches the $n^{\text{th}}$
  stage.}\label{fig:matching_tree}
\end{figure}

\subsection{Last Round Payoff of Two Interactions}

Here the learner and role model consider the last round payoff of two
interactions. At each time step of the evolutionary process there are twenty
four possible pairs. At the first stage there are fiver possible pairs. For four of
these pairs there are five possible pairs in the second stage (\(4 \times 5 =
20\)), and for the last one there are four possible pairs in the second stage
(20 + 4 = 24). 

We assume that player \(s_1\) receives the payoff
\(u_{11}\) with their first interaction and \(u_{12}\) with their second.
Respectively, player \(s_2\) receives \(u_{21}\) and \(u_{22}\). We define the
probability that the respective last round payoffs of the two players \(s_1,
s_2\) are given by \((u_{11}, u_{11})\) and \((u_{21}, u_{22})\) as
\(x^{2}((u_{11}, u_{11}), (u_{21}, u_{22}))\) given by,

\begin{equation}
  \resizebox{.99\textwidth}{!}
  {%
$
  \setlength{\arraycolsep}{1pt}
  \begin{array}{llrl}
  x^{2}((u_{11}, u_{12}), (u_{21}, u_{22}))	& = \displaystyle \frac{1}{N\!-\!1}\cdot v_{u_{11}}(s_1,s_2)\cdot 1_{(u_{11},u_{21})\in \mathcal{U}_F} \cdot A + \left(1\!-\!\frac{1}{N\!-\!1}\right) \left[ \right. \\[0.5cm] 
  & \left. v_{u_{11}}(s_1,s_2) _{u_{21}}(s_2,s_2)  \frac{\left(k - 2\right) \left(k - 1\right)}{(N\!-\!2)(N\!-\!3)}
  \left(\frac{1}{N\!-\!2}\cdot v_{u_{11}}(s_1,s_2)\cdot 1_{(u_{11},u_{21})\in \mathcal{U}_F} + (1 - \frac{1}{N - 2}) [B_1 + B_2 + B_3 + B_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_{11}}(s_1,s_2) _{u_{21}}(s_2,s_1)  \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_{11}}(s_1,s_2)\cdot 1_{(u_{11},u_{21})\in \mathcal{U}_F} + (1 - \frac{1}{N - 2}) [C_1 + C_2 + C_3 + C_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_{11}}(s_1,s_1) _{u_{21}}(s_2,s_2)  \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_{11}}(s_1,s_2)\cdot 1_{(u_{11},u_{21})\in \mathcal{U}_F} + (1 - \frac{1}{N - 2}) [D_1 + D_2 + D_3 + D_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_{11}}(s_1,s_1) _{u_{21}}(s_2,s_1)  \frac{\left(N - k - 2\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_{11}}(s_1,s_2)\cdot 1_{(u_{11},u_{21})\in \mathcal{U}_F} + (1 - \frac{1}{N - 2}) [E_1 + E_2 + E_3 + E_4]\right) \right]
  \end{array} $}
\end{equation}

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
$
  \setlength{\arraycolsep}{1pt}
  \begin{array}{llrl}
  A & = \left(1\!-\!\frac{1}{N\!-\!1}\right)  \left[ \frac{k\!-\!1}{N\!-\!2}\frac{k\!-\!2}{N\!-\!3} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_2) + 
   \frac{k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!1}{N\!-\!3} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_1) + \right. \\[0.5cm]
   & \left. \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{k\!-\!1}{N\!-\!3} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_2) + 
   \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!2}{N\!-\!3} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_1)\right] \\[0.5cm] 
   B_1 & = \frac{\left(k - 3\right) \left(k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_2) \quad
   B_2 = \frac{\left(k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_2) \\[0.5cm] 
   B_3 & = \frac{\left(k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_1) \quad
   B_4 = \frac{\left(N - k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_1) \\[0.5cm] 

   C_1 & = \frac{\left(k - 3\right) \left(k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_2) \quad
   C_2 = \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_2) \\[0.5cm] 
   C_3 & = \frac{\left(k - 2\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_1) \quad
   C_4 = \frac{\left(N - k - 2\right)^{2}}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_1) \\[0.5cm] 

   D_1 & = \frac{\left(k - 2\right)^{2}}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_2) \quad
   D_2 = \frac{\left(k - 2\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_2) \\[0.5cm] 
   D_3 & = \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_1) \quad
   D_4 = \frac{\left(N - k - 3\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_1) \\[0.5cm]

   E_1 & = \frac{\left(k - 2\right) \left(k - 1\right)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_2) \quad
   E_2 = \frac{\left(k - 1\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_2) \\[0.5cm] 
   E_3 & = \frac{\left(k - 1\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_2) v_{u_{22}}(s_2,s_1) \quad
   E_4 = \frac{\left(N - k - 3\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_{21}}(s_1,s_1) v_{u_{22}}(s_2,s_1) \\[0.5cm] 
  \end{array}$}
\end{equation}

The first term on the right side corresponds to the case that the learner and
the role model happened to be matched during the first stage,
followed by them being paired with another member of the population on the
second stage. The second terms corresponds to the case that the learner and the
role model interact with a mutant with a probability ($\frac{\left(k - 2\right)
\left(k - 1\right)}{(N\!-\!2)(N\!-\!3)}$). In the seconds stage, they can either
interact with each other $\frac{1}{N - 2}$ or not $(1 - \frac{1}{N - 2})$. If
they do not interact with each other, then each of the following can happen:
both of them interact with a mutant with a probability
$\frac{(k-3)(k-2)}{(N-4)(N-3)}$ and both interact with a resident with a
probability $\frac{(N-k-1)(N-k-2)}{(N-3)(N-4)}$. The last two possible pairs
are that either of them interacts with a resident whilst the other interacts
with a mutant, and this happens with a probability
$\frac{(N-k-2)(k-1)}{(N-4)(N-3)}$, and so on.

The probability that the number of mutants increases, and decreases
respectively, by one is now given by,

\begin{align}\label{eq:ratio_limited}
\lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R1},u_{M1}\in\mathcal{U}} \quad \sum_{u_{R2},u_{M2}\in\mathcal{U}} x^{2}((u_{R1}, u_{R2}),(u_{M1}, u_{M2}))\cdot \rho\left(\frac{u_{R1} + u_{R2}}{2}, \frac{u_{M1} + u_{M2}}{2}\right), \\
\lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R1},u_{M1}\in\mathcal{U}} \quad \sum_{u_{R2},u_{M2}\in\mathcal{U}} x^{2}((u_{R1}, u_{R2}),(u_{M1}, u_{M2}))\cdot \rho\left(\frac{u_{M1} + u_{M2}}{2}, \frac{u_{R1} + u_{R2}}{2}\right).
\end{align}

We use $x^{2}((u_{R1}, u_{R2}),(u_{M1}, u_{M2}))$ to denote the probability that
the randomly chosen resident obtained a payoff of $u_{R1}$ in the last round of
their first respective game and $u_{R1}$ in the last round of their second game.
Similarly, that the mutant obtained a payoff of $u_{M1}$ from their first
interaction and $u_{M2}$ from the second. Note that we assume that the players
compare their average payoff of their two interaction when considering adopting
the others strategy. For example the mutant adopts the resident's strategy with
a probability $\rho\left(\frac{u_{R1} + u_{R2}}{2}, \frac{u_{M1} +
u_{M2}}{2}\right)$.

\subsubsection*{Invasion Analysis}

In a similar fashion we can calculate the condition for which a population of
GTFT players can not be invaded by an ALLD mutant. Using the new formulation we
obtain the following probabilities \(x^{2}((u_{11}, u_{12}), (u_{21}, u_{22}))\) that the payoff of a randomly
chosen GTFT player is \((u_{11}, u_{12})\) and that the payoff of the ALLD player is \((u_{21}, u_{22})\),

\begin{align*}
  x^{2}((r, r), (t, t)) = & \frac{N - 3}{N - 1}\left(\delta q - \delta + 1\right)^{2} & 
  x^{2}((r, r), (t, p)) = & - \frac{N - 3}{N - 1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right)\\
  x^{2}((r, r), (p, t)) = & - \frac{N - 3}{N - 1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right) &
  x^{2}((r, r), (p, p)) = & \frac{N - 3}{N - 1} \delta^{2}  \left(q - 1\right)^{2}\\
  x^{2}((r, s), (t, t)) = & \frac{1}{N-1} \left(\delta q - \delta + 1\right)^{2} &
  x^{2}((r, s), (p, t)) = & - \frac{1}{N-1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right) \\
  x^{2}((r, p), (t, p)) = & - \frac{1}{N-1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right) &
  x^{2}((r, p), (p, p)) = & \frac{1}{N-1} \delta^{2} \left(q - 1\right)^{2} \\
  x^{2}((s, r), (t, t)) = & \frac{1}{N-1} \left(\delta q - \delta + 1\right)^{2} &
  x^{2}((s, r), (t, p)) = & - \frac{1}{N-1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right)\\
  x^{2}((p, r), (p, t)) = & - \frac{1}{N-1} \delta \left(q - 1\right) \left(\delta q - \delta + 1\right) &
  x^{2}((p, r), (p, p)) = & \frac{1}{N-1} \delta^{2} \left(q - 1\right)^{2} \\
  x^{2}((u_{11}, u_{12}), (u_{21}&, u_{22})) =  0 \text{ for all other payoff pairs} & ((u_{11}, u_{12}), (u_{21}, u_{22})). &
\end{align*}

The ratio of transition probabilities is given by,

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
$\frac{\lambda^{+}}{\lambda^{-}} =
\frac{\frac{N - 3}{N-1} \left(\frac{\delta^{2} \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- b + c\right)}} + \frac{2 \delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} + c\right)}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta c}}\right) 
+ \frac{2}{N-1} \left(\frac{\delta^{2} \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- \frac{b}{2} + \frac{c}{2}\right)}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \frac{\beta c}{2}}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta c}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta \left(\frac{b}{2} + c\right)}}\right)}
{\frac{N - 3}{N-1} \left(\frac{\delta^{2} \left(q - 1\right)^{2}}{1 + e^{- \beta \left(b - c\right)}} + \frac{2 \delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} - c\right)}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{e^{\beta c} + 1}\right) + \frac{2}{N-1} \left(\frac{\delta^{2} \left(q - 1\right)^{2}}{1 + e^{- \beta \left(\frac{b}{2} - \frac{c}{2}\right)}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{e^{\beta c} + 1} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{e^{\frac{\beta c}{2}} + 1} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta \left(- \frac{b}{2} - c\right)}}\right)}
$}
\end{equation}

In the limit of strong selection \(\beta \rightarrow \infty\) and large
populations \(N \rightarrow \infty \) we obtain the following cases,

\begin{equation}
\frac{\lambda^{+}}{\lambda^{-}} = 
\begin{cases}
  - \frac{q \left(\delta q - \delta + 1\right)}{\left(q - 1\right) \left(\delta q + 1\right)}  & \frac{b}{2} > c \\[0.5cm]
  \frac{- \delta q + \delta - 1}{\delta \left(q - 1\right)}  & \frac{b}{2} = c \\[0.5cm]
  - \frac{\delta q^{2} - 2 \delta q + \delta - 1}{\delta \left(q - 1\right)^{2}} & \frac{b}{2} < c
\end{cases}
\end{equation}

We note that the relationship between the cost and benefit have an effect on how
generous a conditional cooperator must be to avoid invasion. In the case of
\(\frac{b}{2}=c\) the result remains the same expression as in the case of
\(m=n=1\). For the other two cases we show that for
\(\frac{\lambda^{+}}{\lambda^{-}} < 1\),

\begin{equation}
\begin{cases}
  q < \left\{\frac{\delta - 1 - \frac{\sqrt{2}}{2}}{\delta}, \frac{\delta - 1 + \frac{\sqrt{2}}{2}}{\delta}\right\}  & \frac{b}{2} > c \\[0.5cm]
  q < \left\{\frac{\delta - \frac{\sqrt{2}}{2}}{\delta}, \  \frac{\delta + \frac{\sqrt{2}}{2}}{\delta}\right\} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{b}{2}>c\) the ratio is smaller for \(q < \left\{\frac{\delta - 1 -
\frac{\sqrt{2}}{2}}{\delta}, \frac{\delta - 1 +
\frac{\sqrt{2}}{2}}{\delta}\right\}\), however, \(\frac{\delta - 1 -
\frac{\sqrt{2}}{2}}{\delta}\) is not a feasible root since it's always smaller
than 1, and thus \(q < \frac{\delta - 1 + \frac{\sqrt{2}}{2}}{\delta}\). For
infinitely repeated games, \(\delta \rightarrow 1\), this condition becomes \(q
< \frac{\sqrt{2}}{2}\). In the case of \(\frac{b}{2}<c\) there are two possible
roots. For repeated games that are repeated for a large number of turn such as
\(\delta \rightarrow 1\) the condition then becomes \(q < 1 -
\frac{\sqrt{2}}{2}\).

\section{Updating Payoffs based on the Last $m$ Rounds Payoffs of One Interaction}\label{section:m_two_n_one}

The second generalised case of the limited memory payoffs we discuss is that of
individuals updating based on their last $m$ rounds payoffs with one member. Let
\(\mathcal{U}^{m}= \{\underbrace{rrr\dots r}_{m}, \underbrace{rrr\dots s}_{m},
\dots, \underbrace{ppp\dots p}_{m}\}\) be the set of feasible payoffs of the
last $m$ rounds. The probability that a reactive strategy receives the payoffs
$u\!\in\! \mathcal{U}^{m}$ is given by
Proposition~\ref{proposition:last_m_rounds}.

\begin{Prop}\label{proposition:last_m_rounds} Consider a repeated game, with
  continuation probability $\delta$, between players with reactive strategies
  $s_1\!=\!(y_1, p_1, q_1$) and $s_2\!=\!(y_2,p_2,q_2)$ respectively. Let
  $\mathcal{U}^{m}$ denote the set of feasible payoffs in the last
  \(m\) rounds, and let \(\mathbf{u}^{m}\) be the corresponding payoff vector.
  Then the probability that the $s_1$ player receives the payoff $u\!\in\!
  \mathcal{U}^{m}$ in the very last two rounds of the game is given by,

  \begin{equation}
  \langle\mathbf{v}^{m}(s_1,s_2),\mathbf{u}^{m}\rangle, \text{ where } \mathbf{v}^{m} \in R^{4^{m}} \text{ is given by },
  \end{equation}
  \begin{equation}
    \mathbf{v}^{m}(s_1,s_2) = (1 - \delta) w_{a_1, a_2} \delta^2 \left[\mathbf{v_0}(I_4 - \delta M)^{-1}\right]_{a_1, a_2}, \quad  w_{a_1, a_2} \in M \ \forall \ a_1, a_2 \in \{1, 2, 3, 4\}.
  \end{equation}
\end{Prop}

\subsection{Last Round Payoff of Two Interactions}

In the special case of $m=2$ the stationary distribution \(\mathbf{v}^{2}(s_1,
s_2)\) is sixteen dimension instead of four dimensional where \(v_{1}(s_1, s_2)\) is
the long term probability that \(s_1\) and \(s_2\) mutually cooperated in the
last two rounds. Let the feasible payoffs a strategy can receive
be $\mathcal{U}^{2} = \{rr, rs, rt, rp, sr, \dots, pp\}$.
The role model and the learner interact with only one other member and so the
probability \(x\) remains the same as in Section~\ref{section:limited_memory}.

The probability that the number of mutants increases, and decreases respectively,
by one is now given by,

\begin{align}
\lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N} \cdot \sum_{u_{R},u_{M}\in\mathcal{U}^{2}} x(u_{R},u_{M})\cdot \rho\left(\frac{u_{R}}{2},\frac{u_{M}}{2}\right), \\
\lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N} \cdot \sum_{u_{R},u_{M}\in\mathcal{U}^{2}} x(u_{R},u_{M})\cdot \rho\left(\frac{u_{M}}{2},\frac{u_{R}}{2}\right).
\end{align}

We assume again that players consider their average payoff of the last two turns
when deciding to imitate another strategy or not.

\subsubsection*{Invasion Analysis}

We once again calculate how easily a single ALLD mutant can invade into a
resident population of GTFT player. When two GTFT players interact in the game,
their respective probabilities for each of the four outcomes in the last round
simplify to,

\begin{align*}
    v_{rr}(GTFT,GTFT) = & \delta^2, \text{ and }\\
    v_{i}(GTFT,GTFT) = &  0 \text{ for all other } i \in \mathcal{U}^{2}.
\end{align*}

On the other hand, if an ALLD player interacts with a GTFT player, the
respective probabilities according to Eq.~\ref{Eq:LastRound} become

\begin{align*}
    v_{tt} (ALLD,GTFT) & = \delta^{2} q \left(\delta q - \delta + 1\right), & 
    v_{tp}(ALLD,GTFT) & = - \delta^{2} \left(q - 1\right) \left(\delta q - \delta + 1\right), \\
    v_{pt} (ALLD, GTFT ) & = \delta^{3} q \left(1 - q\right), & 
    v_{pp} (ALLD, GTFT) & = \delta^{3} \left(q - 1\right)^{2} \text{ and }\\
    v_{i}(ALLD,GTFT) & =  0 \text{ for all other } i \in \mathcal{U}^{2}.
\end{align*}

As a consequence, we obtain the following probabilities \(x(u_1, u_2)\) that the
payoff of a randomly chosen GTFT player is \(u_1\) and that the payoff of the
ALLD player is \(u_2\),

\begin{align*}
  x(rr, tt) = & \frac{N-2}{N - 1} \delta^{4} q \left(N - 2\right) \left(\delta q - \delta + 1\right) &
  x(rr, tp) = & - \frac{N-2}{N - 1} \delta^{4} \left(N - 2\right) \left(q - 1\right) \left(\delta q - \delta + 1\right)\\
  x(rr, pt) = & - \frac{N-2}{N - 1} \delta^{5} q \left(N - 2\right) \left(q - 1\right) &
  x(rr, pp) = & \frac{N-2}{N - 1} \delta^{5} \left(N - 2\right) \left(q - 1\right)^{2}\\
  x(ss, tt) = & \frac{1}{N -1}\delta^{2} q \left(\delta q - \delta + 1\right) & 
  x(sp, tp) = & - \frac{1}{N -1}\delta^{2} \left(q - 1\right) \left(\delta q - \delta + 1\right) \\
  x(ps, pt) = & - \frac{1}{N -1}\delta^{3} q \left(q - 1\right) &
  x(pp, pp) = & \frac{1}{N -1}\delta^{3} \left(q - 1\right)^{2}\\
  x(u_1, u_2) = &  0 \text{ for all other payoff pairs } (u_1, u_2). &
\end{align*}

We now calculate the ratio of transition probabilities as

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
  $\frac{\lambda^{+}}{\lambda^{-}} =
  \frac{\frac{\delta^{2} \left(N - 2\right)}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- b + c\right)}} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta c}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} + c\right)}}\right) + \frac{1}{N-1}\left(\frac{\delta \left(1 - q\right)^{2}}{2} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(b + c\right)}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} + \frac{c}{2}\right)}}\right)}
  {\frac{\delta^{2} \left(N - 2\right)}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{1 + e^{- \beta \left(b - c\right)}} + \frac{q \left(\delta q - \delta + 1\right)}{e^{\beta c} + 1} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} - c\right)}}\right) + \frac{1}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{2} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(- b - c\right)}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} - \frac{c}{2}\right)}}\right)}
  $}
\end{equation}

In the limit of strong selection \(\beta \rightarrow \infty\) and large
populations \(N \rightarrow \infty \) we obtain three expressions depending on
the cost-benefit relationship. We note that for \(\frac{b}{2}=c\) the result
remains the same expression as in the case of \(m=n=1\).

\begin{equation}
\frac{\lambda^{+}}{\lambda^{-}} = 
\begin{cases}
  - \frac{q \left(\delta q - \delta + 1\right)}{\left(q - 1\right) \left(\delta q + 1\right)}  & \frac{b}{2} > c \\[0.5cm]
  - \frac{\delta q - \delta + q + 1}{\left(\delta + 1\right) \left(q - 1\right)}  & \frac{b}{2} = c \\[0.5cm]
  - \frac{\delta q^{2} - 2 \delta q + \delta - 1}{\delta \left(q - 1\right)^{2}} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{\lambda^{+}}{\lambda^{-}} < 1\):

\begin{equation}
\begin{cases}
  q \in \left\{\frac{\delta - \sqrt{\delta^{2} + 1} - 1}{2 \delta}, \  \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\right\}  & \frac{b}{2} > c \\[0.5cm]
  q \in \left\{1 - \frac{\sqrt{2}}{2 \sqrt{\delta}}, \  1 + \frac{\sqrt{2}}{2 \sqrt{\delta}}\right\} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{b}{2}>c\) the ratio is smaller for
\(q < \left\{\frac{\delta - \sqrt{\delta^{2} + 1} - 1}{2 \delta}, \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\right\}\)
, however, the first root is not a feasible root since it's always smaller
than 1, and thus \(q < \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\). For
infinitely repeated games, \(\delta \rightarrow 1\), this condition becomes \(q
< \frac{\sqrt{2}}{2}\). In the case of \(\frac{b}{2}<c\) there are two possible
roots. For repeated games that are repeated for a large number of turn such as
\(\delta \rightarrow 1\) the condition then becomes \(q < 1 -
\frac{\sqrt{2}}{2}\).

Note that as \(\delta \rightarrow 1\) the condition for which condition
cooperators can avoid invasion is the same for the case of \(m=2\) and \(n=2\).

\section{Updating Payoffs based on the last two rounds payoff of two interactions (\(m=2\) and \(n=2\))}\label{section:m_two_n_two}

The final extension to the limited memory framework we consider is that of
increasing the number of rounds and the number of interactions. For this case we
need to consider a combination of the methods we presented in
Section~\ref{section:m_one_n_two} and Section~\ref{section:m_two_n_one}. As an
example consider the case of \(m=n=2\). The probability that the number of
mutants increases, and decreases respectively, by one is now given by,

\begin{align}
  \lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R1},u_{M1}\in\mathcal{U}^{2}} \quad \sum_{u_{R2},u_{M2}\in\mathcal{U}^{2}} x^{2}((u_{R1}, u_{R2}),(u_{M1}, u_{M2}))\cdot \rho\left(\frac{u_{R1} + u_{R2}}{2}, \frac{u_{M1} + u_{M2}}{2}\right), \\
  \lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R1},u_{M1}\in\mathcal{U}^{2}} \quad \sum_{u_{R2},u_{M2}\in\mathcal{U}^{2}} x^{2}((u_{R1}, u_{R2}),(u_{M1}, u_{M2}))\cdot \rho\left(\frac{u_{M1} + u_{M2}}{2}, \frac{u_{R1} + u_{R2}}{2}\right).
\end{align}

Though we do not carry any further analytical exploration of this case, in the
next section we present simulation results when the updating payoffs of the
pairwise process depend on the limited memory framework with \(m=n=2\), as well
as the rest of the cases we have discussed so far.

\section{Simulation Results on the Pairwise Comparison Process}\label{section:simulation_results}

We simulate the evolutionary process described in
Algorithm~\ref{algorithm:pairwise_comparison} for the different updating
mechanisms we have described in
Sections~\ref{section:perfect_memory}-\ref{section:m_two_n_two}. For each
approach we performed an independent run of the process and for each time step we
recorded the current resident population \((y, p, q)\). The results are shown in
Figure~\ref{fig:expected_payoffs_results}. We observe that in most cases the
resident population consists either of defectors or conditional cooperators. A
conditional cooperator always cooperates if the co-player cooperated
($p\approx1$) and cooperates with a probability \(q\) if the co-player defected.
The most abundant conditional cooperators in each simulation differ as a
result of the updating payoffs. More specifically, in order for a resident
population of conditional cooperators to avoid being invaded they need to adopt
a different value of \(q\). For each method we have discussed this under the
invasion analysis subsection. 

In the cases of perfect memory the resident population adopts a \(q \leq 1 -
\frac{c}{b}=0.9\). In the limited memory case the generosity of a conditional
cooperator is independently of the benefit lower than \(\frac{1}{2}\). The rest
of the cases also condition on the cost benefit relationship. In these
simulations the cost of cooperation is set to 1 and the benefit to 10. As a
result, in the case of two interactions \(q \leq \frac{\delta - 1 +
\frac{\sqrt{2}}{2}}{\delta}=0.7068\), in the case of two rounds \(q \leq
\frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}=0.7069\), and in the last
case \(q \leq 1 - \frac{c}{b}=0.9\). The higher tolerance to defection results
in a more cooperative population. As a result the expected payoffs allow for the
most cooperative population. Between the limited memory approaches, we observe a
big jump in the cooperation rate when we allow for more information
(in the form of interactions or rounds). We hypothesise that as we
allow for more information the results will tend to the case of perfect memory.

\begin{figure}[!htbp]
    \centering 
    \includegraphics[width=\textwidth]{static/more_memory_heatmaps_donation_game_with_illustrations.pdf}
    \caption{\textbf{Evolutionary dynamics with difference updating approach.}
    From left to right, we present result on the following updating payoffs
    cases; the expected payoffs (perfect memory), the last round payoff
    from one interaction (limited memory), the last round payoff from two
    interactions, the last two rounds payoffs from one interaction, the
    last two rounds payoffs from two interactions. We run each simulation for \(T
    = 10^7\) time steps. For each time step we recorded the current resident
    population \((y, p, q)\). Since \(\delta \rightarrow 1\) we do not report the
    players' initial cooperation probability \(y\). The graphs show how often the
    resident population chooses each combination \((p, q)\) of conditional
    cooperation probabilities in the subsequent rounds. In both cases players
    update based on their expected payoffs.}\label{fig:expected_payoffs_results}
\end{figure}

\section{Expected and Last Round Updating Payoffs for Memory One Strategies}\label{section:memory_one}

So far we have assumed that individuals can adopt reactive strategies.
To demonstrate that our results hold for higher memory strategies here we
present results for the expected payoffs, and the last round payoff when members
use memory-one strategies. Memory-one strategies consider the outcome of the
previous round to decide on an action. There are four possible outcome in each
round; \((C, C), (C, D), (D, C), (D, D)\). A memory-one strategy \(s\) can be
written as a five-dimensional vector \(s=(y, p_1, p_2, p_3, p_4)\). The
parameter \(y\) is the probability that the strategy opens with a cooperation
and \(p_1\), \(p_2\), \(p_3\), \(p_4\) are the probabilities that the strategy
cooperates for each of the possible outcomes of the last round.

We perform four separate simulations where we differ the updating payoff and the
benefit of cooperation \(b\). The results for a low value of benefit are given
in Figure~\ref{fig:memory_one_low_benefit}, and for a high benefit in
Figure~\ref{fig:memory_one_high_benefit}. We verify that even when individuals
are allowed to use memory-one strategies, the cooperation rate is higher
in the perfect memory approach compared to the limited memory.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=\textwidth]{static/memory_one_results_low_benefit.pdf}
  \caption{\textbf{Evolutionary dynamics results for memory-one strategies for low benefit.}
  We perform two independent simulations. In one simulation individuals use
  expected payoffs and in the other the last round one interacts when they
  update their strategies. We run each simulation for \(T = 10^8\) time steps.
  For each time step, we have recorded the current resident population, who is
  now of the form  \((y, p_1, p_2, p_3, p_4)\) In the left panel we report the
  cooperation rates for each simulation. It can be shown than even for
  memory-one strategies expected payoffs result in a more cooperative
  population. The right panel reports the most abundant strategy of each
  simulation. Abundance is the number of mutants a strategy can repel before
  being invaded. The most abundant strategies have some similarities,
  namely, \(p_1 \approx 1\), \(p_3 \approx 0\) and \(p_4 > \frac{1}{2}\). There are also differences, in the latter
  case a strategy is more likely to open with cooperation and their tolerance to
  a \((C, D)\) outcome is almost zero. A difference between the strategies is their
  abundance. In the expected payoffs case a strategy can repel a way greater
  number of mutants. In the case of last round payoffs strategies become less
  robust. Parameters: \(N =100, c=1, b=3, \beta=1\).}\label{fig:memory_one_low_benefit}
\end{figure}


\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=\textwidth]{static/memory_one_results_high_benefit.pdf}
  \caption{\textbf{Evolutionary dynamics results for memory-one strategies for high benefit.}
  We perform two independent simulations. In the case of high benefit expected
  payoffs again result in a more cooperative population. The right panel reports
  the most abundant strategy of each simulation. Abundance is the number of
  mutants a strategy can repel before being invaded. For the expected payoffs he
  most abundant is that of win-stay lose-shift. However in the later case the
  most abundant strategy is a strategy with no tolerance to one defection, and
  it cooperates with a probability 0.5 after a mutual defection.  In the expected
  payoffs case strategies are more robust.  Parameters: \(N =100, c=1, b=10,
  \beta=1\).}\label{fig:memory_one_high_benefit}
\end{figure}


\section{Expected and Last Round Updating Payoffs for High Mutation ($\mu \neq 0$)}\label{section:mutation}

In this section we evaluate the main result of this work for \(\mu \neq 0\).
Namely, we explore the evolved population when individuals use perfect and
limited updating payoff memory for different values of \(\mu\). We perform five
independent runs of the pairwise process described in
Section~\ref{section:pairwise_comparison}, and at each time step we record the
average player \(\bar{s}=(\bar{y}, \bar{p}, \bar{q})\). The average cooperation
of the resident population for different values of mutation are shown in
Figure~\ref{fig:mutation}. The cooperation rate in the case of perfect
memory is always higher compared to the limited memory regardless of the
mutation value. For mutation value of 1 the processes become random and
this results to a cooperation rate of \(\frac{1}{2}\) in both simulations.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.5\textwidth]{static/mutation_perfect_and_limited_memory_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics results for perfect and limited memory
  for different mutation values.}
  We perform five independent simulations. Simulations are run
  for $T\!=\!4\times 10^7$ time steps for each parameter. In each time step
  we introduce a new mutant with a probability \(\mu\), and we then select
  two random players to serve as the role model and the learner. The learner
  adopts the strategy of the role model with a probability \(\rho(\pi_{L}, \pi_{RM})\) where the
  updating payoffs depend on the method. In the case of perfect memory
  the expected payoffs are used and in the case of limited memory the
  last round payoff against one opponent. We plot the average cooperation rate
  within the resident population for each value of \(\mu\). For \(\mu=1\)
  the process becomes random and so the cooperation rates are 0.5. For the rest
  of the mutation values the perfect memory payoffs once again overestimate the
  evolved cooperation, confirming the results of low mutation. Parameters: \(N
  =100, c=1, b=10, \beta=1\).}\label{fig:mutation}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{document}