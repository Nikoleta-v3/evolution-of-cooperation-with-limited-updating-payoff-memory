\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,array,eurosym,multirow}
\usepackage[sort&compress]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage[ruled,vlined]{algorithm2e}
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.3}
\newcommand{\R}{\mathbb{R}}

\usetikzlibrary{decorations.pathreplacing, backgrounds, fit, calc, matrix, positioning}

\definecolor{darkblue}{rgb}{0,0,.8}
\definecolor{darkgreen}{rgb}{0,0.5,0.1}
\definecolor{lightapricot}{rgb}{0.99, 0.84, 0.69}
\newcommand{\matlabfunc}[1]{\textcolor{darkblue}{#1}}
\newcommand{\matlabcomment}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\christian}[1]{\textcolor{blue}{\textbf{CH}: #1}}
\newcommand{\alex}[1]{\textcolor{red}{\textbf{AL}: #1}}

\definecolor{expectedcolor}{rgb}{0.1791464821222607, 0.49287197231833907, 0.7354248366013072}
\definecolor{oneone}{rgb}{0.8503344867358708, 0.14686658977316416, 0.13633217993079583}
\definecolor{onetwo}{rgb}{0.18246828143021915, 0.5933256439830834, 0.3067589388696655}
\definecolor{twoone}{rgb}{0.4488427527873895, 0.3839600153787005, 0.6738792772010764}
\definecolor{twotwo}{rgb}{0.8871510957324106, 0.3320876585928489, 0.03104959630911188}


%% Adding shortcut commands to refer to our figures %%
\newcommand{\FigEvoProc}{{\bf Fig.~1}} \newcommand{\FigInvAnalysis}{{\bfFig.~2}} \newcommand{\FigResultsOverPara}{{\bf Fig.~3}}


\titleformat{\section}{\sffamily \fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sffamily
\fontsize{11.5}{11.5}\bfseries}{\thesubsection}{1em}{}

\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{treenode/.style = {align=center, inner sep=0pt, text centered,
  font=\sffamily}, arn_n/.style = {treenode, circle, white,
  font=\sffamily\bfseries, draw=black, inner sep=-6pt, fill=black, text
  width=1.5em},% arbre rouge noir, noeud noir
  arn_r/.style = {treenode, circle, red, text width=1.5em, very thick, inner
    sep=4pt},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black, minimum width=0.5em, minimum
    height=0.5em}% arbre rouge noir, nil
}

\newtheoremstyle{plainCl1}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newtheoremstyle{plainCl2}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{$'$.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newcommand{\splitatcommas}[1]{%
  \begingroup
  \begingroup\lccode`~=`, \lowercase{\endgroup \edef~{\mathchar\the\mathcode`,
    \penalty0 \noexpand\hspace{0pt plus 1em}}%
  }\mathcode`,="8000 #1%
  \endgroup
}

\theoremstyle{plainCl1}
\newtheorem{Claim}{Claim}
\newtheorem{Thm}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem*{Lem}{Lemma}
\newtheorem{Cor}{Corollary}
\newtheorem*{Def}{Definition}

\theoremstyle{plainCl2}
\newtheorem{Claim2}{Claim}

\title{\bf  \sffamily \LARGE Supplementary Information: Evolution of cooperation among individuals with
limited updating payoff memory\\}
\date{}
\author{Nikoleta E. Glynatsi, Christian Hilbe, Alex McAvoy}

\begin{document}
\maketitle

Section~\ref{section:pairwise_comparison} gives a brief overview of the pairwise
comparison process. The pairwise comparison process consists of three phases;
(1) the mutation phase (2) the game phase and (3) the update phase. In the update
phase an individual adopts the strategy of another individual based on their
``updating payoffs''. In Section~\ref{section:updating_payoffs} we
describe the conventional approach for calculating updating payoffs and we
present our new approaches.

\section{Pairwise comparison process}\label{section:pairwise_comparison}

Pairwise comparison process is a stochastic process for modelling the evolution
of a finite population. The process starts with assigning all individuals of the
population the same strategy. A strategy is a set of rules of how an individual
should behave in an interaction with another individual. Each elementary time
step of the process consists of three phases; (1) the \textbf{mutation phase}
(2) the \textbf{game phase} and (3) the \textbf{update phase}. These are summarised
in Figure~\ref{fig:pairwise_phases}.

In the \textbf{mutation phase} one individual is
chosen to switch to a new mutant strategy with a probability \(\mu\).
In the \textbf{game phase} individuals are randomly matched with other
individuals in the population and they engage in a repeated game where each
subsequent turn occurs with a fixed probability \(\delta\). At each turn the
individuals decide on an action based on their strategies.
In repeated games there are infinitely many strategies, however, it is commonly
assumed that individuals can only choose strategies from a restricted set. One
such set is that of reactive strategies. A reactive
strategy considers only the previous action of the other player, and thus, a
reactive strategy \(s\) can be written as a three-dimensional vector \(s=(y, p,
q)\). The parameter \(y\) is the probability that the strategy opens with a
cooperation and \(p\), \(q\) are the probabilities that the strategy cooperates
given that the opponent cooperated and defected equivalently.

In the \textbf{update stage} two individuals are randomly selected. From
the two individuals, one  serves as the `learner' and the other as the `role
model'. The learner adopts the role model's strategy with a probability \(\rho\)
given by,

\begin{equation} \label{Eq:rho}
    \rho(\pi_{L}, \pi_{RM}) = \frac{1}{1\!+\! e^{\!-\!\beta (\pi_{RM}\!-\! \pi_{L})}}.
\end{equation}

\(\pi_{L}\) and \(\pi_{RM}\) are the updating payoffs of the learner and the
role model respectively. The updating payoffs are a measure of how successful
individuals are in the current standing of the population. The parameter
\(\beta\) is known as the selection strength, namely, it shows how important the
payoff difference is when the learner is considering adopting the strategy of
the role model.

\begin{figure}[!htbp]
    \centering
    \includestandalone[width=\textwidth]{static/pairwise_comparison_process_diagram}
    \caption{\textbf{Pairwise comparison process phases.} \textbf{A) Initialisation} The process begins
    with a finite population where each member is assigned a given strategy.
    Each color represents a different strategy, and the members are labelled by
    letters. \textbf{B) Mutation phase.} An individual is selected (in the
    example individual A) and with a given probability \(\mu\) that individual
    adopts a new strategy. \textbf{C) Game phase.} Individuals are
    selected to interact in a repeated social dilemma with other individuals. We
    demonstrate the case where individuals B and F have been selected to
    interact. They use the reactive strategies \(s_{B} = (y_B, p_B, q_B)\) and
    \(s_{F} = (y_F, p_F, q_F)\) respectively. The opening moves depend on their
    \(y_i\) probability. In turn 1, individual B cooperated,
    thus, F cooperates with a probability \(p_{F}\) in turn 2. On the opposite,
    individual F defected in turn 1, and so B cooperates in the next turn with
    a probability \(q_{B}\). At each turn there is a probability \(\delta\) that a
    subsequent turn will occur, and with a probability (\(1- \delta\)) the interaction ends.
    \textbf{D) Update phase.} At the updating phase two individuals are chosen;
    one serves as the role of the learner and the other one as the 
    role model. In our example C adopts D's strategy with a probability
    \(\rho(\pi_{C}, \pi_{D})\) where \(\pi_{C}, \pi_{D}\) denote the updating
    payoffs of the individuals.}\label{fig:pairwise_phases}
\end{figure}

This elementary step of the process (mutation, game and update phases) is
repeated for a large number of time steps, and at each time step we record the
state of the population.

\subsection{Low mutation \(\mu \rightarrow 0\)}

In the case of low mutation (\(\mu \rightarrow 0\)) we assume that mutations are
rare. In fact, so rare that only two different strategies can be present in the
population at any given time. The case of low mutation is vastly adopted because
it allows us to explicitly calculate the fixation probability of a newly
introduced mutant.

More specifically, the process again starts with a population where all members are of the same
strategy. At each step one individual adopts a mutant strategy randomly selected
from the set of feasible strategies. The fixation probability \(\phi_{M}\) of the
mutant strategy can be calculated explicitly,

\begin{equation}\label{eq:appendix_fixation_probability}
    \varphi_{M} = \frac{1}{1+\sum\limits_{i=1}^{N-1}\prod\limits_k^i \frac{\lambda^-_k}{\lambda^+_k}},
\end{equation}

where \(\lambda^-_k, \lambda^+_k\) are the probabilities that the number of
mutants decreases and increases respectively, \(N\) is the size of the
population, and \(k\) is the number of mutants. The probabilities \(\lambda^-_k
\text{ and } \lambda^+_k\) depend on the updating payoffs of the mutant and the
resident strategies. Depending on the fixation probability \(\phi_{M}\) the
mutant either fixes (becomes the new resident) or goes extinct. Regardless, in
the elementary time step another mutant strategy is introduced to the
population. We iterate this elementary population updating process for a large
number of mutant strategies and we record the resident strategies at each time
step. The process is summarised by Algorithm~\ref{algorithm:pairwise_comparison}.

\begin{algorithm}[!htbp]
  \SetAlgoLined
  $N \leftarrow$ population size\;
  $k \leftarrow 1$\; resident
   $\leftarrow$ starting resident\;
   \While{ $t <$ maximum number of steps}
   {mutant $\leftarrow$ random strategy\;
   fixation probability $\leftarrow \varphi_M $\;
   \If{$\varphi_M >$ random: $i \rightarrow [0,1]$}
   {resident $\leftarrow$ mutant;}}
   \caption{Evolutionary process}\label{algorithm:pairwise_comparison}
\end{algorithm}

Most of the results we present in this work consider the case of low mutation,
however, we have also verified that the main result holds in the case of
high mutation rates (Section~\ref{section:mutation}).

\section{Updating Payoffs}\label{section:updating_payoffs}

The updating payoffs depend on the interactions of the individuals at the game
phase. In this work we assume that in the game phase individuals are matched in
pairs and that they participate in a repeated 2 person donation game. In the
donation game there are two actions: cooperation (\(C\)) and defection (\(D\)).
By cooperating a player provides a benefit \(b\) to the other player at their
cost \(c\), with \(0 < c < b\). Thus the payoffs for a player in each turn are,

\begin{equation}
    \begin{blockarray}{ccc}
        & \text{cooperate} & \text{defect} \\
        \begin{block}{c(cc)}
            \text{cooperate} & b - c & -c \\
            \text{defect} & b & 0 \\
        \end{block}
    \end{blockarray}.
\end{equation}

Let \(\mathbf{u} = (b-c, -c, b, 0)\) be payoffs in a vector format, and let \(\mathcal{U}
= \{r, s, t, p\}\) denote the set of feasible payoffs, where \(r\) denotes the
payoff of mutual cooperation, \(s\) the sucker's payoff, \(t\) the temptation to
defect payoff, and \(p\) the punishment payoff.

In the following subsections we present several approaches for calculating
the updating payoffs. Initially, we discuss the conventional approach of the
expected payoffs and afterwards we present our new approach.

\subsection{Updating Payoffs based on the expected payoffs}\label{section:perfect_memory}

The expected payoffs are the conventional payoffs used in the updating stage. We
refer to this approach as the perfect memory approach. They are defined as the
mean payoff of an individual in a well-mixed population that engages in an
infinitely repeated games with all other population members. In an infinitely
repeated game the payoff of a reactive strategy can explicitly be calculated
using a markovian approach~\cite{nowak:APC:1989}. Namely, assume two reactive
strategies $s_1\!=\!(y_1, p_1, q_1$) and $s_2\!=\!(y_2,p_2,q_2)$, their play can
be defined as a Markov process with the transition matrix \(M\),

\begin{equation}\label{eq:transition_matrix}
  \input{static/matrix.tex}.
\end{equation}

The stationary vector \(\mathbf{v}(s_1,s_2)\),
which is the solution to \(\mathbf{v}(s_1,s_2) M = \mathbf{v}(s_1,s_2)\), combined with the payoff vector
\(\mathbf{u}\), yields the game stage outcome for each strategy,

\[\langle\mathbf{v}(s_1,s_2),\mathbf{u}\rangle \quad  \text{ and } \quad  \langle\mathbf{v}(s_2,s_1), \mathbf{u}\rangle.\]

In the case of low mutation there can be only one type of mutant strategy in the
population. So in a population of size \(N\), there will be \(k\) mutants and
\(N - k\) residents, whose strategies we denote respectively as \(s_M =(y_M,
p_M, q_M)\) and \(s_R = (y_R, p_R, q_R)\).
The expected payoffs of a resident (\(\pi_R\)) and for a
mutant (\(\pi_M\)) are give by,

\begin{equation} \label{Eq:ExpPay}
  \begin{array}{lcrcr}
  \displaystyle \pi_R & = &\displaystyle \frac{N\!-\!k\!-\!1}{N-1}\cdot \langle\mathbf{v}(s_R,s_R),\mathbf{u}\rangle	&+	&\displaystyle\frac{k}{N-1}\cdot \langle\mathbf{v}(s_R,s_M),\mathbf{u}\rangle,\\[0.5cm]
  \displaystyle \pi_M & = &\displaystyle\frac{N-k}{N-1}\cdot \langle\mathbf{v}(s_M,s_R),\mathbf{u}\rangle&+	&\displaystyle\frac{k-1}{N-1}\cdot \langle\mathbf{v}(s_M,s_M),\mathbf{u}\rangle.\\
  \end{array}
\end{equation}

\(\frac{N\!-\!k\!-\!1}{N-1}\) is the probability that a resident meets another
resident and \(\frac{k}{N-1}\) is the probability that a resident meets a
mutant. Likewise, \(\frac{N-k}{N-1}\) is the probability that a mutant interacts
with a resident and \(\frac{k-1}{N-1}\) the probability that the mutants
interacts with a mutant.

The number of mutants in the population increases if a resident adopts the strategy
of a mutant, and decreases if a mutant adopts the strategy of a resident. The
probabilities that the number of mutants decreases and increases,
\(\lambda^-_k\) and \(\lambda^+_k\), are now explicitly defined as,

\begin{align*}
  \lambda^-_k \!=\!\rho(\pi_M, \pi_R) \quad \text{ and } \quad \lambda^+_k \!=\!\rho(\pi_R, \pi_M).
\end{align*}

\subsubsection*{Simulation Results based on Expected Payoffs}

We simulate the evolutionary process, as described in
Algorithm~\ref{algorithm:pairwise_comparison}, when individuals use reactive
strategies and update their strategies based on their expected payoffs. We
performed two independent runs of the process where we differed the benefit of
cooperation \(b\). The results are shown in
Figure~\ref{fig:expected_payoffs_results}. We observed that a higher value of
benefit results in a more cooperative population. For a low benefit the resident
population cooperates on average 52\% of the time. For a high benefit the
average cooperation increases to 98\%. For a low benefit the resident population
consists either of defectors or conditional cooperators, and for a high value of
benefit the population predominantly consists of conditional cooperators.A
conditional cooperator, otherwise known as a generous tit for tat (GTFT)
player, always cooperates if the co-player cooperated ($p\approx1$) and
cooperates with a probability \(q\) if the co-player defected. The conditional
cooperator strategy adopted by the population differs between the two
simulations, namely, a higher value of benefit results in higher values
of \(q\).

\begin{figure}[!htbp]
    \centering 
    \includegraphics[width=.70\textwidth]{static/expected_payoffs_donation_game.pdf}
    \caption{\textbf{Evolutionary dynamics under expected payoffs with
    low (left) and high (right) benefit.} We run the simulations for \(T =
    10^7\) time steps. For each time step we recorded the current resident
    population \((y, p, q)\). Since \(\delta \rightarrow
    1\) we do not report the players' initial cooperation probability \(y\).
    The graphs show how often the resident population chooses each combination
    \((p, q)\) of conditional cooperation probabilities in the subsequent
    rounds. In both cases players update based on their expected payoffs.
    ({\bf Low Benefit}) In
    the case where \(b=3\), the resident population either consists of defectors
    (with \(p \approx q \approx 0\)) or of conditional cooperators for which \(p
    \approx 1\) and \(q \leq 1 - \frac{1}{3}=0.7\). ({\bf High Benefit}) In the case where
    \(b=10\), the resident population typically applies a strategy for which \(p
    \approx 1\) and \(q \leq 1 - \frac{1}{10}=0.9\). Parameters: \(N =100, c=1,
    \beta=1\).
    }\label{fig:expected_payoffs_results}
\end{figure}

\subsubsection*{Invasion Analysis based on Expected Payoffs}

We can explicitly calculate the value of \(q\) which evolves. Namely, we will
calculate how easily a single defecting mutant can invade into a resident
population of GTFT players. Let GTFT\(= (1, 1, q)\), ALLD\(= (0, 0, 0)\), and
\(k = 1\). When two GTFT players interact in an infinitely repeated game, their
respective probabilities for each of the four outcomes in the last round
simplify to,

\begin{align*}
    \mathbf{v}(\text{GTFT}, \text{ALLD}) = (1, 0, 0, 0).
\end{align*}

On the other hand, if an ALLD player interacts with a GTFT player, the
respective probabilities become,

\begin{align*}
  \mathbf{v}(\text{ALLD}, \text{GTFT}) = (0, q, 0, (1 - q)).
\end{align*}

Using the above we can define the payoffs of a GTFT individual (resident)
and of the ALLD individual (mutant) follows,

\begin{align*}
  \displaystyle \pi_{\text{GTFT}} & = \displaystyle \frac{N\!-\!2}{N-1} (b - c)  -	\displaystyle\frac{q c}{N-1} \quad \text{ and } \quad \displaystyle \pi_{\text{ALLD}}  = \displaystyle b q.
\end{align*}

As a consequence, we can calculate the ratio of transition probabilities as,

\begin{align*}
    \frac{\lambda^{+}}{\lambda^{-}} & = \frac{\rho(\pi_{\text{GTFT}}, \pi_{\text{ALLD}})}{\rho(\pi_{\text{ALLD}}, \pi_{\text{GTFT}})}  = \frac{e^{-\beta \left(\frac{(N-2) (b-c)}{N-1} - q (b + \frac{c}{N-1})\right)}+1}
    {e^{-\beta\left(b q - \frac{b (N - 2)}{N-1} - \frac{c (N - 2 - q)}{N-1}\right)}+1}
\end{align*}

In particular, in the limit of strong selection \(\beta \rightarrow \infty\)
and large populations \(N \rightarrow \infty\), we obtain that the ratio is than
smaller to 1 if \(q \leq 1 - \frac{c}{b}\). Thus, ALLD is disfavored to invade
if \(q \leq 1 - \frac{c}{b}\). For \(q = 1 -\frac{c}{b}\) the probability that
the number of mutants increase by one equals the probability that the mutant
goes extinct.

\subsection{Updating Payoffs based on the last last \(m\) round(s) payoffs of \(n\) interaction(s)}

The aim of this work is to explore the effect of the updating payoffs on the
evolution of cooperation. To this end we introduce a new method for calculating
the updating payoffs and we compare the results of the new method to those of
the expected payoffs. In our approach we do not assume that an individual has a
perfect memory. Instead an individual remembers only a limited number of
interactions and limited outcomes from each interaction. In the expected payoffs
case the payoff of a pair depends on the average payoff they received over an
infinite number of turns. In our approach, the payoff of a pair depends on the
average payoffs they received in the last \(m\) turns. Furthermore, in expected
payoffs it is assumed that a player interacts with every member of the
population, whereas in our approach a player has \(n\) interactions.  Here we
present results for four cases; (\(m=1\) and \(n=1\)), (\(m=2\) and \(n=1\)),
(\(m=1\) and \(n=2\)), and (\(m=2\) and \(n=2\)).

\subsubsection{Updating Payoffs based on the Last Round Payoff of One Interaction (\(m=1\) and \(n=1\))}\label{section:m_one_n_one}

In this case an individual updates her/his strategy based on the last payoff
they received against one other member of the population. Initially, we define
the probability that a reactive strategy receives the payoff $u\!\in\!
\mathcal{U}$ in the very last round of the game against another reactive
strategy (Proposition~\ref{proposition:last_round}).

\begin{Prop}\label{proposition:last_round} Consider a repeated game, with
    continuation probability $\delta$, between players with reactive strategies
    $s_1\!=\!(y_1, p_1, q_1$)  and $s_2\!=\!(y_2,p_2,q_2)$ respectively. Then
    the probability that the $s_1$ player receives the payoff $u\!\in\!
    \mathcal{U}$ in the very last round of the game is given by
    $v_{u}(s_1,s_2)$, as given by Equation~(\ref{Eq:LastRound}).

    \begin{equation} \label{Eq:LastRound}
      \setlength{\arraycolsep}{1pt}
      \begin{array}{rcl}
    
      v_{r}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1y_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(q_1+l_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(q_2+l_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)} \times r,\\[1cm]
    
      v_{s}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{y_1\bar{y}_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(q_1+l_1\big((1\!-\!\delta)y_2+\delta q_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)} \times s,\\[1cm]
    
      v_{t}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1y_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(q_2+l_2\big((1\!-\!\delta)y_1+\delta q_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)} \times t,\\[1cm]
    
      v_{p}(s_1,s_2) &= &\displaystyle (1\!-\!\delta)\frac{\bar{y}_1\bar{y}_2}{1\!-\!\delta^2 l_1 l_2}+\delta \frac{\Big(\bar{q}_1+\bar{r}_1\big((1\!-\!\delta)y_2+\delta p_2\big)\Big) \Big(\bar{q}_2+\bar{r}_2\big((1\!-\!\delta)y_1+\delta p_1\big)\Big)}
      {\displaystyle(1\!-\!\delta l_1l_2)(1\!-\!\delta^2 l_1 l_2)} \times p.
      \end{array}
    \end{equation}

In these expressions, we have used the notation $l_i:=p_i\!-\!q_i$,
$\bar{y}_i\!=\!1\!-\!y_i$, $\bar{q}_i:=1\!-\!q_i$, and
$\bar{l}_i:=\bar{p}_i\!-\!\bar{q}_i=-l_i$ for $i\!\in\!\{1,2\}$.
\end{Prop}

Note that in the proposition we here we focus on the case of the donation game/prisoner's
dilemma but the result applies to any \(2 \times 2\) symmetric game.

\begin{proof}
Given a play between two reactive strategies with continuation probability
$\delta$. The outcome at turn \(t\) is given by,

\begin{equation}\label{eq:}
  (1 - \delta) \mathbf{v_0} \sum \delta^{t} M^{(t)},
\end{equation}

where $\mathbf{v_0}$ denotes the expected distribution of the four outcomes in
the very first round, and \(1- \delta\) the probability that the game ends. It
can be shown that,

\begin{align*}
  (1 - \delta) \mathbf{v_0} \sum \delta^{t} M^{(t)} & = (1 - \delta)(\mathbf{v_0} + \delta \mathbf{v_0} M + \delta^{2}\mathbf{v_0} M ^{2} + \dots )\\ 
   & = (1 - \delta)\mathbf{v_0} (1 + \delta M + \delta^{2}M ^{2} + \dots ) \text{ using standard formula for geometric series}\\ 
   & = (1 - \delta)\mathbf{v_0}(I_4 - \delta M)^{-1}
\end{align*}

where \((1 - \delta)\mathbf{v_0}(I_4 - \delta M)^{-1}\) is vector \(\in R^{4}\)
and it the probabilities for being in any of the outcomes \(CC, CD, DC, DD\) in
the last round. Combining this with the payoff vector \(u\) and some algebraic
manipulation we derive to the Equation~\ref{Eq:LastRound}.
\end{proof}

At each step of the evolutionary process we choose a role model and a learner to
update the population. In this case both the role model and the learner estimate
their fitness after interacting with a single member of the population, and so
there are five possible pairings at each step. They interact with each other with a
probability \(\frac{1}{N - 1}\), and they do not interact with other with a
probability \(1 - \frac{1}{N - 1}\). In the latter case, each of them can
interact with either a mutant or a resident. Both of them interact with a mutant
with a probability $\frac{(k-1)(k-2)}{(N-2)(N-3)}$ and both interact with a
resident with a probability $\frac{(N-k-1)(N-k-2)}{(N-2)(N-3)}$. The last two
possible pairings are that either of them interacts with a resident whilst the
other interacts with a mutant, and this happens with a probability
$\frac{(N-k-1)(k-1)}{(N-2)(N-3)}$. Given the possible pairings and
Proposition~\ref{proposition:last_round}, we define the probability that the
respective last round payoffs of two players \(s_1, s_2\) are given by $u_1$ and
$u_2$ as,

\begin{equation}\label{eq:Chi}
\setlength{\arraycolsep}{1pt}
\begin{array}{llrl}
x(u_1,u_2)	 &=&\displaystyle \frac{1}{N\!-\!1}\cdot  &v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F}\\[0.5cm]
&+	
&\displaystyle \left(1\!-\!\frac{1}{N\!-\!1}\right)  
&\left[ \frac{k\!-\!1}{N\!-\!2}\frac{k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) + 
 \frac{k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1)\right.\\[0.5cm]
&&&\left. +\frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) + 
 \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1)\right].
\end{array}
\end{equation}

The first term on the right side corresponds to the case that the learner and
the role model happened to be matched during the game stage, which happens with
probability $\frac{1}{(N\!-\!1)}$. In that case, we note that only those payoff
pairs can occur that are feasible in a direct interaction,
$\splitatcommas{(u_1,u_2)\in \mathcal{U}^2_F:=\big\{ (r, r), (s, t), (t, s), (p,
p) \big\}}$, as represented by the respective indicator function. Otherwise, if
the learner and the role model did not interact directly, we need to distinguish
four different cases, depending on whether the learner was matched with a
resident or a mutant, and depending on whether the role model was matched with a
resident or a mutant.

The probability that the number of mutants increases, and decreases respectively,
by one is now given by,

\begin{align}
\lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} x(u_{R},u_{M})\cdot \rho(u_{R},u_{M}), \\
\lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} x(u_{R},u_{M})\cdot \rho(u_{M},u_{R}).
\end{align}

In this expression, $\frac{(N\!-\!k)}{N}$ is the probability that the randomly
chosen learner is a resident, and $\frac{k}{N}$ is the probability that the role
model is a mutant. The sum corresponds to the total probability that the learner
adopts the role model's strategy over all possible payoffs $u_R$ and $u_M$ that
the two players may have received in their respective last rounds. We use
$x(u_R,u_M)$ to denote the probability that the randomly chosen resident
obtained a payoff of $u_R$ in the last round of his respective game, and that
the mutant obtained a payoff of $u_M$.

\subsubsection*{Simulation Results based on the Last Round Payoff of One Interaction}

We simulate the evolutionary process given that individuals now use the last
round payoff of one interaction to update their strategies. The results are
shown in Figure~\ref{fig:one_interaction_last_round_payoffs_results}. Similarly
to the results of the expected payoffs, a higher benefit results in a more
cooperative population. However, we note that the cooperation rate of the
evolved populations is lower compared to the case of expected payoffs, and that
the increase of the cooperation rate between the two simulations is smaller. The
residents population in both cases consist of defectors and conditional
cooperators with \(p \approx 1\) and  \(q \approx \frac{1}{2}\). Thus, the
evolved \(q\)s are smaller which is the reason why the population is less
cooperative. In the invasion analysis we demonstrate that in the case of the
last round payoff of one interaction, a conditional player needs to have \(q
\leq \frac{1}{2}\) to avoid being invaded by a defector regardless of the
benefit of cooperation.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.70\textwidth]{static/one_interaction_last_round_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics under one interaction last round
  payoffs with low (left) and high (right) benefit.} The cooperation rate
  regardless of the benefit of cooperation is less compared to that of the
  expected payoffs. The increase of cooperation between the low benefit case and
  the high benefit case is also smaller. In both cases the resident population
  either consists of defectors (with
  \(p \approx q \approx 0\)) or of conditional cooperators for which \(p \approx
  1\) and \(q \leq \frac{1}{2}\). Parameters: \(N =100, c=1, \beta=1, T=10^{7}\).
  }\label{fig:one_interaction_last_round_payoffs_results}
\end{figure}

\subsubsection*{Invasion Analysis based on the Last Round Payoff of One Interaction}

We once again calculate how easily a single ALLD mutant can invade into a
resident population of GTFT player. When two GTFT players interact in the game,
their respective probabilities for each of the four outcomes in the last round
simplify to,

\begin{align*}
    v_{r}(GTFT,GTFT) = 1, & \quad v_{t}(GTFT,GTFT) = 0, \\
    v_{s}(GTFT,GTFT) = 0, & \quad v_{p}(GTFT,GTFT) = 0.
\end{align*}

On the other hand, if an ALLD player interacts with a GTFT player, the
respective probabilities according to Eq.~\ref{Eq:LastRound} become

\begin{align*}
    v_{r}(ALLD,GTFT) & = 0, &  v_{s}(ALLD,GTFT) & = 0, \\
    v_{t} (ALLD, GTFT ) & = 1 - \delta + \delta q, &  v_{p} (ALLD, GTFT) & = \delta(1 - q).
\end{align*}

As a consequence, we obtain the following probabilities \(x(u_1, u_2)\) that the
payoff of a randomly chosen GTFT player is \(u_1\) and that the payoff of the
ALLD player is \(u_2\),

\begin{align*}
  x(r, t) = & \frac{N - 2}{N - 1} \cdot (1 - \delta + \delta q)\\
  x(r, r) = & \frac{N - 2}{N - 1} \cdot \delta (1 - q) \\
  x(s, r) = & \frac{1}{N - 1} \cdot (1 - \delta + \delta q) \\
  x(p, p) = & \frac{1}{N - 1} \cdot \delta (1 - q)
\end{align*}

We now calculate the ratio of transition probabilities as

\begin{equation*}
\frac{\lambda^{+}}{\lambda^{-}} = \frac{ \frac{N - 2}{N - 1}  \left( \frac{ \delta  \left(1 - q \right)}{1
+ e^{-  \beta  \left(b - c \right)}} +  \frac{ \delta q -  \delta + 1}{e^{ \beta
c} + 1} \right)  +  \frac{1}{N-1}  \left(\frac{ \delta  \left(1 - q \right)}{2} +
 \frac{ \delta q -  \delta + 1}{1 + e^{-  \beta  \left(- b - c \right)}}\right)}
 { \frac{N - 2}{N - 1}  \left( \frac{ \delta  \left(1 - q \right)}{1 +
e^{-  \beta  \left(- b + c \right)}} +  \frac{ \delta q -  \delta + 1}{1 + e^{-
 \beta c}} \right) +  \frac{1}{N -1} \left(\frac{ \delta  \left(1 - q \right)}{2} +
 \frac{ \delta q -  \delta + 1}{1 + e^{-  \beta  \left(b + c \right)}}\right)}
\end{equation*}

In particular, in the limit of strong selection \(\beta \rightarrow \infty\) and
large populations \(N \rightarrow \infty \), we obtain

\begin{equation*}
    \frac{\lambda^{+}}{\lambda^{-}} = \frac{1 - \delta + \delta q}{\delta(1 - q)}
\end{equation*}

This ratio is smaller than 1 (such that ALLD is disfavored to invade) if \(q <
1- 1/(2 \delta)\). For infinitely repeated games, \(\delta \rightarrow 1\), this
condition becomes \(q < 1/2\) (for \(q = 1/2\), the payoff of the ALLD player is
\(r > r\) for half of the time, and it is \(p < r\) for the other half. The
probability that the number of mutants increase by one equals the probability
that the mutant goes extinct).

\subsubsection{Updating Payoffs based on the last round payoff of two interactions (\(m=1\) and \(n=2\))}\label{section:m_one_n_two}

Here we consider the case where both the role model and the learner estimate
their updating payoff after interacting with two members of the population.
There are two stages of matching and there are twenty four possible pairs. In
the first stage the role model and the learner are matched together with a
probability $\frac{1}{N-1}$ or not with a probability $(1 - \frac{1}{N-1})$. If
they were matched together then in the second stage there are only four possible
outcomes; both of them interact with a mutant with a probability
$\frac{(k-1)(k-2)}{(N-2)(N-3)}$ and both interact with a resident with a
probability $\frac{(N-k-1)(N-k-2)}{(N-2)(N-3)}$. The last two possible pairs are
that either of them interacts with a resident whilst the other interacts with a
mutant, and this happens with a probability $\frac{(N-k-1)(k-1)}{(N-2)(N-3)}$.
In the later case, where they were not matched in the first stage, there are
four possible outcomes; both of them interact with a mutant and both interact
with a resident, either of them interacts with a resident whilst the other
interacts with a mutant. For each of the above pairs of the first stage there
are five possible pairs in the second stage; they interact with each other, both
of them interact with a mutant and both interact with a resident, either of them
interacts with a resident whilst the other interacts with a mutant.

The new possible pairs change how we define the probability that the
respective last round payoffs of two players \(s_1, s_2\) are given by \(u_1\)
and \(u_2\). The new probability denoted as \(\tilde{x}(u_1,u_2)\) is given by,

\begin{equation}
  \resizebox{.99\textwidth}{!}
  {%
$
  \setlength{\arraycolsep}{1pt}
  \begin{array}{llrl}
  \tilde{x}(u_1,u_2)	& = \displaystyle \frac{1}{N\!-\!1}\cdot v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F} \cdot A + \left(1\!-\!\frac{1}{N\!-\!1}\right) \left[ \right. \\[0.5cm] 
  & \left. v_{u_1}(s_1,s_2) _{u_2}(s_2,s_2)  \frac{\left(k - 2\right) \left(k - 1\right)}{(N\!-\!2)(N\!-\!3)}
  \left(\frac{1}{N\!-\!2}\cdot v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F} + (1 - \frac{1}{N - 2}) [B_1 + B_2 + B_3 + B_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_1}(s_1,s_2) _{u_2}(s_2,s_1)  \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F} + (1 - \frac{1}{N - 2}) [C_1 + C_2 + C_3 + C_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_1}(s_1,s_1) _{u_2}(s_2,s_2)  \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F} + (1 - \frac{1}{N - 2}) [D_1 + D_2 + D_3 + D_4]\right) + \right. \\[0.5cm] 
  & \left. v_{u_1}(s_1,s_1) _{u_2}(s_2,s_1)  \frac{\left(N - k - 2\right) \left(N - k - 1\right)}{(N\!-\!2)(N\!-\!3)} \left(\frac{1}{N\!-\!2}\cdot v_{u_1}(s_1,s_2)\cdot 1_{(u_1,u_2)\in \mathcal{U}^2_F} + (1 - \frac{1}{N - 2}) [E_1 + E_2 + E_3 + E_4]\right) \right]
  \end{array} $}
\end{equation}

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
$
  \setlength{\arraycolsep}{1pt}
  \begin{array}{llrl}
  A & = \left(1\!-\!\frac{1}{N\!-\!1}\right)  \left[ \frac{k\!-\!1}{N\!-\!2}\frac{k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) + 
   \frac{k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1) + \right. \\[0.5cm]
   & \left. \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{k\!-\!1}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) + 
   \frac{N\!-\!k\!-\!1}{N\!-\!2}\frac{N\!-\!k\!-\!2}{N\!-\!3} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1)\right] \\[0.5cm] 
   B_1 & = \frac{\left(k - 3\right) \left(k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) \quad
   B_2 = \frac{\left(k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) \\[0.5cm] 
   B_3 & = \frac{\left(k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1) \quad
   B_4 = \frac{\left(N - k - 2\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1) \\[0.5cm] 

   C_1 & = \frac{\left(k - 3\right) \left(k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) \quad
   C_2 = \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) \\[0.5cm] 
   C_3 & = \frac{\left(k - 2\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1) \quad
   C_4 = \frac{\left(N - k - 2\right)^{2}}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1) \\[0.5cm] 

   D_1 & = \frac{\left(k - 2\right)^{2}}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) \quad
   D_2 = \frac{\left(k - 2\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) \\[0.5cm] 
   D_3 & = \frac{\left(k - 1\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1) \quad
   D_4 = \frac{\left(N - k - 3\right) \left(N - k - 1\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1) \\[0.5cm]

   E_1 & = \frac{\left(k - 2\right) \left(k - 1\right)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_2) \quad
   E_2 = \frac{\left(k - 1\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_2) \\[0.5cm] 
   E_3 & = \frac{\left(k - 1\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_2) v_{u_2}(s_2,s_1) \quad
   E_4 = \frac{\left(N - k - 3\right) \left(N - k - 2\right)}{(N - 3) (N - 4)} v_{u_1}(s_1,s_1) v_{u_2}(s_2,s_1) \\[0.5cm] 
  \end{array}$}
\end{equation}

The first term on the right side corresponds to the case that the learner and
the role model happened to be matched during the first stage of pairing,
followed by them being paired with another member of the population on the
second stage. The second terms corresponds to the case that the learner and the
role model interact with a mutant with a probability ($\frac{\left(k - 2\right)
\left(k - 1\right)}{(N\!-\!2)(N\!-\!3)}$). In the seconds stage, they can either
interact with each other $\frac{1}{N - 2}$ or not $(1 - \frac{1}{N - 2})$. If
they do not interact with each other, then each of the following can happen:
both of them interact with a mutant with a probability
$\frac{(k-3)(k-2)}{(N-4)(N-3)}$ and both interact with a resident with a
probability $\frac{(N-k-1)(N-k-2)}{(N-3)(N-4)}$. The last two possible pairings
are that either of them interacts with a resident whilst the other interacts
with a mutant, and this happens with a probability
$\frac{(N-k-2)(k-1)}{(N-4)(N-3)}$, and so on.

The probability that the number of mutants increases, and decreases
respectively, by one is now given by,

\begin{align}\label{eq:ratio_limited}
\lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} \tilde{x}(u_{R},u_{M})\cdot \rho(u_{R},u_{M}), \\
\lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{u_{R},u_{M}\in\mathcal{U}} \tilde{x}(u_{R},u_{M})\cdot \rho(u_{M},u_{R}).
\end{align}

\subsubsection*{Simulation Results based on the Last Round Payoff of Two Interactions}

We simulate the evolutionary process when individuals update their strategies
based on the last round payoffs from two interactions with members of the
population, Figure~\ref{fig:one_interaction_last_round_payoffs_results}. For
both a low and a high benefit value the average cooperation rate has increased
compared to the last round against a single other member of the population. In
the case of low benefit the evolved population's cooperation rate is almost the
same as in the case of expected payoffs. However, the cooperation rate does not
increase as much as the expected payoffs for \(b=10\).

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.70\textwidth]{static/two_interactions_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics under two interactions last round
  payoffs with low (left) and high (right) benefit.}
  The cooperation rates for both the simulations have increased and are similar
  to the case of expected payoffs. The increase of the cooperation rate between
  the two cases however is smaller. In both cases the resident population
  either consists of defectors (with
  \(p \approx q \approx 0\)) or of conditional cooperators for which \(p \approx
  1\) and \(q \leq \frac{1}{2}\). Parameters: \(N =100, c=1, \beta=1, T=10^{7}\).
  }\label{fig:two_interactions_results}
\end{figure}

\subsubsection*{Invasion Analysis based on the Last Round Payoff of Two Interactions}

In a similar fashion we can calculate the condition for which a population of
GTFT players can not be invaded by an ALLD mutant. The ratio of transition
probabilities is given by,

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
$\frac{\lambda^{+}}{\lambda^{-}} =
\frac{\frac{N - 3}{N-1} \left(\frac{\delta^{2} \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- b + c\right)}} + \frac{2 \delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} + c\right)}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta c}}\right) 
+ \frac{2}{N-1} \left(\frac{\delta^{2} \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- \frac{b}{2} + \frac{c}{2}\right)}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \frac{\beta c}{2}}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta c}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta \left(\frac{b}{2} + c\right)}}\right)}
{\frac{N - 3}{N-1} \left(\frac{\delta^{2} \left(q - 1\right)^{2}}{1 + e^{- \beta \left(b - c\right)}} + \frac{2 \delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} - c\right)}} + \frac{\left(\delta q - \delta + 1\right)^{2}}{e^{\beta c} + 1}\right) + \frac{2}{N-1} \left(\frac{\delta^{2} \left(q - 1\right)^{2}}{1 + e^{- \beta \left(\frac{b}{2} - \frac{c}{2}\right)}} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{e^{\beta c} + 1} + \frac{\delta \left(1 - q\right) \left(\delta q - \delta + 1\right)}{e^{\frac{\beta c}{2}} + 1} + \frac{\left(\delta q - \delta + 1\right)^{2}}{1 + e^{- \beta \left(- \frac{b}{2} - c\right)}}\right)}
$}
\end{equation}

In the limit of strong selection \(\beta \rightarrow \infty\) and large
populations \(N \rightarrow \infty \) we obtain the following cases,

\begin{equation}
\frac{\lambda^{+}}{\lambda^{-}} = 
\begin{cases}
  - \frac{q \left(\delta q - \delta + 1\right)}{\left(q - 1\right) \left(\delta q + 1\right)}  & \frac{b}{2} > c \\[0.5cm]
  \frac{- \delta q + \delta - 1}{\delta \left(q - 1\right)}  & \frac{b}{2} = c \\[0.5cm]
  - \frac{\delta q^{2} - 2 \delta q + \delta - 1}{\delta \left(q - 1\right)^{2}} & \frac{b}{2} < c
\end{cases}
\end{equation}

We note that the relationship between the cost and benefit have an effect on how
generous a conditional cooperator must be to avoid invasion. In the case of
\(\frac{b}{2}=c\) the result remains the same expression as in the case of
\(m=n=1\). For the other two cases we show that for
\(\frac{\lambda^{+}}{\lambda^{-}} < 1\),

\begin{equation}
\begin{cases}
  q < \left\{\frac{\delta - 1 - \frac{\sqrt{2}}{2}}{\delta}, \frac{\delta - 1 + \frac{\sqrt{2}}{2}}{\delta}\right\}  & \frac{b}{2} > c \\[0.5cm]
  q < \left\{\frac{\delta - \frac{\sqrt{2}}{2}}{\delta}, \  \frac{\delta + \frac{\sqrt{2}}{2}}{\delta}\right\} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{b}{2}>c\) the ratio is smaller for \(q < \left\{\frac{\delta - 1 -
\frac{\sqrt{2}}{2}}{\delta}, \frac{\delta - 1 +
\frac{\sqrt{2}}{2}}{\delta}\right\}\), however, \(\frac{\delta - 1 -
\frac{\sqrt{2}}{2}}{\delta}\) is not a feasible root since it's always smaller
than 1, and thus \(q < \frac{\delta - 1 + \frac{\sqrt{2}}{2}}{\delta}\). For
infinitely repeated games, \(\delta \rightarrow 1\), this condition becomes \(q
< \frac{\sqrt{2}}{2}\). In the case of \(\frac{b}{2}<c\) there are two possible
roots. For repeated games that are repeated for a large number of turn such as
\(\delta \rightarrow 1\) the condition then becomes \(q < 1 -
\frac{\sqrt{2}}{2}\).

\subsection{Updating Payoffs based on the Last Two Rounds Payoff of One Interaction (\(m=2\) and \(n=1\))}\label{section:m_two_n_one}

In this section we consider the case of \(n=1\) and \(m=2\), thus individuals
update based on their last two rounds payoffs with one member. To this end we
define the probability that a reactive strategy receives the payoffs $u\!\in\!
\mathcal{\tilde{U}}$, for \(\mathcal{\tilde{U}}= \{rrr\dots r, rrr\dots s, \dots,
ppp\dots p\}\), in the last \(m\) rounds against another reactive strategy
(Proposition~\ref{proposition:last_m_rounds}).

\begin{Prop}\label{proposition:last_m_rounds} Consider a repeated game, with
  continuation probability $\delta$, between players with reactive strategies
  $s_1\!=\!(y_1, p_1, q_1$) and $s_2\!=\!(y_2,p_2,q_2)$ respectively. Let
  $\mathcal{\tilde{U}}$ denote the set of feasible payoffs in the last
  \(n\) rounds, and let \(\tilde{\mathbf{u}}\) be the corresponding payoff vector.
  Then the probability that the $s_1$ player receives the payoff $u\!\in\!
  \mathcal{\tilde{U}}$ in the very last two rounds of the game is given by,

  \begin{equation}
  \langle\mathbf{\tilde{v}}(s_1,s_2),\mathbf{\tilde{u}}\rangle, \text{ where } \mathbf{\tilde{v}} \in R^{4^{n}} \text{ is given by },
  \end{equation}
  \begin{equation}
    \mathbf{\tilde{v}}(s_1,s_2) = (1 - \delta) w_{a_1, a_2} \delta^2 \left[\mathbf{v_0}(I_4 - \delta M)^{-1}\right]_{a_1, a_2}, \quad  w_{a_1, a_2} \in M \ \forall \ a_1, a_2 \in \{1, 2, 3, 4\}.
  \end{equation}
\end{Prop}

The probability that the number of mutants increases, and decreases respectively
remains the same as in section~\ref{section:m_one_n_one} (Eq.~\ref{eq:ratio_limited}). The
frameworks differ in the game stage payoffs,

\begin{align}
  \lambda^+_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{\tilde{u}_{R},\tilde{u}_{M}\in\mathcal{\tilde{U}}} \tilde{x}(\tilde{u}_{R},\tilde{u}_{M})\cdot \rho(\tilde{u}_{R},\tilde{u}_{M}), \\
  \lambda^-_k=\frac{N\!-\!k}{N}\cdot \frac{k}{N}\cdot \sum_{\tilde{u}_{R},\tilde{u}_{M}\in\mathcal{\tilde{U}}} \tilde{x}(\tilde{u}_{R},\tilde{u}_{M})\cdot \rho(\tilde{u}_{M},\tilde{u}_{R}).
\end{align}

\subsubsection*{Simulation Results based on the Last Two Rounds Payoff of One Interaction}

The simulation results for this case are given in
Figure~\ref{fig:two_rounds_results}. Updating on the two rounds payoffs received
against a single opponent result to a lower cooperation rate compare to updating
based on one round against two opponents.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.70\textwidth]{static/two_rounds_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics under two interactions last round
  payoffs with low (left) and high (right) benefit.}\\Parameters: \(N =100, c=1, \beta=1, T=10 ^7\).
  }\label{fig:two_rounds_results}
\end{figure}

\subsubsection*{Invasion Analysis based on the Last Two Rounds Payoff of One Interaction}

In a similar fashion we can calculate the condition for which a population of
GTFT players can not be invaded by an ALLD mutant. The ratio of transition
probabilities is given by,

\begin{equation}
  \resizebox{.9\textwidth}{!}
  {%
  $\frac{\lambda^{+}}{\lambda^{-}} =
  \frac{\frac{\delta^{2} \left(N - 2\right)}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{1 + e^{- \beta \left(- b + c\right)}} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta c}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} + c\right)}}\right) + \frac{1}{N-1}\left(\frac{\delta \left(1 - q\right)^{2}}{2} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(b + c\right)}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} + \frac{c}{2}\right)}}\right)}
  {\frac{\delta^{2} \left(N - 2\right)}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{1 + e^{- \beta \left(b - c\right)}} + \frac{q \left(\delta q - \delta + 1\right)}{e^{\beta c} + 1} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(\frac{b}{2} - c\right)}}\right) + \frac{1}{N-1} \left(\frac{\delta \left(1 - q\right)^{2}}{2} + \frac{q \left(\delta q - \delta + 1\right)}{1 + e^{- \beta \left(- b - c\right)}} + \frac{\left(1 - q\right) \left(2 \delta q - \delta + 1\right)}{1 + e^{- \beta \left(- \frac{b}{2} - \frac{c}{2}\right)}}\right)}
  $}
\end{equation}

In the limit of strong selection \(\beta \rightarrow \infty\) and large
populations \(N \rightarrow \infty \) we obtain three expressions depending on
the cost-benefit relationship. We note that for \(\frac{b}{2}=c\) the result
remains the same expression as in the case of \(m=n=1\).

\begin{equation}
\frac{\lambda^{+}}{\lambda^{-}} = 
\begin{cases}
  - \frac{q \left(\delta q - \delta + 1\right)}{\left(q - 1\right) \left(\delta q + 1\right)}  & \frac{b}{2} > c \\[0.5cm]
  - \frac{\delta q - \delta + q + 1}{\left(\delta + 1\right) \left(q - 1\right)}  & \frac{b}{2} = c \\[0.5cm]
  - \frac{\delta q^{2} - 2 \delta q + \delta - 1}{\delta \left(q - 1\right)^{2}} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{\lambda^{+}}{\lambda^{-}} < 1\):

\begin{equation}
\begin{cases}
  q \in \left\{\frac{\delta - \sqrt{\delta^{2} + 1} - 1}{2 \delta}, \  \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\right\}  & \frac{b}{2} > c \\[0.5cm]
  q \in \left\{1 - \frac{\sqrt{2}}{2 \sqrt{\delta}}, \  1 + \frac{\sqrt{2}}{2 \sqrt{\delta}}\right\} & \frac{b}{2} < c
\end{cases}
\end{equation}

For \(\frac{b}{2}>c\) the ratio is smaller for
\(q < \left\{\frac{\delta - \sqrt{\delta^{2} + 1} - 1}{2 \delta}, \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\right\}\)
, however, the first root is not a feasible root since it's always smaller
than 1, and thus \(q < \frac{\delta + \sqrt{\delta^{2} + 1} - 1}{2 \delta}\). For
infinitely repeated games, \(\delta \rightarrow 1\), this condition becomes \(q
< \frac{\sqrt{2}}{2}\). In the case of \(\frac{b}{2}<c\) there are two possible
roots. For repeated games that are repeated for a large number of turn such as
\(\delta \rightarrow 1\) the condition then becomes \(q < 1 -
\frac{\sqrt{2}}{2}\).

\subsection{Updating Payoffs based on the last two rounds payoff of two interactions (\(m=2\) and \(n=2\))}

The last case we present in this work is the case of \(m = 2\) and \(n = 2\).
That is, an individual considers the last two rounds of payoffs he or she
received against two different members of the population. For this case we
combine the approaches described in sections~\ref{section:m_one_n_two}
and~\ref{section:m_two_n_one}. Here we only present results on the numerical
simulations, Figure~\ref{fig:two_rounds_opponents_results}. The evolved
cooperation rates are between the other cases and the expected payoffs. For a
low benefit we get results similar to the case where one the last round was
considered, however, in the high benefit case the evolved cooperation rate
increases to 91\% which is higher then any of the newly introduced approaches.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.70\textwidth]{static/two_rounds_opponents_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics under two interactions last round
  payoffs with low (left) and high (right) benefit.} In the cases of low and
  high benefit case the resident population either consists of defectors (with
  \(p \approx q \approx 0\)) or of conditional cooperators for which \(p \approx
  1\) and \(q \leq \frac{1}{2}\). Parameters: \(N =100, c=1, \beta=1\).
  }\label{fig:two_rounds_opponents_results}
\end{figure}

\newpage

\section{Expected and Last Round Updating Payoffs for Memory One Strategies}

So far we have considered the case where members can adopt reactive strategies.
To demonstrate that our results hold for higher memory strategies here we
present results for the expected payoffs, and the last round payoff when members
use memory-one strategies. Memory-one strategies consider the outcome of the
previous round to decide on an action. There are four possible outcome in each
round; \((C, C), (C, D), (D, C), (D, D)\). A memory-one strategy \(s\) can be
written as a five-dimensional vector \(s=(y, p_1, p_2, p_3, p_4)\). The
parameter \(y\) is the probability that the strategy opens with a cooperation
and \(p_1\), \(p_2\), \(p_3\), \(p_4\) are the probabilities that the strategy
cooperates for each of the possible outcomes of the last round.

We perform four separate simulations where we differ the updating payoff and the
benefit of cooperation \(b\). The results for a low value of benefit are given
in Figure~\ref{fig:memory_one_low_benefit}, and for a high benefit in
Figure~\ref{fig:memory_one_high_benefit}.

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=\textwidth]{static/memory_one_results_low_benefit.pdf}
  \caption{\textbf{Evolutionary dynamics results for memory-one strategies for low benefit.}
  We perform two independent simulations. In one simulation individuals use
  expected payoffs and in the other the last round one interacts when they
  update their strategies. We run each simulation for \(T = 10^8\) time steps.
  For each time step, we have recorded the current resident population, who is
  now of the form  \((y, p_1, p_2, p_3, p_4)\) In the left panel we report the
  cooperation rates for each simulation. It can be shown than even for
  memory-one strategies expected payoffs result in a more cooperative
  population. The right panel reports the most abundant strategy of each
  simulation. Abundance is the number of mutants a strategy can repel before
  being invaded. The most abundant strategies have some similarities,
  namely, \(p_1 \approx 1\), \(p_3 \approx 0\) and \(p_4 > \frac{1}{2}\). There are also differences, in the latter
  case a strategy is more likely to open with cooperation and their tolerance to
  a \((C, D)\) outcome is almost zero. A difference between the strategies is their
  abundance. In the expected payoffs case a strategy can repel a way greater
  number of mutants. In the case of last round payoffs strategies become less
  robust. Parameters: \(N =100, c=1, b=3, \beta=1\).}\label{fig:memory_one_low_benefit}
\end{figure}


\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=\textwidth]{static/memory_one_results_high_benefit.pdf}
  \caption{\textbf{Evolutionary dynamics results for memory-one strategies for high benefit.}
  We perform two independent simulations. In the case of high benefit expected
  payoffs again result in a more cooperative population. The right panel reports
  the most abundant strategy of each simulation. Abundance is the number of
  mutants a strategy can repel before being invaded. For the expected payoffs he
  most abundant is that of win-stay lose-shift. However in the later case the
  most abundant strategy is a strategy with no tolerance to one defection, and
  it cooperates with a probability 0.5 after a mutual defection.  In the expected
  payoffs case strategies are more robust.  Parameters: \(N =100, c=1, b=10,
  \beta=1\).}\label{fig:memory_one_high_benefit}
\end{figure}


\section{Expected and Last Round Updating Payoffs for High Mutation ($\mu \neq 0$)}\label{section:mutation}

In this section we evaluate the main result of this work for \(\mu \neq 0\).
Namely, we explore the evolved population when individuals use perfect and
limited updating payoff memory for different values of \(\mu\). We perform five
independent runs of the pairwise process described in
Section~\ref{section:pairwise_comparison}, and at each time step we record the
average player \(\bar{s}=(\bar{y}, \bar{p}, \bar{q})\).

\begin{figure}[!htbp]
  \centering 
  \includegraphics[width=.5\textwidth]{static/mutation_perfect_and_limited_memory_donation_game.pdf}
  \caption{\textbf{Evolutionary dynamics results for perfect and limited memory
  for different mutation values.}
  We perform five independent simulations. Simulations are run
  for $T\!=\!4\times 10^7$ time steps for each parameter. In each time step
  we introduce a new mutant with a probability \(\mu\), and we then select
  two random players to serve as the role model and the learner. The learner
  adopts the strategy of the role model with a probability \(\rho(\pi_{L}, \pi_{RM})\) where the
  updating payoffs depend on the method. In the case of perfect memory
  the expected payoffs are used and in the case of limited memory the
  last round payoff against one opponent. We plot the average cooperation rate
  within the resident population for each value of \(\mu\). For \(\mu=1\)
  the process becomes random and so the cooperation rates are 0.5. For the rest
  of the mutation values the perfect memory payoffs once again overestimate the
  evolved cooperation, confirming the results of low mutation. Parameters: \(N
  =100, c=1, b=10, \beta=1\).}\label{fig:mutation}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{document}