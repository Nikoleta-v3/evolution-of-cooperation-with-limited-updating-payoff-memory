\documentclass[11pt]{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL Revision/main.tex   Sat May  4 13:45:42 2024
%DIF ADD main.tex            Sat May  4 14:13:03 2024
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,color,array,eurosym,multirow}
\usepackage[sort&compress,comma,round,numbers]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{lineno}
%DIF 17d17
%DIF < \linenumbers
%DIF -------
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.3}
\newcommand{\R}{\mathbb{R}}

\newcommand{\todo}[1]{\textcolor{blue}{#1}}

\def\alld{\texttt{ALLD}}
\def\tft{\texttt{TFT}}
\def\gtft{\texttt{GTFT}}
\def\rolemodel{\text{RM}}
\def\learner{\text{L}}
\def\resident{\texttt{R}}
\def\mutant{\texttt{M}}
\def\strategy{s}
\def\esm{electronic supplementary material}

%% Adding shortcut commands to refer to our figures %%
\newcommand{\FigBaseResults}{Figure~\ref{fig:expected_and_stochastic_for_donation}}
\newcommand{\FigDependenceParameters}{Figure~\ref{fig:cooperation_rate_over_benefit_and_beta}} 
\newcommand{\FigHigherMemory}{Figure~\ref{fig:cooperation_rate_all_updating_payoffs}}

\newcommand{\FigInvasionAnalysis}{{Supplementary Figure~3}} 
\newcommand{\FigMemoryOneParameters}{Supplementary Figure~7}

\titleformat{\section}{\sffamily \fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sffamily
\fontsize{11.5}{11.5}\bfseries}{\thesubsection}{1em}{}

\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{treenode/.style = {align=center, inner sep=0pt, text centered,
  font=\sffamily}, arn_n/.style = {treenode, circle, white,
  font=\sffamily\bfseries, draw=black, inner sep=-6pt, fill=black, text
  width=1.5em},% arbre rouge noir, noeud noir
  arn_r/.style = {treenode, circle, red, text width=1.5em, very thick, inner
    sep=4pt},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black, minimum width=0.5em, minimum
    height=0.5em}% arbre rouge noir, nil
}

\newtheoremstyle{plainCl1}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newtheoremstyle{plainCl2}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{$'$.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec


\theoremstyle{plainCl1}
\newtheorem{Claim}{Claim}
\newtheorem{Thm}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem*{Lem}{Lemma}
\newtheorem{Cor}{Corollary}
\newtheorem*{Def}{Definition}

\theoremstyle{plainCl2}
\newtheorem{Claim2}{Claim}




\title{\bf  \sffamily \Large Evolution of reciprocity %among individuals\\ 
with limited payoff memory\\}
\date{}
\author{\parbox[c]{16cm}{\centering \onehalfspacing 
Nikoleta E. Glynatsi$^1$,  Alex McAvoy$^{2,3, \dagger}$, Christian Hilbe$^{1, \dagger}$\\ \quad \\
$^{\rm 1}$Max Planck Research Group on the Dynamics of Social Behavior,\\ Max Planck Institute for Evolutionary Biology, Pl\"{o}n, Germany \\
$^{\rm 2}$School of Data Science and Society, University of North Carolina at Chapel Hill,\\ Chapel Hill, NC 27599 \\
$^{\rm 3}$Department of Mathematics, University of North Carolina at Chapel Hill,\\ Chapel Hill, NC 27599 \\
$^{\rm \dagger}$ A.M. and C.H. contributed equally to this work.}}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF LISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
\maketitle


\begin{abstract}
\noindent
Direct reciprocity is a mechanism for the evolution of cooperation in repeated social interactions.
According to this literature, individuals naturally learn to adopt conditionally cooperative strategies if they have multiple encounters with their partner.
Corresponding models have greatly facilitated our understanding of cooperation, yet they often make strong assumptions on how individuals remember and process payoff information.
For example, when strategies are updated through social learning, it is commonly assumed that individuals compare their average payoffs.
This would require them to compute (or remember) their payoffs against everyone else in the population.
To understand how more realistic constraints influence direct reciprocity, we consider the evolution of conditional behaviors when individuals learn based on more recent experiences.
Even in the most extreme case that they only take into account their very last interaction, we find that cooperation can still evolve.
However, such individuals adopt less generous strategies, and they cooperate less often than in the classical setup with average payoffs.
Interestingly, once individuals remember the payoffs of two or three recent interactions, cooperation rates quickly approach the classical limit.
These findings contribute to a literature that explores which kind of cognitive capabilities are required for reciprocal cooperation.
While our results suggest that some rudimentary form of payoff memory is necessary, it suffices to remember a few interactions.
\end{abstract}



~\\
{\it Keywords:} Direct reciprocity; evolution of cooperation; evolutionary dynamics; repeated prisoner's dilemma; social learning



\clearpage
\newpage


%%%%%%%%%
%%  INTRO  %%
%%%%%%%%%

\section{Introduction}

%% INTRO: General introduction to evolutionary game theory and cognitive constrains

Evolutionary game theory describes the dynamics of populations when an individual's fitness depends on the traits or strategies of other population members~\citep{hofbauer1998evolutionary, nowak:Nature:2004, hauert2005game,Traulsen:PhilTrans:2022}.  
This theory can be used to describe the dynamics of animal conflict~\citep{maynard-smith:Nature:1973}, cancer cells~\citep{Stein:PTRS:2023}, and of cooperation~\citep{nowak:Science:2006}. 
Respective models translate strategic interactions into games~\citep{smith1982evolution}. 
These games specify how individuals (players) interact, which strategies individuals can choose, and what fitness consequences (or payoffs) the different strategies have. 
In addition, these models also specify the mode by which successful strategies spread over time. 
In models of biological evolution, individuals with a high fitness produce more offspring; in models of cultural evolution, such individuals are imitated more often. 
Although biological and cultural evolution are sometimes treated as equivalent, there can be important differences~\citep{Wu2015,Smolla:PTRS:2021,Denton:TPB:2022}. 
For example, models of biological evolution do not require individuals to have any particular cognitive abilities.
Here, it is the evolutionary process itself that biases the population towards strategies with higher fitness. 
In contrast, in models of cultural evolution, individuals need to be aware of the different strategies present in the population, and they need to identify those strategies with a higher payoff. 
As a consequence, evolutionary outcomes may depend on how easily different behaviors can be learned~\citep{Chatterjee:JTB:2012}, as well as on how readily payoffs can be compared.

%% INTRO: Literature on direct reciprocity and why social learning of strategies is hard

These difficulties to learn strategies by social imitation are particularly pronounced in models of direct reciprocity. 
This literature follows Trivers' insight that individuals have more of an incentive to cooperate in social dilemmas when they interact repeatedly~\citep{trivers1971evolution}. 
In repeated interactions, individuals can condition their behavior on their past experiences with their interaction partner. 
They may use strategies such as Tit-for-Tat~\citep{rapoport:book:1965,axelrod1981evolution} or Generous Tit-for-Tat~\citep{molander:jcr:1985,Nowak1992tit} to preferentially cooperate with other cooperators. 
Such conditional strategies approximate human behavior fairly well~\citep{fischbacher:EconL:2001,Rand:TCS:2013,DalBo:AER:2019,Montero-Porras:SciRep:2022,Rossetti:ETH:2023} and they have also been documented in several other species~\citep{Carter:PRSB:2013,Schweinfurth:AnBehav:2019,Voelkl:PNAS:2015}--although direct reciprocity is generally more difficult to demonstrate in animals~\citep{CluttonBrock:Nature:2009,Silk:CurrentBiology:2013,taborsky:CurrentBiology:2013}.
However, at the outset, it is not clear how easy it is to {\it learn} reciprocal strategies by social imitation. 
As one obstacle, even if others' strategies are perfectly observable, individuals might find it difficult to identify which ones have the highest payoff. 
After all, the payoff of a strategy of direct reciprocity is not determined by the outcome of any single round.
Rather, it is determined by how well this strategy fares over an entire sequence of rounds, against many different population members. 
In practice, such information might be difficult both to obtain and to process. 

%% INTRO: Gap in the literature and what  question we wish to address. 

Most models of direct reciprocity do not address these difficulties~\citep{brauchli:JTB:1999,brandt:JTB:2006,ohtsuki:JTB:2007b,szolnoki:pre:2009b,imhof2010stochastic,vansegbroeck:prl:2012,grujic:jtb:2012,Martinez2012,stewart:pnas:2013,pinheiro:PLoSCB:2014,stewart:games:2015,Baek2016,McAvoy:ProcA:2019,glynatsi:SCR:2020,Schmid:PlosCB:2022,Murase:SciRep:2022,Cooney:BMB:2022,Chen:PNASNexus:2023} and, instead, assume individuals can easily copy the strategies of others. 
Similarly, they assume that updating decisions are based on the strategies' average (or expected) payoffs, which are based on all rounds and all interactions. 
These assumptions create a curious inconsistency in how models represent an individual's cognitive abilities. 
On the one hand, when playing the game, individuals are often assumed to have restricted memory. 
Respective studies typically assume that individuals make their decisions each round based on the outcome of the last round only~\citep[with only a few exceptions, see Refs.]{Hauert1997,vanveelen:PNAS:2012,Stewart2016,hilbe:PNAS:2017,Li:NatCS:2022,Murase:PLoSCompBio:2023a}. 
Yet when learning new strategies, individuals are assumed to remember (or compute) each others' precise average payoff across many rounds and many interaction partners. 
Herein, we wish to explore whether this latter assumption is actually necessary for the evolution of reciprocity through social imitation. 
We ask whether individuals can learn to adopt reciprocal strategies even when learning is based on payoff information from a limited number of rounds. 


%% INTRO: Basic description of our model and our findings 
To explore that question, we theoretically study imitation dynamics in the repeated prisoner's dilemma, using two extreme scenarios. 
The first scenario is the classical modeling approach. 
Here, individuals update their strategies based on their expected payoffs. 
We contrast this model with an alternative scenario where individuals update their strategies based on the very
last (one-shot) payoff they obtained. 
We find that individuals with limited payoff memory tend to adopt less generous strategies. 
Yet moderate levels of cooperation can still evolve. 
Moreover, as we increase the individuals' payoff memory to include the last two or three one-shot payoffs, cooperation rates quickly approach the rates observed in the classical baseline. 


%%%%%%%%%
%% MODEL  %%
%%%%%%%%%


\section{Model and Methods}\label{section:model}

%% MODEL: Basic intro

To explore the impact of limited payoff memory, we adapt existing models of the evolution of direct reciprocity.
These models involve two different time scales. 
The short time scale describes the game dynamics. 
Here, individuals with fixed strategies are randomly matched to interact with each other in repeated social dilemmas. 
The long time scale describes the evolutionary dynamics. 
Here, individuals can update their repeated-game strategies based on the payoffs they yield. 
In the following, we introduce the basic setup of our model; all details and derivations are described in the \esm.\\

%% MODEL: (Repeated) donation game

\noindent
{\bf Description of the game dynamics.} We consider a well-mixed population consisting of $N$~players.
Players are randomly matched in pairs to participate in a repeated donation game~\citep{sigmund2010calculus} with their respective co-player.
Each round, they can either cooperate (\(C\)) or defect (\(D\)). 
A cooperating player provides a benefit~\(b\) to the other player at their own cost~\(c\), with \(0 \!<\! c \!<\! b\). 
A defecting player provides no benefit and pays no cost. 
Thus, the players' payoffs in a single round are given by the matrix
\begin{equation}
\bordermatrix{%
	& C & D \cr
C &\ b-c &\ -c\  \cr
D &\ b &\ 0\ \cr
} .
\end{equation}
In particular, payoffs take the form of a prisoner's dilemma:
Mutual cooperation yields a better payoff than mutual defection ($b\!-\!c\!>\!0$), but each player individually prefers to defect, independent of the co-player's action ($b\!>\!b\!-\!c$ and $0\!>\!-c$). 
To incorporate repetition, we assume that after each round, there is a constant continuation probability $\delta$ of interacting for another round. 
For $\delta\!=\!0$, we recover the case of a conventional (one-shot) prisoner's dilemma. 
Here, mutual defection is the only equilibrium. 
As $\delta$ increases, the game turns into a repeated game. Here, additional equilibria emerge, with some of them allowing for full cooperation~\citep{friedman:RES:1971,Akin:chapter:2016,hilbe:GEB:2015,stewart:pnas:2014}. 


%% MODEL: Reactive strategies

In a one-shot donation game, players can only choose among two pure strategies (they can either cooperate or defect).
In the repeated game, strategies become arbitrarily complex. 
Here, strategies are contingent rules, telling players what to do depending on the outcome of all previous rounds. 
For simplicity, in the following we assume individuals use {\it reactive strategies}~\citep{Nowak1992tit}. 
A reactive strategy only depends on the other player's action in the last round. 
Such strategies can be written as a three-dimensional tuple \(\strategy=(y, p,q)\).
The first entry \(y\) is the probability that the player opens with cooperation in the first round. 
The two other entries are the probabilities that the player cooperates in all subsequent rounds, depending on whether the co-player cooperated~($p$) or defected~($q$) in the previous round. 
The set of reactive strategies is simple enough to facilitate an explicit mathematical analysis~\citep{hofbauer1998evolutionary}. 
Yet it is rich enough to capture several important strategies of repeated games. 
For example, it contains \alld{} $=\!(0,0,0)$, the strategy that always defects. 
Similarly, it contains Tit-for-Tat, \tft{} $=\!(1,1,0)$, the strategy that copies the co-player's previous action (and that cooperates in the first round). 
Finally, it contains Generous Tit-for-Tat, \gtft $=\!(1,1,q)$, where $q\!>\!0$ reflects a player's generosity in response to a co-player's defection~\citep{molander:jcr:1985,Nowak1992tit}.  

In the short run, the players' strategies are taken to be fixed.
Players use their strategies to decide whether to cooperate in a series of repeated games against all other population members. 
In the long run, however, the players' strategies may change depending on the payoffs they yield, as we describe in the following.\\

 %% MODEL: Pairwise comparison process

\noindent
{\bf Description of the evolutionary dynamics.}
Herein, we assume population members update their strategies based on social learning. 
To model these strategy updates, we consider a pairwise comparison process~\citep{traulsen2007pairwise}. 
This process assumes that at regular time intervals, one population member is randomly selected, and given the chance to revise its strategy.
We refer to this player as the ``learner''.
With probability $\mu$ (reflecting a mutation rate), the learner simply adopts a random strategy (all reactive strategies have the same probability to be chosen). 
With the converse probability $1\!-\!\mu$, the learner randomly picks a ``role model'' from the population. 
The learner then compares its own payoff $\pi_\learner$ from the repeated game to the role model's payoff $\pi_\rolemodel$. 
The learner adopts the role model's strategy with a probability \(\varphi\) described by a Fermi function~\citep{blume:GEB:1995,szabo:PRE:1998}, 
\begin{equation} \label{Eq:rho}
    \varphi\left(\pi_\learner, \pi_\rolemodel\right) = \frac{1}{1\!+\! e^{- \!\beta\left(\pi_\rolemodel- \pi_\learner\right)}}.
\end{equation}
The selection strength parameter $\beta\!\ge\!0$ indicates how sensitive players are to payoff differences. 
For $\beta\!=\!0$, payoff differences are irrelevant, and the learner simply adopts the role model's strategy with probability $1/2$. As the selection strength~$\beta$ increases, players are increasingly biased to imitate the role model only if it has the higher payoff. 
%In either case, we assume in the following that the role model's strategy can be identified.
%That is, if the learner decides to imitate the role model's strategy, the learner copies it exactly, as in previous works on direct reciprocity. 

%% MODEL: Perfect vs limited payoff memory

We deviate from previous models in how we interpret the payoffs $\pi_\learner$ and $\pi_\rolemodel$, which form the basis of the pairwise comparisons in Eq.~\eqref{Eq:rho}. 
In previous work, these payoffs are taken to be the respective players' expected payoffs. 
We interpret that setup as a model with perfect payoff memory. 
There, the payoffs  $\pi_\learner$ and $\pi_\rolemodel$ represent an average over all possible repeated games the two individuals have played with all population members (\FigBaseResults, upper left panel). 
The use of expected payoffs is mathematically convenient, because explicit formulas for these payoffs are available~\citep{hofbauer1998evolutionary}.
Herein, we compare this model of perfect payoff memory to a model with limited payoff memory. 
In the latter model, the players' payoffs $\pi_\learner$ and $\pi_\rolemodel$ are taken to be the payoffs that each player received in their very last round prior to making social comparisons. 
That is, players only consider the very last repeated game they participated in, and there they only take into account the outcome of the very last round (\FigBaseResults, lower left panel). 
This assumption could reflect, for example, a strong recency bias in how individuals evaluate payoffs.  
In addition to this extreme case of limited payoff memory, later on we also explore cases in which players take into account the outcome of two, three, four, or more recent rounds. 

%% MODEL: Summary and rare mutation assumption. 

Both in the case of perfect and limited memory, we iterate the elementary strategy update step described above many times. 
This gives rise to a stochastic process that describes which strategies players adopt over time. 
Provided the selection strength is finite, this process satisfies the mathematical property of being ergodic. 
This implies that in the long run, the time average of the players' cooperation rates converges, and that this limit is independent of the initial population. 
We explore the dynamics of this process mathematically and with numerical simulations.
These simulations are run sufficiently long for convergence to occur (we numerically checked that independent runs are within a 1\% error tolerance). 
For the results presented in the following, we assume that mutations are rare (\(\mu\!\rightarrow\! 0\)). 
This assumption is fairly common in evolutionary game theory, both because it makes some computations more efficient~\citep{fudenberg:JET:2006,wu:JMB:2012,mcavoy:jet:2015} and because the results can be interpreted more easily.  
However, in Section~3 of the \esm{} we show that our main results continue to hold for strictly positive mutation rates.



%%%%%%%%%%
%% RESULTS  %%
%%%%%%%%%%

\section{Results}

%% 	RESULTS: Setup to get analytical results

\noindent
{\bf Stability of cooperative populations.}
To get some intuition for the differences between perfect and limited payoff memory, we first analyze when full cooperation is stable in either scenario.
For a population to be fully cooperative, players need to cooperate in the first round (that is, $y\!=\!1$) and after any round with mutual cooperation (that is, $p\!=\!1$). 
We refer to such a strategy as Generous Tit-for-Tat, \gtft{} $=\!(1,1,q)$. 
To explore whether such a population consisting of \gtft{} players is stable, we introduce a single mutant who adopts \alld. 
We say {\it (full) cooperation is stochastically stable} if the single mutant is more likely to imitate the residents than vice versa. 
For simplicity, we consider a large population~($N\!\rightarrow\!\infty$) and strong selection~($\beta\!\rightarrow\!\infty$).
More general results are derived in the \esm. 

%% RESULTS: Analytical results for perfect memory

In the case of perfect payoff memory, it is straightforward to characterize when cooperation is stochastically stable. 
Here, we simply need to compute the players' expected payoffs. 
Because the population mostly consists of residents, and because residents mutually cooperate with each other, their expected payoff is $\pi_\gtft = b\!-\!c$. 
On the other hand, the defecting mutant only interacts with residents. 
Given the residents' strategy, the mutant receives a benefit~$b$ in the first round, and in every subsequent round with probability $q$. 
As a result, the mutant's expected payoff is $\pi_\alld \!=\! (1\!-\!\delta\!+\!\delta q)b$. 
For perfect payoff memory, the requirement for cooperation to be stochastically stable reduces to the condition $\pi_\gtft > \pi_\alld$, which yields
\begin{equation} \label{Eq:PerfectMemory}
q < 1\!-\!\frac{c}{\delta  b}.
\end{equation}
In particular, we recover the previous observation that $q\!=\!1-c/(\delta b)$ is the maximum generosity that cooperators should have~\citep{molander:jcr:1985,Nowak1992tit,Schmid:NHB:2021}. 
Moreover, because $q$ needs to be non-negative, we also conclude that cooperation can only be stable if $\delta \!>\! c/b$.
Again, this condition for the feasibility of direct reciprocity already exists in the literature~\citep{nowak:Science:2006}.

%% RESULTS: Analytical results for limited memory

The logic of the case with limited payoff memory is somewhat different. 
Here we need to compute how likely each player obtains one of the four possible payoffs $\{b\!-\!c, -c, b, 0\}$ in the very last round of a game, before they make social comparisons.
Because residents almost always interact with other residents, their last one-shot payoff is $\pi_\gtft = b\!-\!c$ almost surely. 
For the defecting mutant, there are two possibilities. If the mutant's co-player happens to cooperate in the last round, the mutant receives $\pi_\alld\!=\!b$. This case occurs with probability $1\!-\!\delta\!+\!\delta q$. On the other hand, if the co-player defects in the last round, the mutant receives $\pi_\alld=\!0$. This occurs with the converse probability $\delta(1\!-\!q)$.
Because $b\!-\!c\!<\!b$, residents tend to imitate the mutant in the first case. 
Because $b\!-\!c\!>\!0$, mutants tend to imitate the resident in the second case. 
Cooperation is stochastically stable if the first case is less likely than the second, which yields the condition
\begin{equation} \label{Eq:LimitedMemory}
q < 1\!-\!\frac{1}{2 \delta}.
\end{equation}
Interestingly, this condition no longer depends on the exact payoff values, $b$ and $c$. 
This independence arises because of our assumption of strong selection, in which case only the payoff ordering $b\!>\!c\!>\!0$ matters. Because $q$ is non-negative, condition~\eqref{Eq:LimitedMemory} can be satisfied only if $\delta\!>\!1/2$. That is, players need to interact in more than two rounds, on average. 

%% RESULTS: Summary of analytical results

By comparing the two cases, we find that payoff memory affects whether a conditionally cooperative strategy ($1,1,q$) is viable. 
With perfect memory, the maximum generosity $q$ must satisfy Eq.~\eqref{Eq:PerfectMemory}.
In particular, this generosity can become arbitrarily large, provided the game's benefit-to-cost ratio~$b/c$ and the continuation probability~$\delta$ are sufficiently large. 
In contrast, with limited payoff memory, the maximum generosity is bounded by one half, and it is independent of the benefit-to-cost ratio.\\


%% RESULTS: Evolving strategies

\noindent
{\bf Evolutionary dynamics of reciprocity.}
The previous static observations describe whether full cooperation, once established, can be sustained. 
In a next step, we explore in which cases cooperation actually evolves. 
To this end, we turn to simulations.
We have run separate simulations for perfect and limited payoff memory. 
In each case, we consider both a low and a high benefit of cooperation ($b/c\!=\!3$ and $b/c\!=\!10$, respectively).  
For each simulation, we record which strategies ($y,p,q$) the players adopt over time.
\FigBaseResults{} depicts the conditional cooperation probabilities $p$ and $q$ (we omit the opening move \(y\) because we use a discount factor~\(\delta\) close to one, such that first-round behavior is largely irrelevant). 
In all simulations, we find that the players' strategies cluster in two regions of the strategy space. 
The first region corresponds to a neighborhood of \alld{} with $(p,q)\!\approx\!(0,0)$.
The second region corresponds to a thin strip of cooperative strategies with $(p,q)\!\approx\!(1,q)$. 
Within this strip, we observe that most strategies satisfy the constraints on $q$ suggested by the inequalities~\eqref{Eq:PerfectMemory} and~\eqref{Eq:LimitedMemory}. 
That is, with perfect memory, most evolving strategies have $q\!<\!1\!-\!c/b$, whereas with limited payoff memory, most strategies have $q\!<\!1/2$. 
In particular, for limited payoff memory, changes in the benefit parameter have no effect on the qualitative distribution of strategies. 

%% RESULTS: Evolving cooperation rates

In each case, the evolutionary dynamics follow a similar cyclic pattern~\citep[as described in Refs.][]{imhof2010stochastic, Nowak1992tit}:
Resident populations of defectors are most likely invaded by strategies close to \tft. 
Once the population adopts conditionally cooperative strategies  $(1,1,q)$, neutral drift may introduce larger values of generosity~$q$. 
If the resident's generosity~$q$ violates the conditions~\eqref{Eq:PerfectMemory} and~\eqref{Eq:LimitedMemory}, defectors can re-invade, and the cycle starts again. 
The relative time spent near \alld{} and near the strip of conditionally cooperative strategies depends on the memory (\FigInvasionAnalysis, depicting the case of high benefits). 
For perfect memory, we find that \alld{} is replaced relatively quickly by more cooperative strategies. 
Here, it takes on average 159 invading mutants until \alld{} is successfully replaced. 
In contrast, for limited memory, \alld{} is more robust, resisting on average 798 mutant strategies. 
This picture reverses for an initial population that adopts \gtft. 
Such populations are much more robust under perfect memory than they are under limited memory. 
Overall, we find that the impact of memory on the population's average cooperation rate is substantial. 
For perfect memory, this rate is 52\% for low benefits, and 98\% for high benefits. 
For limited payoff memory, the evolving cooperation rates are smaller but still strictly positive, with 37\% cooperation for low benefits and 51\% cooperation for high benefits (\FigBaseResults). 

%% RESULTS: Evolving cooperation rates

To further investigate the influence of different parameters, we have systematically varied the benefit~$b$ and the selection strength~$\beta$ in~\FigDependenceParameters.
According to \FigDependenceParameters{\it a}, perfect memory consistently results in a higher cooperation rate, and this relative advantage further increases with an increasing benefit~$b$. 
Interestingly, for limited payoff memory, the cooperation rate remains stable at approximately 50\% once \(b \!\ge\! 5\).
This finding is reminiscent of our earlier static results, which also suggested that changes in the game's benefits may have a negligible role on the stochastic stability of full cooperation.  
With respect to the effect of different selection strengths, \FigDependenceParameters{\it b} suggests that both perfect and limited payoff memory yield similar cooperation rates for weak selection (\(\beta \!\ll\! 1\)). This is not a coincidence: it is known that, under weak selection, stochastic payoffs can be replaced by their (deterministic) expectations without altering the evolutionary dynamics \citep{mcavoy:NHB:2020}--and perfect payoff memory corresponds to the expected value of the payoffs in the limited payoff memory model, due to the law of large numbers.
Beyond weak selection, increasing selection has a positive effect under perfect payoff memory, but a negative effect under limited payoff memory.\\

%% RESULTS: Setup of higher-memory simulations

\noindent 
{\bf The effect of increasing individual payoff memory.}
So far, we have taken a rather extreme interpretation of limited payoff memory. 
We assumed that individuals update their strategies based on their experience in a single round of the prisoner's dilemma, against a single co-player. 
The limited payoff memory framework can be expanded in various ways. 
In particular, individuals may recall a larger number of rounds, they may recall their interactions with several co-players, or both. 
To gain further insights on the impact of payoff memory, we explore four additional scenarios. 
In the first scenario, players recall the payoffs they obtained in the last two rounds against a single co-player. 
In the second scenario, players recall their last-round payoffs against two co-players. 
In the third scenario, they recall the two last rounds against two co-players. 
Finally, in the last scenario, players update based on the average payoff they
receive over all rounds with a single co-player. (Further extensions are possible, but we do not explore them here.)

%% RESULTS: Analytical results for higher memory

For most scenarios, we can again derive an analytical condition for when cooperation is stochastically stable. 
As before, we assume populations are large and that selection is strong. 
For simplicity, we also assume that the game continues almost certainly after each round (i.e., $\delta$ approaches one). 
The details of this analysis can be found in the \esm. 
In the first two scenarios, we interestingly find that for $b\!>\!2c$, cooperation is stochastically stable when $q\!<\! \frac{\sqrt{2}}{2}\approx 0.707$. 
Comparing this condition with the more stringent condition in Eq.~\eqref{Eq:LimitedMemory} suggests that there are now more conditionally cooperative strategies that can sustain cooperation. 
Hence, cooperation should evolve more easily.
In the last scenario, we find that cooperation is stochastically stable when \(q
< 1 - \frac{c}{b}\), which is the same condition as in
Eq.~\eqref{Eq:PerfectMemory}, even though only a single co-player is considered
instead of the whole population. 

%% RESULTS: Simulations for higher memory

We complement these analytical results with additional simulations; see \FigHigherMemory.
We observe that a minimal increase in the players' payoff memory (compared to the baseline case with a single round recalled) can promote cooperation considerably. 
Specifically, in all four scenarios with extended memory, we see similar cooperation rates that approach the rates observed under perfect memory. 
These results suggest that while it takes {\it some} payoff memory to sustain substantial cooperation rates, the memory requirements are rather modest. 
Already remembering a few interactions, either with the same co-player or across different co-players, may provide players with enough information to adopt reciprocal strategies.\\



%% RESULTS: Memory-1 strategies %%

\noindent
{\bf Beyond reactive strategies and the donation game.} 
While the results presented in the main text focus on reactive strategies and the donation game, the observed patterns hold more generally.
To illustrate this point in more detail, in the \esm{} we first consider the dynamics of the donation game among memory-1 strategies. 
Here, players take into account both their co-player's and their own last move, see Refs.~\citep{nowak:Nature:1993,imhof:JTB:2007}. 
Also in that case, we also observe that perfect memory leads to systematically higher cooperation rates (Supplementary Figures~5,6).
Again, this advantage of perfect memory is particularly pronounced for strong selection, or when there is a high benefit of cooperation (\FigMemoryOneParameters).

%% RESULTS: Snowdrift Game
In a second step, we also analyze the evolutionary dynamics of a different social dilemma, the snowdrift game~\citep{doebeli:Ecology:2005}. 
In this game, individuals can again cooperate or defect. 
However, now they get the benefit $b$ if at least one of them cooperates. 
Cooperators in turn pay a cost of $c$ or $c/2$, depending on whether they are the only one to cooperate or not. 
Compared to the donation game, the snowdrift game represents a weaker form of a social dilemma, because defection is no longer the dominant action in any single round~\citep{nowak:JTB:2012}.
Again, we derive analytical results in the case of large populations and strong selection. 
For perfect memory, cooperation is stochastically stable when \(q < 1 \!-\! c/(2 \delta b)\). 
Interestingly, for limited payoff memory, we obtain the same condition as for the donation game, \(q < 1 \!-\! 1/(2 \delta)\). 
We explain this invariance in detail in the \esm{}: compared to the donation game, the ordering of one-shot payoffs in the snowdrift game only differs in one instance (the {\it sucker's payoff} now exceeds the {\it punishment payoff}); however, this instance is irrelevant for the stochastic stability of cooperation. 
Further evolutionary simulations suggest that again, perfect memory leads to higher cooperation rates than limited memory; but now even limited memory can result in substantial cooperation (Supplementary Figures~8, 9). 


%%%%%%%%%%%%
%%  DISCUSSION %%
%%%%%%%%%%%%

\section{Discussion}\label{section:discussion}
%
In economics, if payoff is measured in terms of money and a decision is to be made at time $t$ in the future, then currency accumulated early on is weighted more in that decision because it has more time to accumulate interest. Such a model discounts the future relative to the past. It also does not necessarily require ``memory'' because rewards are accumulated into a factor used in decision-making; the specific time stamps of rewards do not themselves provide better information beyond their effects on total payoff. In a similar fashion, the probabilistic interpretation of discounting as a continuation probability \citep{axelrod1981evolution} also effectively discounts the future relative to the past. A foraging animal deciding between two behaviors might tend to choose the one that yields a moderate reward sooner relative to a larger reward later, since earlier rewards (e.g., food) contribute to immediate survival, and there is no guarantee that later rewards will happen at all \citep{stephens1986foraging}.

Within the context of a single repeated game, the model we consider here is, in some ways, dual to the classical model of temporally-discounted rewards in repeated games. Instead of making decisions based on expected rewards in the future, we consider individuals who make decisions based on actual rewards in the past. Thus, the ability to estimate the future payoff of a strategy is replaced by the memory of how this strategy previously fared against others. This involves two time scales: interaction partners and rounds within those interactions. As a result, we are dealing with a model that discounts the past rather than the future. Intriguingly, treating payoffs in this manner is reminiscent of the reward-smoothing technique of ``eligibility traces'' in reinforcement learning \citep{sutton:MIT:2018}, which uses past rewards (discounted appropriately) to shape present perceived payoff. There is a sound basis for this method in neuroscience, where rewards and (temporal-difference) learning are associated to dopaminergic neurons \citep{schultz:JN:1998} and spike-timing-dependent plasticity \citep{dan:Neuron:2004}. This suggests a more biologically-encoded interpretation of memory, which is equally applicable to models of direct reciprocity where rewards have a neurological basis.

Of course, the precise nature of ``memory'' also depends on what payoffs in a game represent, which should be taken into account when applying game-theoretic models. For example, a payoff stream of monetary currency might truly accumulate and not require memory on the parts of agents. Even in the context of money, however, not all of what was obtained in the past is necessarily available at the time a decision is made, which brings memory into play. The serial position effect in human psychology shows that in an ordered list of items (e.g., words), humans tend to have difficulty remembering the entirety of sequences, demonstrating moderate recall for those items coming earlier (primacy effect), substantial recall for those coming later (recency effect), and lower recall for those in between \citep{murdock:JEP:1962}. It is therefore reasonable that when presented with a stream of payoffs, whether on the timescale of pairing for repeated interactions or in a stream of one-shot games, players might be able to effectively incorporate only the most recent payoffs.

In fact, even beyond specific psychological considerations, a curious interpretation of payoffs arises from the formula commonly used for expected payoffs in repeated games. If $\delta\in\left[0,1\right)$ is the probability of continuing to another round in the game, then the expected payoff to an agent is $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$, where $u_{t}$ is the reward the agent receives in the stage game at time $t$. Here, additional stochasticity arises due to uncertainty in the game length, and an agent might not be able to compute his or her expected payoff for use in decision-making. The probability that the game terminates after the interaction at time $T$ is $\delta^{T}\left(1-\delta\right)$, in which case $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$ is exactly the expected payoff the agent receives at time $T$, i.e. \emph{in the last round}. As an unbiased estimator of this expectation, the agent might thus use $u_{T}$ as a proxy for ``success'' when evaluating his or her behavior. This gives a purely model-driven justification for why considering payoff in the last round of the game can result in more realistic extensions of traditional models.

We note that the expected \emph{total} payoff in the game, $u_{0}+u_{1}+\cdots +u_{T}$, is given by $\sum_{t=0}^{\infty}\delta^{t}u_{t}$. This version of ``expected payoff'' appears less common in the literature on direct reciprocity than its normalization, $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$, likely owing to the fact that payoffs can grow arbitrarily large with sufficiently long time horizons ($\delta\rightarrow 1^{-}$). Non-normalized payoffs interfere with selection intensity ($\beta$) in models of social imitation, which is (presumably) why they appear less frequently in the literature. On that point, we note that differences between realized and expected payoffs disappear in the limit of weak selection, which is known in a general setting \citep{mcavoy:NHB:2020}. Non-weak selection can introduce substantial differences between models with realized and expected payoffs \citep{mcavoy:PLOSCB:2021}, which is especially important to understand in models of social systems with cultural transmission \citep{cavalli1981cultural}.

Our main contribution is an application of these ideas to direct reciprocity, which is one of the key mechanisms to explain why unrelated individuals might cooperate~\citep{nowak:Science:2006}. 
According to this mechanism, cooperation pays if it makes the interaction partner more cooperative in future. 
To describe which strategies are most effective, the previous theoretical literature assumes that the evolutionary dynamics are driven by the players' expected payoffs~\citep{brauchli:JTB:1999,brandt:JTB:2006,ohtsuki:JTB:2007b,szolnoki:pre:2009b,imhof2010stochastic,vansegbroeck:prl:2012,grujic:jtb:2012,Martinez2012,stewart:pnas:2013,pinheiro:PLoSCB:2014,stewart:games:2015,Baek2016,McAvoy:ProcA:2019,glynatsi:SCR:2020,Schmid:PlosCB:2022,Murase:SciRep:2022}.
To the extent that strategies are learned (not inherited), this assumption seems to impose rather stringent requirements on the individuals' cognitive abilities. 
In the most extreme case, individuals would have to remember (or compute) their payoffs against all population members, for all possible ways in which their repeated games may unfold. 
This assumption introduces a curious inconsistency in how these models represent an individual's cognitive abilities. 
For playing their games, individuals are often assumed to only recall the outcome of the very last round. 
Yet to update their strategies, individuals are implicitly assumed to have a record of the outcome of all rounds, across all interaction partners. 

It is natural to ask, then, to what extent perfect payoff memory is in fact required for the evolution of reciprocity. 
To this end, we consider a model in which individuals only remember the payoff of their very last interaction, or the payoffs of the last few interactions. 
By only considering an individual's most recent experiences, the evolutionary process is subject to additional stochasticity. 
Strategies that perform well on average (across an entire repeated game and across many interaction partners) may still get replaced if the respective player happened to yield an inferior payoff in the very last round. 
A similar element of stochasticity has been previously explored in the context of one-shot (non-repeated) games~\citep{sanchez:JTB:2005,roca:PhysicalReview:2006,Traulsen:JTB:2007,Woelfing:JTB:2009,Hauert:PRE:2018}. 
This literature studies which strategies are selected for when individuals only interact with a finite sample of population members. 
In the respective models, individuals can only choose among two strategies. 
They can either cooperate or defect, and stochastic sampling affects which of these two strategies is favored. 
Instead, in repeated games, players have access to a large set of strategies~\citep[in our case, all reactive strategies;][]{nowak:APC:1989}. 
Here, stochastic sampling does not only affect whether cooperative or non-cooperative strategies are favored; it also affects {\it which} conditionally cooperative strategies are favored. 

To explore the effect of payoff memory, we only considered the simplest model of reciprocity. 
This leaves several possible directions for future work. 
For example, for both our analytical analysis and our simulations, we assumed that populations are {\it well-mixed}. 
As a result, any population member is equally likely to be a given player's last co-player. 
A similar analysis for structured populations would be desirable, as population structure can often promote cooperation without an explicit need for reciprocity~\citep{szabo:PR:2007,vanveelen:PNAS:2012}.
Similarly, we assumed that after identifying a suitable role model, players can
easily imitate that role model's strategy. \DIFaddbegin \DIFadd{Especially in repeated games, this
assumption is less innocuous than it may appear. After all, a player can
typically only observe the co-players' actions, but not their underlying
strategies. This player may only attempt to infer the co-players' strategies
from their past reactions. Once players only have a restricted recollection of
the past, this task seems particularly difficult. }\DIFaddend In the context of our study,
this might be a particular simplification, because players with limited payoff
memory may also have less accurate information to infer a co-player's strategy.
Finally, we have taken a player's payoff memory to be a given parameter of the model. 
Instead, future models could explore how that payoff memory might co-evolve along with the players' strategies \citep[similar to previous work on the evolution of the players' feasible strategy sets, e.g.][]{Stewart2016,Schmid:PlosCB:2022,Murase:PLoSCompBio:2023a}. 
Here, individuals might experience an interesting trade-off. 
Better payoff-memory would help them to make better decisions, yet it might also come with higher cognitive costs. 
For such a setup, our results suggest that in many instances, a modest amount of memory may be sufficient to achieve good outcomes. 

 
%%%%%%%%%%%%%
%%  CONCLUSION  %%
%%%%%%%%%%%%%

\section{Conclusion}\label{section:discussion}

Herein, we combine analytical methods and computer simulations to explore the impact of payoff memory on the evolution of reciprocal altruism. 
In the most extreme case, we consider individuals who update their strategies based on only one piece of information:  the last round of a single repeated game. 
Compared to exisiting models where decisions are made based on expected payoffs (perfect payoff memory), we find that individuals are less generous, and they tend to be less cooperative overall~(\FigBaseResults). 
However, once individuals update their strategies based on two or more recent experiences, overall cooperation rates quickly approach the levels observed under perfect payoff memory~(\FigHigherMemory). 
These findings suggest that models based on expected payoffs can serve as a useful approximation to more realistic models with limited payoff memory. 
Our findings also contribute to a wider literature that explores which kinds of cognitive capacities are required for reciprocal altruism to be feasible~\citep[e.g.,][]{Stevens:fip:2011,Volstorf:PlosOne:2011}. 
While more payoff memory is always favorable, reciprocal cooperation can already be sustained if individuals have a record of two or three past outcomes. We believe that this kind of result, derived entirely within a theoretical model, is crucial for making model-informed deductions about reciprocity in natural systems.\\[0.5cm]

%%%%%%%%%%%
%% END NOTES %%
%%%%%%%%%%%

\noindent
{\bf Ethics.}
This work is purely theoretical and did not require ethical approval from a human subject or animal welfare committee.\\

\noindent
{\bf Data accessibility.}
All data generated for this manuscript are openly available at:
\href{https://zenodo.org/records/10844104}{https://zenodo.org/records/10844104}.
The code for producing the results and generating the figures is available
on the online \DIFaddbegin \DIFadd{GitHub }\DIFaddend repository
\href{https://github.com/Nikoleta-v3/evolution-of-cooperation-with-limited-updating-payoff-memory}{evolution-of-cooperation-with-limited-updating-payoff-memory}.\\

\noindent
{\bf Declaration of AI use.}
We have not used AI-assisted technologies in creating this article.\\

\noindent
{\bf Authors' contributions.}
N.G.: conceptualization, formal analysis, investigation, methodology, visualization, writing - original draft, writing -- review and editing; 
A.M.: conceptualization, formal analysis, methodology, writing -- review and editing; 
C.H.: conceptualization, formal analysis, methodology, writing -- review and editing.\\

\noindent
{\bf Conflict of interest declaration.}
We declare we have no competing interests.\\

\noindent
{\bf Funding.}
N.G. and C.H. acknowledge generous support by the European Research Council Starting Grant 850529:
E-DIRECT, and by the Max Planck Society.\\



%%%%%%%%%%%%
%% REFERENCES %%
%%%%%%%%%%%%

\DIFdelbegin %DIFDELCMD < {
%DIFDELCMD < {\setlength{\bibsep}{0\baselineskip}
%DIFDELCMD < \bibliographystyle{naturemag}
%DIFDELCMD < \bibliography{bibliography}
%DIFDELCMD < }
%DIFDELCMD < }
%DIFDELCMD < %%%
\DIFdelend %DIF >  {
%DIF >  {\setlength{\bibsep}{0\baselineskip}
%DIF >  \bibliographystyle{naturemag}
%DIF >  \bibliography{bibliography}
%DIF >  }
%DIF >  }
\DIFaddbegin 

\begin{thebibliography}{10}
\expandafter\ifx\csname \DIFadd{url}\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname \DIFadd{urlprefix}\endcsname\relax\def\urlprefix{URL }\fi
\providecommand{\bibinfo}[2]{#2}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem{hofbauer1998evolutionary}
\bibinfo{author}{Hofbauer, J.}\DIFadd{, }\bibinfo{author}{Sigmund, K.} \emph{\DIFadd{et~al.}}
\newblock \emph{\bibinfo{title}{Evolutionary games and population dynamics}}
  \DIFadd{(}\bibinfo{publisher}{Cambridge university press}\DIFadd{, }\bibinfo{year}{1998}\DIFadd{).
}

\bibitem{nowak:Nature:2004}
\bibinfo{author}{Nowak, M.~A.}\DIFadd{, }\bibinfo{author}{Sasaki, A.}\DIFadd{,
  }\bibinfo{author}{Taylor, C.} \DIFadd{\& }\bibinfo{author}{Fudenberg, D.}
\newblock \bibinfo{title}{Emergence of cooperation and evolutionary stability
  in finite populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{428}}\DIFadd{,
  }\bibinfo{pages}{646--650} \DIFadd{(}\bibinfo{year}{2004}\DIFadd{).
}

\bibitem{hauert2005game}
\bibinfo{author}{Hauert, C.} \DIFadd{\& }\bibinfo{author}{Szab{\'o}, G.}
\newblock \bibinfo{title}{Game theory and physics}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{American Journal of Physics}}
  \textbf{\bibinfo{volume}{73}}\DIFadd{, }\bibinfo{pages}{405--414}
  \DIFadd{(}\bibinfo{year}{2005}\DIFadd{).
}

\bibitem{Traulsen:PhilTrans:2022}
\bibinfo{author}{Traulsen, A.} \DIFadd{\& }\bibinfo{author}{Glynatsi, N.~E.}
\newblock \bibinfo{title}{The future of theoretical evolutionary game theory}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Philosophical Transactions B}}
  \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{maynard-smith:Nature:1973}
\bibinfo{author}{Maynard~Smith, J.} \DIFadd{\& }\bibinfo{author}{Price, G.~R.}
\newblock \bibinfo{title}{The logic of animal conflict}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{246}}\DIFadd{,
  }\bibinfo{pages}{15--18} \DIFadd{(}\bibinfo{year}{1973}\DIFadd{).
}

\bibitem{Stein:PTRS:2023}
\bibinfo{author}{Stein, A.} \emph{\DIFadd{et~al.}}
\newblock \bibinfo{title}{Stackelberg evolutionary game theory: how to manage
  evolving systems}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Philosophical Transactions of the Royal
  Society B}} \textbf{\bibinfo{volume}{378}}\DIFadd{, }\bibinfo{pages}{20210495}
  \DIFadd{(}\bibinfo{year}{2023}\DIFadd{).
}

\bibitem{nowak:Science:2006}
\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Five rules for the evolution of cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Science}} \textbf{\bibinfo{volume}{314}}\DIFadd{,
  }\bibinfo{pages}{1560--1563} \DIFadd{(}\bibinfo{year}{2006}\DIFadd{).
}

\bibitem{smith1982evolution}
\bibinfo{author}{Smith, J.~M.}
\newblock \emph{\bibinfo{title}{Evolution and the Theory of Games}}
  \DIFadd{(}\bibinfo{publisher}{Cambridge university press}\DIFadd{, }\bibinfo{year}{1982}\DIFadd{).
}

\bibitem{Wu2015}
\bibinfo{author}{Wu, B.}\DIFadd{, }\bibinfo{author}{Bauer, B.}\DIFadd{, }\bibinfo{author}{Galla,
  T.} \DIFadd{\& }\bibinfo{author}{Traulsen, A.}
\newblock \bibinfo{title}{Fitness-based models and pairwise comparison models
  of evolutionary games are typically different---even in unstructured
  populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{New Journal of Physics}}
  \textbf{\bibinfo{volume}{17}}\DIFadd{, }\bibinfo{pages}{023043}
  \DIFadd{(}\bibinfo{year}{2015}\DIFadd{).
}

\bibitem{Smolla:PTRS:2021}
\bibinfo{author}{Smolla, M.} \emph{\DIFadd{et~al.}}
\newblock \bibinfo{title}{Underappreciated features of cultural evolution}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Philosophical Transactions of the Royal
  Society B}} \textbf{\bibinfo{volume}{376}}\DIFadd{, }\bibinfo{pages}{20200259}
  \DIFadd{(}\bibinfo{year}{2021}\DIFadd{).
}

\bibitem{Denton:TPB:2022}
\bibinfo{author}{Denton, K.~K.}\DIFadd{, }\bibinfo{author}{Ram, Y.} \DIFadd{\&
  }\bibinfo{author}{Feldman, M.~W.}
\newblock \bibinfo{title}{Conformity and content-biased cultural transmission
  in the evolution of altruism}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Theoretical Population Biology}}
  \textbf{\bibinfo{volume}{143}}\DIFadd{, }\bibinfo{pages}{52--61}
  \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Chatterjee:JTB:2012}
\bibinfo{author}{Chatterjee, K.}\DIFadd{, }\bibinfo{author}{Zufferey, D.} \DIFadd{\&
  }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Evolutionary game dynamics in populations with
  different learners}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{301}}\DIFadd{, }\bibinfo{pages}{161--173}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{trivers1971evolution}
\bibinfo{author}{Trivers, R.~L.}
\newblock \bibinfo{title}{The evolution of reciprocal altruism}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{The Quarterly review of biology}}
  \textbf{\bibinfo{volume}{46}}\DIFadd{, }\bibinfo{pages}{35--57}
  \DIFadd{(}\bibinfo{year}{1971}\DIFadd{).
}

\bibitem{rapoport:book:1965}
\bibinfo{author}{Rapoport, A.} \DIFadd{\& }\bibinfo{author}{Chammah, A.~M.}
\newblock \emph{\bibinfo{title}{Prisoner's Dilemma}}
  \DIFadd{(}\bibinfo{publisher}{University of Michigan Press, Ann Arbor}\DIFadd{,
  }\bibinfo{year}{1965}\DIFadd{).
}

\bibitem{axelrod1981evolution}
\bibinfo{author}{Axelrod, R.} \DIFadd{\& }\bibinfo{author}{Hamilton, W.~D.}
\newblock \bibinfo{title}{The evolution of cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Science}} \textbf{\bibinfo{volume}{211}}\DIFadd{,
  }\bibinfo{pages}{1390--1396} \DIFadd{(}\bibinfo{year}{1981}\DIFadd{).
}

\bibitem{molander:jcr:1985}
\bibinfo{author}{Molander, P.}
\newblock \bibinfo{title}{The optimal level of generosity in a selfish,
  uncertain environment}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Conflict Resolution}}
  \textbf{\bibinfo{volume}{29}}\DIFadd{, }\bibinfo{pages}{611--618}
  \DIFadd{(}\bibinfo{year}{1985}\DIFadd{).
}

\bibitem{Nowak1992tit}
\bibinfo{author}{Nowak, M.~A.} \DIFadd{\& }\bibinfo{author}{Sigmund, K.}
\newblock \bibinfo{title}{Tit for tat in heterogeneous populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{355}}\DIFadd{,
  }\bibinfo{pages}{250--253} \DIFadd{(}\bibinfo{year}{1992}\DIFadd{).
}

\bibitem{fischbacher:EconL:2001}
\bibinfo{author}{Fischbacher, U.}\DIFadd{, }\bibinfo{author}{G{\"a}chter, S.} \DIFadd{\&
  }\bibinfo{author}{Fehr, E.}
\newblock \bibinfo{title}{Are people conditionally cooperative? {E}vidence from
  a public goods experiment}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Economic Letters}}
  \textbf{\bibinfo{volume}{71}}\DIFadd{, }\bibinfo{pages}{397--404}
  \DIFadd{(}\bibinfo{year}{2001}\DIFadd{).
}

\bibitem{Rand:TCS:2013}
\bibinfo{author}{Rand, D.~G.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Human cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Trends in Cogn. Sciences}}
  \textbf{\bibinfo{volume}{117}}\DIFadd{, }\bibinfo{pages}{413--425}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{DalBo:AER:2019}
\bibinfo{author}{Dal~B{\'o}, P.} \DIFadd{\& }\bibinfo{author}{Fr{\'e}chette, G.~R.}
\newblock \bibinfo{title}{Strategy choice in the infinitely repeated prisoner's
  dilemma}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{American Economic Review}}
  \textbf{\bibinfo{volume}{109}}\DIFadd{, }\bibinfo{pages}{3929--3952}
  \DIFadd{(}\bibinfo{year}{2019}\DIFadd{).
}

\bibitem{Montero-Porras:SciRep:2022}
\bibinfo{author}{Montero-Porras, E.}\DIFadd{, }\bibinfo{author}{Gruji{\'c}, J.}\DIFadd{,
  }\bibinfo{author}{Fern{\'a}ndez~Domingos, E.} \DIFadd{\& }\bibinfo{author}{Lenaerts,
  T.}
\newblock \bibinfo{title}{Inferring strategies from observations in long
  iterated prisoner's dilemma experiments}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Scientific Reports}}
  \textbf{\bibinfo{volume}{12}}\DIFadd{, }\bibinfo{pages}{7589} \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Rossetti:ETH:2023}
\bibinfo{author}{Rossetti, C.} \DIFadd{\& }\bibinfo{author}{Hilbe, C.}
\newblock \bibinfo{title}{Direct reciprocity among humans}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Ethology}}
  \bibinfo{pages}{https://doi.org/10.1111/eth.13407} \DIFadd{(}\bibinfo{year}{2023}\DIFadd{).
}

\bibitem{Carter:PRSB:2013}
\bibinfo{author}{Carter, G.~G.} \DIFadd{\& }\bibinfo{author}{Wilkinson, G.~S.}
\newblock \bibinfo{title}{Food sharing in vampire bats, reciprocal help
  predicts donations more than relatedness or harassment}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the Royal Society B:
  Biological Sciences}} \textbf{\bibinfo{volume}{280}}\DIFadd{,
  }\bibinfo{pages}{20122573} \DIFadd{(}\bibinfo{year}{2013}\DIFadd{).
}

\bibitem{Schweinfurth:AnBehav:2019}
\bibinfo{author}{Schweinfurth, M.~K.}\DIFadd{, }\bibinfo{author}{Aeschbacher, J.}\DIFadd{,
  }\bibinfo{author}{Santi, M.} \DIFadd{\& }\bibinfo{author}{Taborsky, M.}
\newblock \bibinfo{title}{Male norway rats cooperate according to direct but
  not generalized reciprocity rules}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Animal Behaviour}}
  \textbf{\bibinfo{volume}{152}}\DIFadd{, }\bibinfo{pages}{93--101}
  \DIFadd{(}\bibinfo{year}{2019}\DIFadd{).
}

\bibitem{Voelkl:PNAS:2015}
\bibinfo{author}{Voelkl, B.} \emph{\DIFadd{et~al.}}
\newblock \bibinfo{title}{Matching times of leading and following suggest
  cooperation through direct reciprocity during {V}-formation flight in ibis}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the National Academy of
  Sciences USA}} \textbf{\bibinfo{volume}{112}}\DIFadd{, }\bibinfo{pages}{2115--2120}
  \DIFadd{(}\bibinfo{year}{2015}\DIFadd{).
}

\bibitem{CluttonBrock:Nature:2009}
\bibinfo{author}{Clutton-Brock, T.}
\newblock \bibinfo{title}{Cooperation between non-kin in animal societies}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{462}}\DIFadd{,
  }\bibinfo{pages}{51--57} \DIFadd{(}\bibinfo{year}{2009}\DIFadd{).
}

\bibitem{Silk:CurrentBiology:2013}
\bibinfo{author}{Silk, J.~B.}
\newblock \bibinfo{title}{Reciprocal altruism}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Current Biology}}
  \textbf{\bibinfo{volume}{23}}\DIFadd{, }\bibinfo{pages}{827--828}
  \DIFadd{(}\bibinfo{year}{2013}\DIFadd{).
}

\bibitem{taborsky:CurrentBiology:2013}
\bibinfo{author}{Taborsky, M.}
\newblock \bibinfo{title}{Social evolution: {R}eciprocity there is}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Current Biology}}
  \textbf{\bibinfo{volume}{23}}\DIFadd{, }\bibinfo{pages}{486--488}
  \DIFadd{(}\bibinfo{year}{2013}\DIFadd{).
}

\bibitem{brauchli:JTB:1999}
\bibinfo{author}{Brauchli, K.}\DIFadd{, }\bibinfo{author}{Killingback, T.} \DIFadd{\&
  }\bibinfo{author}{Doebeli, M.}
\newblock \bibinfo{title}{Evolution of cooperation in spatially structured
  populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{200}}\DIFadd{, }\bibinfo{pages}{405--417}
  \DIFadd{(}\bibinfo{year}{1999}\DIFadd{).
}

\bibitem{brandt:JTB:2006}
\bibinfo{author}{Brandt, H.} \DIFadd{\& }\bibinfo{author}{Sigmund, K.}
\newblock \bibinfo{title}{The good, the bad and the discriminator - errors in
  direct and indirect reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{239}}\DIFadd{, }\bibinfo{pages}{183--194}
  \DIFadd{(}\bibinfo{year}{2006}\DIFadd{).
}

\bibitem{ohtsuki:JTB:2007b}
\bibinfo{author}{Ohtsuki, H.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Direct reciprocity on graphs}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{247}}\DIFadd{, }\bibinfo{pages}{462--470}
  \DIFadd{(}\bibinfo{year}{2007}\DIFadd{).
}

\bibitem{szolnoki:pre:2009b}
\bibinfo{author}{Szolnoki, A.}\DIFadd{, }\bibinfo{author}{Perc, M.} \DIFadd{\&
  }\bibinfo{author}{Szab{\'{o}}, G.}
\newblock \bibinfo{title}{Phase diagrams for three-strategy evolutionary
  prisoner's dilemma games on regular graphs}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physical Review E}}
  \textbf{\bibinfo{volume}{80}}\DIFadd{, }\bibinfo{pages}{056104}
  \DIFadd{(}\bibinfo{year}{2009}\DIFadd{).
}

\bibitem{imhof2010stochastic}
\bibinfo{author}{Imhof, L.~A.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Stochastic evolutionary dynamics of direct
  reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the Royal Society B:
  Biological Sciences}} \textbf{\bibinfo{volume}{277}}\DIFadd{,
  }\bibinfo{pages}{463--468} \DIFadd{(}\bibinfo{year}{2010}\DIFadd{).
}

\bibitem{vansegbroeck:prl:2012}
\bibinfo{author}{van Segbroeck, S.}\DIFadd{, }\bibinfo{author}{Pacheco, J.~M.}\DIFadd{,
  }\bibinfo{author}{Lenaerts, T.} \DIFadd{\& }\bibinfo{author}{Santos, F.~C.}
\newblock \bibinfo{title}{Emergence of fairness in repeated group
  interactions}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physical Review Letters}}
  \textbf{\bibinfo{volume}{108}}\DIFadd{, }\bibinfo{pages}{158104}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{grujic:jtb:2012}
\bibinfo{author}{Grujic, J.}\DIFadd{, }\bibinfo{author}{Cuesta, J.~A.} \DIFadd{\&
  }\bibinfo{author}{Sanchez, A.}
\newblock \bibinfo{title}{On the coexistence of cooperators, defectors and
  conditional cooperators in the multiplayer iterated prisoner's dilemma}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{300}}\DIFadd{, }\bibinfo{pages}{299--308}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{Martinez2012}
\bibinfo{author}{Martinez-Vaquero, L.~A.}\DIFadd{, }\bibinfo{author}{Cuesta, J.~A.} \DIFadd{\&
  }\bibinfo{author}{Sanchez, A.}
\newblock \bibinfo{title}{Generosity pays in the presence of direct
  reciprocity: A comprehensive study of 2$\times$ 2 repeated games}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{{PLoS} One}} \textbf{\bibinfo{volume}{7}}\DIFadd{,
  }\bibinfo{pages}{e35135} \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{stewart:pnas:2013}
\bibinfo{author}{Stewart, A.~J.} \DIFadd{\& }\bibinfo{author}{Plotkin, J.~B.}
\newblock \bibinfo{title}{From extortion to generosity, evolution in the
  iterated prisoner's dilemma}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the National Academy of
  Sciences USA}} \textbf{\bibinfo{volume}{110}}\DIFadd{, }\bibinfo{pages}{15348--15353}
  \DIFadd{(}\bibinfo{year}{2013}\DIFadd{).
}

\bibitem{pinheiro:PLoSCB:2014}
\bibinfo{author}{Pinheiro, F.~L.}\DIFadd{, }\bibinfo{author}{Vasconcelos, V.~V.}\DIFadd{,
  }\bibinfo{author}{Santos, F.~C.} \DIFadd{\& }\bibinfo{author}{Pacheco, J.~M.}
\newblock \bibinfo{title}{Evolution of all-or-none strategies in repeated
  public goods dilemmas}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{PLoS Comput Biol}}
  \textbf{\bibinfo{volume}{10}}\DIFadd{, }\bibinfo{pages}{e1003945}
  \DIFadd{(}\bibinfo{year}{2014}\DIFadd{).
}

\bibitem{stewart:games:2015}
\bibinfo{author}{Stewart, A.~J.} \DIFadd{\& }\bibinfo{author}{Plotkin, J.~B.}
\newblock \bibinfo{title}{The evolvability of cooperation under local and
  non-local mutations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Games}} \textbf{\bibinfo{volume}{6}}\DIFadd{,
  }\bibinfo{pages}{231--250} \DIFadd{(}\bibinfo{year}{2015}\DIFadd{).
}

\bibitem{Baek2016}
\bibinfo{author}{Baek, S.~K.}\DIFadd{, }\bibinfo{author}{Jeong, H.-C.}\DIFadd{,
  }\bibinfo{author}{Hilbe, C.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Comparing reactive and memory-one strategies of
  direct reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Scientific reports}}
  \textbf{\bibinfo{volume}{6}}\DIFadd{, }\bibinfo{pages}{1--13} \DIFadd{(}\bibinfo{year}{2016}\DIFadd{).
}

\bibitem{McAvoy:ProcA:2019}
\bibinfo{author}{McAvoy, A.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Reactive learning strategies for iterated games}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the Royal Society A}}
  \textbf{\bibinfo{volume}{475}}\DIFadd{, }\bibinfo{pages}{20180819}
  \DIFadd{(}\bibinfo{year}{2019}\DIFadd{).
}

\bibitem{glynatsi:SCR:2020}
\bibinfo{author}{Glynatsi, N.~E.} \DIFadd{\& }\bibinfo{author}{Knight, V.~A.}
\newblock \bibinfo{title}{Using a theory of mind to find best responses to
  memory-one strategies}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Scientific Reports}}
  \textbf{\bibinfo{volume}{10}}\DIFadd{, }\bibinfo{pages}{17287} \DIFadd{(}\bibinfo{year}{2020}\DIFadd{).
}

\bibitem{Schmid:PlosCB:2022}
\bibinfo{author}{Schmid, L.}\DIFadd{, }\bibinfo{author}{Hilbe, C.}\DIFadd{,
  }\bibinfo{author}{Chatterjee, K.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Direct reciprocity between individuals that use
  different strategy spaces}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{PLoS Computational Biology}}
  \textbf{\bibinfo{volume}{18}}\DIFadd{, }\bibinfo{pages}{e1010149}
  \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Murase:SciRep:2022}
\bibinfo{author}{Murase, Y.}\DIFadd{, }\bibinfo{author}{Hilbe, C.} \DIFadd{\&
  }\bibinfo{author}{Baek, S.~K.}
\newblock \bibinfo{title}{Evolution of direct reciprocity in group-structured
  populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Scientific Reports}}
  \textbf{\bibinfo{volume}{12}}\DIFadd{, }\bibinfo{pages}{18645} \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Cooney:BMB:2022}
\bibinfo{author}{Cooney, D.~B.}
\newblock \bibinfo{title}{Assortment and reciprocity mechanisms for promotion
  of cooperation in a model of multilevel selection}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Bulletin of Mathematical Biology}}
  \textbf{\bibinfo{volume}{84}}\DIFadd{, }\bibinfo{pages}{126} \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Chen:PNASNexus:2023}
\bibinfo{author}{Chen, X.} \DIFadd{\& }\bibinfo{author}{Fu, F.}
\newblock \bibinfo{title}{Outlearning extortioners: {U}nbending strategies can
  foster reciprocal fairness and cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{PNAS {N}exus}} \textbf{\bibinfo{volume}{2}}\DIFadd{,
  }\bibinfo{pages}{pgad176} \DIFadd{(}\bibinfo{year}{2023}\DIFadd{).
}

\bibitem{Hauert1997}
\bibinfo{author}{Hauert, C.} \DIFadd{\& }\bibinfo{author}{Schuster, H.~G.}
\newblock \bibinfo{title}{Effects of increasing the number of players and
  memory size in the iterated prisoner's dilemma: a numerical approach}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the Royal Society of London.
  Series B: Biological Sciences}} \textbf{\bibinfo{volume}{264}}\DIFadd{,
  }\bibinfo{pages}{513--519} \DIFadd{(}\bibinfo{year}{1997}\DIFadd{).
}

\bibitem{vanveelen:PNAS:2012}
\bibinfo{author}{van Veelen, M.}\DIFadd{, }\bibinfo{author}{Garcia, J.}\DIFadd{,
  }\bibinfo{author}{Rand, D.~G.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Direct reciprocity in structured populations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the National Academy of
  Sciences USA}} \textbf{\bibinfo{volume}{109}}\DIFadd{, }\bibinfo{pages}{9929--9934}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{Stewart2016}
\bibinfo{author}{Stewart, A.~J.} \DIFadd{\& }\bibinfo{author}{Plotkin, J.~B.}
\newblock \bibinfo{title}{Small groups and long memories promote cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Scientific reports}}
  \textbf{\bibinfo{volume}{6}}\DIFadd{, }\bibinfo{pages}{1--11} \DIFadd{(}\bibinfo{year}{2016}\DIFadd{).
}

\bibitem{hilbe:PNAS:2017}
\bibinfo{author}{Hilbe, C.}\DIFadd{, }\bibinfo{author}{Martinez-Vaquero, L.~A.}\DIFadd{,
  }\bibinfo{author}{Chatterjee, K.} \DIFadd{\& }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Memory n strategies of direct reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the National Academy of
  Sciences}} \textbf{\bibinfo{volume}{114}}\DIFadd{, }\bibinfo{pages}{4715--4720}
  \DIFadd{(}\bibinfo{year}{2017}\DIFadd{).
}

\bibitem{Li:NatCS:2022}
\bibinfo{author}{Li, J.} \emph{\DIFadd{et~al.}}
\newblock \bibinfo{title}{Evolution of cooperation through cumulative
  reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature Computational Science}}
  \textbf{\bibinfo{volume}{2}}\DIFadd{, }\bibinfo{pages}{677--686}
  \DIFadd{(}\bibinfo{year}{2022}\DIFadd{).
}

\bibitem{Murase:PLoSCompBio:2023a}
\bibinfo{author}{Murase, Y.} \DIFadd{\& }\bibinfo{author}{Baek, S.~K.}
\newblock \bibinfo{title}{Grouping promotes both partnership and rivalry with
  long memory in direct reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{PLoS Computational Biology}}
  \textbf{\bibinfo{volume}{19}}\DIFadd{, }\bibinfo{pages}{e1011228}
  \DIFadd{(}\bibinfo{year}{2023}\DIFadd{).
}

\bibitem{sigmund2010calculus}
\bibinfo{author}{Sigmund, K.}
\newblock \emph{\bibinfo{title}{The calculus of selfishness}}
  \DIFadd{(}\bibinfo{publisher}{Princeton University Press}\DIFadd{, }\bibinfo{year}{2010}\DIFadd{).
}

\bibitem{friedman:RES:1971}
\bibinfo{author}{Friedman, J.}
\newblock \bibinfo{title}{A non-cooperative equilibrium for supergames}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Review of Economic Studies}}
  \textbf{\bibinfo{volume}{38}}\DIFadd{, }\bibinfo{pages}{1--12} \DIFadd{(}\bibinfo{year}{1971}\DIFadd{).
}

\bibitem{Akin:chapter:2016}
\bibinfo{author}{Akin, E.}
\newblock \bibinfo{title}{The iterated prisoner's dilemma: {G}ood strategies
  and their dynamics}\DIFadd{.
}\newblock \DIFadd{In }\bibinfo{editor}{Assani, I.} \DIFadd{(ed.)
  }\emph{\bibinfo{booktitle}{Ergodic Theory, Advances in Dynamics}}\DIFadd{,
  }\bibinfo{pages}{77--107} \DIFadd{(}\bibinfo{publisher}{de Gruyter}\DIFadd{,
  }\bibinfo{address}{Berlin}\DIFadd{, }\bibinfo{year}{2016}\DIFadd{).
}

\bibitem{hilbe:GEB:2015}
\bibinfo{author}{Hilbe, C.}\DIFadd{, }\bibinfo{author}{Traulsen, A.} \DIFadd{\&
  }\bibinfo{author}{Sigmund, K.}
\newblock \bibinfo{title}{Partners or rivals? {S}trategies for the iterated
  prisoner's dilemma}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Games and Economic Behavior}}
  \textbf{\bibinfo{volume}{92}}\DIFadd{, }\bibinfo{pages}{41--52}
  \DIFadd{(}\bibinfo{year}{2015}\DIFadd{).
}

\bibitem{stewart:pnas:2014}
\bibinfo{author}{Stewart, A.~J.} \DIFadd{\& }\bibinfo{author}{Plotkin, J.~B.}
\newblock \bibinfo{title}{Collapse of cooperation in evolving games}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Proceedings of the National Academy of
  Sciences USA}} \textbf{\bibinfo{volume}{111}}\DIFadd{, }\bibinfo{pages}{17558 --
  17563} \DIFadd{(}\bibinfo{year}{2014}\DIFadd{).
}

\bibitem{traulsen2007pairwise}
\bibinfo{author}{Traulsen, A.}\DIFadd{, }\bibinfo{author}{Pacheco, J.~M.} \DIFadd{\&
  }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Pairwise comparison and selection temperature in
  evolutionary game dynamics}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of theoretical biology}}
  \textbf{\bibinfo{volume}{246}}\DIFadd{, }\bibinfo{pages}{522--529}
  \DIFadd{(}\bibinfo{year}{2007}\DIFadd{).
}

\bibitem{blume:GEB:1995}
\bibinfo{author}{Blume, L.~E.}
\newblock \bibinfo{title}{The statistical mechanics of best-response strategy
  revision}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Games and Economic Behavior}}
  \textbf{\bibinfo{volume}{11}}\DIFadd{, }\bibinfo{pages}{111--145}
  \DIFadd{(}\bibinfo{year}{1995}\DIFadd{).
}

\bibitem{szabo:PRE:1998}
\bibinfo{author}{Szab{\'o}, G.} \DIFadd{\& }\bibinfo{author}{T{\H o}ke, C.}
\newblock \bibinfo{title}{Evolutionary {P}risoner's {D}ilemma game on a square
  lattice}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physical Review E}}
  \textbf{\bibinfo{volume}{58}}\DIFadd{, }\bibinfo{pages}{69--73}
  \DIFadd{(}\bibinfo{year}{1998}\DIFadd{).
}

\bibitem{fudenberg:JET:2006}
\bibinfo{author}{Fudenberg, D.} \DIFadd{\& }\bibinfo{author}{Imhof, L.~A.}
\newblock \bibinfo{title}{Imitation processes with small mutations}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Economic Theory}}
  \textbf{\bibinfo{volume}{131}}\DIFadd{, }\bibinfo{pages}{251--262}
  \DIFadd{(}\bibinfo{year}{2006}\DIFadd{).
}

\bibitem{wu:JMB:2012}
\bibinfo{author}{Wu, B.}\DIFadd{, }\bibinfo{author}{Gokhale, C.~S.}\DIFadd{,
  }\bibinfo{author}{Wang, L.} \DIFadd{\& }\bibinfo{author}{Traulsen, A.}
\newblock \bibinfo{title}{How small are small mutation rates?}
\newblock \emph{\bibinfo{journal}{Journal of Mathematical Biology}}
  \textbf{\bibinfo{volume}{64}}\DIFadd{, }\bibinfo{pages}{803--827}
  \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{mcavoy:jet:2015}
\bibinfo{author}{McAvoy, A.}
\newblock \bibinfo{title}{Comment on ``{I}mitation processes with small
  mutations''}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{J. Econ. Theory}}
  \textbf{\bibinfo{volume}{159}}\DIFadd{, }\bibinfo{pages}{66--69}
  \DIFadd{(}\bibinfo{year}{2015}\DIFadd{).
}

\bibitem{Schmid:NHB:2021}
\bibinfo{author}{Schmid, L.}\DIFadd{, }\bibinfo{author}{Chatterjee, K.}\DIFadd{,
  }\bibinfo{author}{Hilbe, C.} \DIFadd{\& }\bibinfo{author}{Nowak, M.}
\newblock \bibinfo{title}{A unified framework of direct and indirect
  reciprocity}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature Human Behaviour}}
  \textbf{\bibinfo{volume}{5}}\DIFadd{, }\bibinfo{pages}{1292--1302}
  \DIFadd{(}\bibinfo{year}{2021}\DIFadd{).
}

\bibitem{mcavoy:NHB:2020}
\bibinfo{author}{McAvoy, A.}\DIFadd{, }\bibinfo{author}{Allen, B.} \DIFadd{\&
  }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Social goods dilemmas in heterogeneous societies}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature Human Behaviour}}
  \textbf{\bibinfo{volume}{4}}\DIFadd{, }\bibinfo{pages}{819--831}
  \DIFadd{(}\bibinfo{year}{2020}\DIFadd{).
}

\bibitem{nowak:Nature:1993}
\bibinfo{author}{Nowak, M.~A.} \DIFadd{\& }\bibinfo{author}{Sigmund, K.}
\newblock \bibinfo{title}{A strategy of win-stay, lose-shift that outperforms
  tit-for-tat in the {P}risoner's {D}ilemma game}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Nature}} \textbf{\bibinfo{volume}{364}}\DIFadd{,
  }\bibinfo{pages}{56--58} \DIFadd{(}\bibinfo{year}{1993}\DIFadd{).
}

\bibitem{imhof:JTB:2007}
\bibinfo{author}{Imhof, L.~A.}\DIFadd{, }\bibinfo{author}{Fudenberg, D.} \DIFadd{\&
  }\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Tit-for-tat or win-stay, lose-shift?}
\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{247}}\DIFadd{, }\bibinfo{pages}{574--580}
  \DIFadd{(}\bibinfo{year}{2007}\DIFadd{).
}

\bibitem{doebeli:Ecology:2005}
\bibinfo{author}{Doebeli, M.} \DIFadd{\& }\bibinfo{author}{Hauert, C.}
\newblock \bibinfo{title}{Models of cooperation based on the prisoner's dilemma
  and the snowdrift game}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Ecology letters}}
  \textbf{\bibinfo{volume}{8}}\DIFadd{, }\bibinfo{pages}{748--766}
  \DIFadd{(}\bibinfo{year}{2005}\DIFadd{).
}

\bibitem{nowak:JTB:2012}
\bibinfo{author}{Nowak, M.~A.}
\newblock \bibinfo{title}{Evolving cooperation}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{299}}\DIFadd{, }\bibinfo{pages}{1--8} \DIFadd{(}\bibinfo{year}{2012}\DIFadd{).
}

\bibitem{stephens1986foraging}
\bibinfo{author}{Stephens, D.~W.} \DIFadd{\& }\bibinfo{author}{Krebs, J.~R.}
\newblock \emph{\bibinfo{title}{Foraging Theory}}\DIFadd{.
}\newblock \DIFadd{Monographs in Behavior and Ecology (}\bibinfo{publisher}{Princeton
  University Press}\DIFadd{, }\bibinfo{year}{1986}\DIFadd{).
}

\bibitem{sutton:MIT:2018}
\bibinfo{author}{Sutton, R.~S.} \DIFadd{\& }\bibinfo{author}{Barto, A.~G.}
\newblock \emph{\bibinfo{title}{Reinforcement Learning: An Introduction}}
  \DIFadd{(}\bibinfo{publisher}{The MIT Press}\DIFadd{, }\bibinfo{year}{2018}\DIFadd{),
  }\bibinfo{edition}{second} \DIFadd{edn.
}

\bibitem{schultz:JN:1998}
\bibinfo{author}{Schultz, W.}
\newblock \bibinfo{title}{{Predictive Reward Signal of Dopamine Neurons}}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Neurophysiology}}
  \textbf{\bibinfo{volume}{80}}\DIFadd{, }\bibinfo{pages}{1--27} \DIFadd{(}\bibinfo{year}{1998}\DIFadd{).
}

\bibitem{dan:Neuron:2004}
\bibinfo{author}{Dan, Y.} \DIFadd{\& }\bibinfo{author}{Poo, M.~M.}
\newblock \bibinfo{title}{{Spike Timing-Dependent Plasticity of Neural
  Circuits}}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Neuron}} \textbf{\bibinfo{volume}{44}}\DIFadd{,
  }\bibinfo{pages}{23--30} \DIFadd{(}\bibinfo{year}{2004}\DIFadd{).
}

\bibitem{murdock:JEP:1962}
\bibinfo{author}{Murdock, B.~B.}
\newblock \bibinfo{title}{The serial position effect of free recall.}
\newblock \emph{\bibinfo{journal}{Journal of Experimental Psychology}}
  \textbf{\bibinfo{volume}{64}}\DIFadd{, }\bibinfo{pages}{482--488}
  \DIFadd{(}\bibinfo{year}{1962}\DIFadd{).
}

\bibitem{mcavoy:PLOSCB:2021}
\bibinfo{author}{McAvoy, A.}\DIFadd{, }\bibinfo{author}{Rao, A.} \DIFadd{\&
  }\bibinfo{author}{Hauert, C.}
\newblock \bibinfo{title}{Intriguing effects of selection intensity on the
  evolution of prosocial behaviors}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{{PLOS} Computational Biology}}
  \textbf{\bibinfo{volume}{17}}\DIFadd{, }\bibinfo{pages}{e1009611}
  \DIFadd{(}\bibinfo{year}{2021}\DIFadd{).
}

\bibitem{cavalli1981cultural}
\bibinfo{author}{Cavalli-Sforza, L.~L.} \DIFadd{\& }\bibinfo{author}{Feldman, M.~W.}
\newblock \emph{\bibinfo{title}{Cultural Transmission and Evolution: A
  Quantitative Approach}}\DIFadd{.
}\newblock \DIFadd{Cultural Transmission and Evolution: A Quantitative Approach
  (}\bibinfo{publisher}{Princeton University Press}\DIFadd{, }\bibinfo{year}{1981}\DIFadd{).
}

\bibitem{sanchez:JTB:2005}
\bibinfo{author}{S{\'a}nchez, A.} \DIFadd{\& }\bibinfo{author}{Cuesta, J.~A.}
\newblock \bibinfo{title}{Altruism may arise from individual selection}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of theoretical biology}}
  \textbf{\bibinfo{volume}{235}}\DIFadd{, }\bibinfo{pages}{233--240}
  \DIFadd{(}\bibinfo{year}{2005}\DIFadd{).
}

\bibitem{roca:PhysicalReview:2006}
\bibinfo{author}{Roca, C.~P.}\DIFadd{, }\bibinfo{author}{Cuesta, J.~A.} \DIFadd{\&
  }\bibinfo{author}{S{\'a}nchez, A.}
\newblock \bibinfo{title}{Time scales in evolutionary dynamics}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physical review letters}}
  \textbf{\bibinfo{volume}{97}}\DIFadd{, }\bibinfo{pages}{158701}
  \DIFadd{(}\bibinfo{year}{2006}\DIFadd{).
}

\bibitem{Traulsen:JTB:2007}
\bibinfo{author}{Traulsen, A.}\DIFadd{, }\bibinfo{author}{Nowak, M.~A.} \DIFadd{\&
  }\bibinfo{author}{Pacheco, J.~M.}
\newblock \bibinfo{title}{Stochastic payoff evaluation increases the
  temperature of selection}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of theoretical biology}}
  \textbf{\bibinfo{volume}{244}}\DIFadd{, }\bibinfo{pages}{349--356}
  \DIFadd{(}\bibinfo{year}{2007}\DIFadd{).
}

\bibitem{Woelfing:JTB:2009}
\bibinfo{author}{Woelfing, B.} \DIFadd{\& }\bibinfo{author}{Traulsen, A.}
\newblock \bibinfo{title}{Stochastic sampling of interaction partners versus
  deterministic payoff assignment}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Journal of Theoretical Biology}}
  \textbf{\bibinfo{volume}{257}}\DIFadd{, }\bibinfo{pages}{689--695}
  \DIFadd{(}\bibinfo{year}{2009}\DIFadd{).
}

\bibitem{Hauert:PRE:2018}
\bibinfo{author}{Hauert, C.} \DIFadd{\& }\bibinfo{author}{Miekisz, J.}
\newblock \bibinfo{title}{Effects of sampling interaction partners and
  competitors in evolutionary games}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physical Review E}}
  \textbf{\bibinfo{volume}{98}}\DIFadd{, }\bibinfo{pages}{052301}
  \DIFadd{(}\bibinfo{year}{2018}\DIFadd{).
}

\bibitem{nowak:APC:1989}
\bibinfo{author}{Nowak, M.} \DIFadd{\& }\bibinfo{author}{Sigmund, K.}
\newblock \bibinfo{title}{Game-dynamical aspects of the prisoner's dilemma}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Applied Mathematics and Computation}}
  \textbf{\bibinfo{volume}{30}}\DIFadd{, }\bibinfo{pages}{191--213}
  \DIFadd{(}\bibinfo{year}{1989}\DIFadd{).
}

\bibitem{szabo:PR:2007}
\bibinfo{author}{Szab{\'o}, G.} \DIFadd{\& }\bibinfo{author}{F{\'a}th, G.}
\newblock \bibinfo{title}{Evolutionary games on graphs}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Physics Reports}}
  \textbf{\bibinfo{volume}{446}}\DIFadd{, }\bibinfo{pages}{97--216}
  \DIFadd{(}\bibinfo{year}{2007}\DIFadd{).
}

\bibitem{Stevens:fip:2011}
\bibinfo{author}{Stevens, J.~R.}\DIFadd{, }\bibinfo{author}{Volstorf, J.}\DIFadd{,
  }\bibinfo{author}{Schooler, L.~J.} \DIFadd{\& }\bibinfo{author}{Rieskamp, J.}
\newblock \bibinfo{title}{Forgetting constrains the emergence of cooperative
  decision strategies}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{Frontiers in Psychology}}
  \textbf{\bibinfo{volume}{1}}\DIFadd{, }\bibinfo{pages}{235} \DIFadd{(}\bibinfo{year}{2011}\DIFadd{).
}

\bibitem{Volstorf:PlosOne:2011}
\bibinfo{author}{Volstorf, J.}\DIFadd{, }\bibinfo{author}{Rieskamp, J.} \DIFadd{\&
  }\bibinfo{author}{Stevens, J.~R.}
\newblock \bibinfo{title}{The good, the bad, and the rare: Memory for partners
  in social interactions}\DIFadd{.
}\newblock \emph{\bibinfo{journal}{PloS one}} \textbf{\bibinfo{volume}{6}}\DIFadd{,
  }\bibinfo{pages}{e18945} \DIFadd{(}\bibinfo{year}{2011}\DIFadd{).
}

\end{thebibliography}
\DIFaddend 


%%%%%%%%%%
%% FIGURES  %%
%%%%%%%%%%

\clearpage
\newpage

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{donation_expected_last_round_summary_results.pdf}
    \caption{{\bf Evolutionary dynamics under perfect and limited payoff memory.}
    The leftmost panels give a schematic overview of the two main scenarios we compare. 
    The two scenarios differ in how many past interactions individuals take into account when updating their strategy. 
    In the scenario with perfect payoff memory, individuals consider all their past interactions (against all population members, and taking every turn of each repeated game into account). 
    In the scenario with limited payoff memory, individuals only consider their very last interaction (against one specific population member, taking into account only one round of the repeated game). 
    The four panels  on the right side depict the outcome of evolutionary simulations for repeated games with either a low or a high benefit of cooperation. 
    Colors represent how often the respective region of the strategy space is visited over time. 
    In all four panels, two regions are visited particularly often. 
    One region corresponds to a neighborhood of \alld{} with $p\approx q\!\approx\!0$ (lower left corner). 
    The other region corresponds to a strip of conditionally cooperative strategies with $p\!\approx\! 1$ and $q$ satisfying the constraints \eqref{Eq:PerfectMemory} and \eqref{Eq:LimitedMemory}, respectively (lower right corner). 
    The resulting average cooperation rate depends on which of these two neighborhoods is visited more often.
    Simulations are run for $T\!=\!10^7$ time steps, using a cost $c\!=\!1$, a continuation probability of $\delta\!=\!0.999$ and a selection strength of $\beta\!=\!1$, in a population of size $N\!=\!100$.}
\label{fig:expected_and_stochastic_for_donation}
\end{figure}


\clearpage
\newpage


\begin{figure}[t]
    \centering
    \includegraphics[width=.75\textwidth]{cooperation_rate_over_b_and_beta.pdf}
    \caption{{\bf Evolution of direct reciprocity for different parameter values.} 
    To explore the robustness of our results, we have run simulations for different benefit values (left panels, {\it a}), and for different selection strengths (right panels, {\it b}). 
    In each case, we record the resulting average cooperation rate over the entire simulation (upper panels). 
    In addition we record the individuals' average generosity. 
    Here, we only take into account those residents with $p\! \approx\! 1$ and we compute
    the average of their cooperation probability~$q$. 
    These simulations suggest that perfect payoff memory consistently leads to more cooperation and more generosity. 
    Unless explicitly varied, the parameters of the simulation are $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.99$.
    Simulations are run for $T\!=\!5\times 10^7$ time steps, and each point represents a single simulation run.}
    \label{fig:cooperation_rate_over_benefit_and_beta}
\end{figure}

\clearpage
\newpage

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{more_memory_summary_results.pdf}
  \caption{{\bf Average cooperation rates for different payoff memories.}
  We vary how much information individuals take into account when updating their strategies. 
  From left to right, we consider the following cases.
  ({\it i}) Updating occurs based on expected payoffs (perfect memory), 
  ({\it ii}) it occurs based on the last round of one interaction (limited memory), 
  ({\it iii}) based on the last round of two interactions,
  ({\it iv}) based on the last two rounds of one interaction,
  ({\it v}) based on the last two rounds of two interactions, and
  ({\it vi}) based on the average payoff of one interaction.
  Again, simulations are run either for a comparably low benefit of cooperation~($b/c\!=\!3$), or for a high benefit~($b/c\!=\!10$). 
  We observe that perfect memory always yields the highest cooperation rate. 
  However, when individuals take into account at least two past interactions -- cases ({\it iii}) to ({\it vi}) -- evolving cooperation rates are close to this optimum. 
  Simulations are run for $T\!=\!10^7$ time steps, based on the parameters $N\!=\!100$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.999$.}
\label{fig:cooperation_rate_all_updating_payoffs}
\end{figure}

\end{document}