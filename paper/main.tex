\documentclass[11pt]{article}
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,color,array,eurosym,multirow}
\usepackage[sort&compress,comma,round,numbers]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage[ruled,vlined]{algorithm2e}
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.3}
\newcommand{\R}{\mathbb{R}}

\newcommand{\todo}[1]{\textcolor{blue}{#1}}

\def\alld{\texttt{ALLD}}
\def\tft{\texttt{TFT}}
\def\gtft{\texttt{GTFT}}
\def\rolemodel{\text{RM}}
\def\learner{\text{L}}
\def\resident{\texttt{R}}
\def\mutant{\texttt{M}}
\def\strategy{s}
\def\esm{electronic supplementary material}

%% Adding shortcut commands to refer to our figures %%
\newcommand{\FigBaseResults}{Figure~\ref{fig:expected_and_stochastic_for_donation}}
\newcommand{\FigDependenceParameters}{Figure~\ref{fig:cooperation_rate_over_benefit_and_beta}} 
\newcommand{\FigHigherMemory}{Figure~\ref{fig:cooperation_rate_all_updating_payoffs}}

\newcommand{\FigInvasionAnalysis}{{Supplementary Figure~3}} 
\newcommand{\FigMemoryOneParameters}{Supplementary Figure~7}

\titleformat{\section}{\sffamily \fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sffamily
\fontsize{11.5}{11.5}\bfseries}{\thesubsection}{1em}{}

\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{treenode/.style = {align=center, inner sep=0pt, text centered,
  font=\sffamily}, arn_n/.style = {treenode, circle, white,
  font=\sffamily\bfseries, draw=black, inner sep=-6pt, fill=black, text
  width=1.5em},% arbre rouge noir, noeud noir
  arn_r/.style = {treenode, circle, red, text width=1.5em, very thick, inner
    sep=4pt},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black, minimum width=0.5em, minimum
    height=0.5em}% arbre rouge noir, nil
}

\newtheoremstyle{plainCl1}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\newtheoremstyle{plainCl2}% name
{9pt}%      Space above, empty = 'usual value'
{9pt}%      Space below
{\it}% 	   Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{$'$.}%        Punctuation after thm head
{0.2cm}% Space after thm head: \newline = linebreak
{}%         Thm head spec


\theoremstyle{plainCl1}
\newtheorem{Claim}{Claim}
\newtheorem{Thm}{Theorem}
\newtheorem{Prop}{Proposition}
\newtheorem*{Lem}{Lemma}
\newtheorem{Cor}{Corollary}
\newtheorem*{Def}{Definition}

\theoremstyle{plainCl2}
\newtheorem{Claim2}{Claim}




\title{\bf  \sffamily \Large Evolution of reciprocity %among individuals\\ 
with limited payoff memory\\}
\date{}
\author{\parbox[c]{16cm}{\centering \onehalfspacing 
Nikoleta E. Glynatsi$^1$,  Alex McAvoy$^{2,3}$, Christian Hilbe$^1$\\ \quad \\
$^{\rm 1}$Max Planck Research Group on the Dynamics of Social Behavior,\\ Max Planck Institute for Evolutionary Biology, Pl\"{o}n, Germany \\
$^{\rm 2}$School of Data Science and Society, University of North Carolina at Chapel Hill,\\ Chapel Hill, NC 27599 \\
$^{\rm 3}$Department of Mathematics, University of North Carolina at Chapel Hill,\\ Chapel Hill, NC 27599}}

\begin{document}
\maketitle


\begin{abstract}
\noindent
Direct reciprocity is a mechanism for the evolution of cooperation in repeated social interactions. 
According to this literature, individuals naturally learn to adopt conditionally cooperative strategies if they have multiple encounters with their partner. 
Corresponding models have greatly facilitated our understanding of cooperation, yet they often make strong assumptions on how individuals remember and process payoff information. 
For example, when strategies are updated through social learning, it is commonly assumed that individuals compare their average payoffs.  
This would require them to compute (or remember) their payoffs against everyone else in the population.
To understand how more realistic constraints influence direct reciprocity, we consider the evolution of conditional behaviors when individuals learn based on more recent experiences.
Even in the most extreme case that they only take into account their very last interaction, we find that cooperation can still evolve. 
However, such individuals adopt less generous strategies, and they tend to cooperate less often than in the classical setup with average payoffs. 
Interestingly, once individuals remember the payoffs of two or three recent interactions, cooperation rates quickly approach the classical limit. 
These findings contribute to a literature that explores which kind of cognitive capabilities are required for reciprocal cooperation. 
While our results suggest that some rudimentary form of payoff memory is necessary, it already suffices to remember a few interactions.
\end{abstract}

~\\
{\it Keywords:} Evolution of cooperation; direct reciprocity; repeated prisoner's dilemma; social learning; evolutionary dynamics



\clearpage
\newpage


%%%%%%%%%
%%  INTRO  %%
%%%%%%%%%

\section{Introduction}

%% INTRO: General introduction to evolutionary game theory and cognitive constrains

Evolutionary game theory describes the dynamics of populations when an individual's fitness depends on the traits or strategies of other population members~\cite{hofbauer1998evolutionary, nowak:Nature:2004, hauert2005game,Traulsen:PhilTrans:2022}.  
This theory can be used to describe the dynamics of animal conflict~\citep{maynard-smith:Nature:1973}, cancer cells~\citep{Stein:PTRS:2023}, and of cooperation~\citep{nowak:Science:2006}. 
Respective models translate strategic interactions into games~\cite{smith1982evolution}. 
These games specify how individuals (players) interact, which strategies individuals can choose, and what fitness consequences (or payoffs) the different strategies have. 
In addition, these models also specify the mode by which successful strategies spread over time. 
In models of biological evolution, individuals with a high fitness produce more offspring; in models of cultural evolution, such individuals are imitated more often. 
Although biological and cultural evolution are sometimes treated as equivalent, there can be important differences~\citep{Wu2015,Smolla:PTRS:2021,Denton:TPB:2022}. 
For example, models of biological evolution do not require individuals to have any particular cognitive abilities.
Here, it is the evolutionary process itself that biases the population towards strategies with higher fitness. 
In contrast, in models of cultural evolution, individuals need to be aware of the different strategies present in the population, and they need to identify those strategies with a higher payoff. 
As a consequence, evolutionary outcomes may depend on how easily different behaviors can be learned~\citep{Chatterjee:JTB:2012}, and on how easy payoff comparisons are. 

%% INTRO: Literature on direct reciprocity and why social learning of strategies is hard

These difficulties to learn strategies by social imitation are particularly pronounced in models of direct reciprocity. 
This literature follows Trivers' insight that individuals have more of an incentive to cooperate in social dilemmas when they interact repeatedly~\citep{trivers1971evolution}. 
In repeated interactions, individuals can condition their behavior on their past experiences with their interaction partner. 
They may use strategies such as Tit-for-Tat~\citep{rapoport:book:1965,axelrod1981evolution} or Generous Tit-for-Tat~\citep{molander:jcr:1985,Nowak1992tit} to preferentially cooperate with other cooperators. 
Such conditional strategies approximate human behavior fairly well~\citep{fischbacher:EconL:2001,Rand:TCS:2013,DalBo:AER:2019,Montero-Porras:SciRep:2022,Rossetti:ETH:2023} and they have also been documented in several other species~\citep{Carter:PRSB:2013,Schweinfurth:AnBehav:2019,Voelkl:PNAS:2015} -- although direct reciprocity is generally more difficult to demonstrate in animals~\citep{CluttonBrock:Nature:2009,Silk:CurrentBiology:2013,taborsky:CurrentBiology:2013}.
However, at the outset, it is not clear how easy it is to {\it learn} reciprocal strategies by social imitation. 
As one obstacle, even if others' strategies are perfectly observable, individuals might find it difficult to identify which ones have the highest payoff. 
After all, the payoff of a strategy of direct reciprocity is not determined by the outcome of any single round.
Rather, it is determined by how well this strategy fares over an entire sequence of rounds, against many different population members. 
In practice, such information might be both difficult to obtain and to process. 

%% INTRO: Gap in the literature and what  question we wish to address. 

Most models of direct reciprocity abstract from these difficulties~\citep{brauchli:JTB:1999,brandt:JTB:2006,ohtsuki:JTB:2007b,szolnoki:pre:2009b,imhof2010stochastic,van-segbroeck:prl:2012,grujic:jtb:2012,Martinez2012,stewart:pnas:2013,pinheiro:PLoSCB:2014,stewart:games:2015,Baek2016,McAvoy:ProcA:2019,glynatsi:SCR:2020,Schmid:PlosCB:2022,Murase:SciRep:2022,Cooney:BMB:2022,Chen:PNASNexus:2023}. 
They just assume individuals can easily copy the strategies of others. 
Similarly, they just assume that updating decisions are based on the strategies' average (or expected) payoffs, which are based on all rounds and all interactions. 
These assumptions create a curious inconsistency in how models represent an individual's cognitive abilities. 
On the one hand, when playing the game, individuals are often assumed to have restricted memory. 
Respective studies typically assume that individuals make their decisions each round based on the outcome of the last round only~\citep[with only a few exceptions, see Refs.][]{Hauert1997,van-veelen:PNAS:2012,Stewart2016,Li:NatCS:2022,Murase:PLoSCompBio:2023a}. 
Yet when learning new strategies, individuals are assumed to remember (or compute) each others' precise average payoff across many rounds and many interaction partners. 
Herein, we wish to explore whether this latter assumption is actually necessary for the evolution of reciprocity through social imitation. 
We ask whether individuals can learn to adopt reciprocal strategies even when learning is based on payoff information from a limited number of rounds. 


%% INTRO: Basic description of our model and our findings 

To explore that question, we theoretically study imitation dynamics in the repeated prisoner's dilemma, using two extreme scenarios. 
The first scenario is the usual modeling approach. 
Here, individuals update their strategies based on their expected payoffs. 
We contrast this model with an alternative scenario where individuals update their strategies based on the very
last (one-shot) payoff they obtained. 
We find that individuals with limited payoff memory tend to adopt less generous strategies. 
Yet moderate levels of cooperation can still evolve. 
Moreover, as we increase the individuals' payoff memory to include the last two or three one-shot payoffs, cooperation rates quickly approach the rates observed in the classical baseline case. 

%% INTRO: Summary and conclusions

Overall, these findings suggest that while memory is important, already minimal payoff information may suffice for the evolution of direct reciprocity based on social learning. 
They also suggest that the classical model of reciprocity (based on expected payoffs) can often be interpreted as a useful approximation to more realistic models that include cognitive constraints. 



%%%%%%%%%
%% MODEL  %%
%%%%%%%%%


\section{Model and Methods}\label{section:model}

%% MODEL: Basic intro

To explore the impact of limited payoff memory, we adapt existing models of the evolution of direct reciprocity.
These models involve two different time scales. 
The short time scale describes the game dynamics. 
Here, individuals with fixed strategies are randomly matched to interact with each other in repeated social dilemmas. 
The long time scale describes the evolutionary dynamics. 
Here, individuals can update their repeated-game strategies based on the payoffs they yield. 
In the following, we introduce the basic setup of our model; all details and derivations are described in the \esm.\\

%% MODEL: (Repeated) donation game

\noindent
{\bf Description of the game dynamics.} We consider a well-mixed population consisting of $N$~players.
Players are randomly matched in pairs to participate in a repeated donation game~\citep{sigmund2010calculus} with their respective co-player.
Each round, they can either cooperate (\(C\)) or defect (\(D\)). 
A cooperating player provides a benefit~\(b\) to the other player at their own cost~\(c\), with \(0 \!<\! c \!<\! b\). 
A defecting player provides no benefit and pays no cost. 
Thus, the players' payoffs in a single round are given by the matrix
\begin{align}
\bordermatrix{%
	& C & D \cr
C &\ b-c &\ -c\  \cr
D &\ b &\ 0\ \cr
} .
\end{align}
In particular, payoffs take the form of a prisoner's dilemma:
Mutual cooperation yields a better payoff than mutual defection ($b\!-\!c\!>\!0$), but each player individually prefers to defect independent of the co-player's action ($b\!>\!b\!-\!c$ and $0\!>\!-c$). 
To incorporate that individuals interact repeatedly, we assume that after each round, there is a constant continuation probability $\delta$ of interacting for another round. 
For $\delta\!=\!0$, we recover the case of a conventional (one-shot) prisoner's dilemma. 
Here, mutual defection is the only equilibrium. 
As $\delta$ increases, the game turns into a repeated game. Here, additional equilibria emerge, with some of them allowing for full cooperation~\citep{friedman:RES:1971,Akin:chapter:2016,hilbe:GEB:2015,stewart:pnas:2014}. 


%% MODEL: Reactive strategies

In a one-shot donation game, players can only choose among two pure strategies (they can either cooperate or defect).
In the repeated game, strategies become arbitrarily complex. 
Here, strategies are contingent rules, telling players what to do depending on the outcome of all previous rounds. 
For simplicity, in the following we assume individuals use {\it reactive strategies}~\citep{Nowak1992tit}. 
A reactive strategy only depends on the other player's action in the last round. 
Such strategies can be written as a three-dimensional tuple \(\strategy=(y, p,q)\).
The first entry \(y\) is the probability that the player opens with cooperation in the first round. 
The two other entries are the probabilities that the player cooperates in all subsequent rounds, depending on whether the co-player cooperated~($p$) or defected~($q$) in the previous round. 
The set of reactive strategies is simple enough to facilitate an explicit mathematical analysis~\citep{hofbauer1998evolutionary}. 
Yet it is rich enough to capture several important strategies of repeated games. 
For example, it contains \alld{} $=\!(0,0,0)$, the strategy that always defects. 
Similarly, it contains Tit-for-Tat, \tft{} $=\!(1,1,0)$, the strategy that copies the co-player's previous action (and that cooperates in the first round). 
Finally, it contains Generous Tit-for-Tat, \gtft $=\!(1,1,q)$, where $q\!>\!0$ reflects a player's generosity in response to a co-player's defection~\citep{molander:jcr:1985,Nowak1992tit}.  

In the short run, the players' strategies are taken to be fixed.
Players use their strategies to decide whether to cooperate in a series of repeated games against all other population members. 
In the long run, however, the players' strategies may change depending on the payoffs they yield, as we describe in the following.\\
 
 %% MODEL: Pairwise comparison process

\noindent
{\bf Description of the evolutionary dynamics.}
Herein, we assume population members update their strategies based on social learning. 
To model these strategy updates, we consider a pairwise comparison process~\citep{traulsen2007pairwise}. 
This process assumes that at regular time intervals, one population member is randomly selected, and given the chance to revise its strategy.
We refer to this player as the `learner'. 
With probability $\mu$ (reflecting a mutation rate), the learner simply adopts a random strategy (all reactive strategies have the same probability to be chosen). 
With the converse probability $1\!-\!\mu$, the learner randomly picks a `role model' from the population. 
The learner then compares its own payoff $\pi_\learner$ from the repeated game to the role model's payoff $\pi_\rolemodel$. 
The learner adopts the role model's strategy with a probability \(\varphi\) described by a Fermi function~\citep{blume:GEB:1995,szabo:PRE:1998}, 
\begin{equation} \label{Eq:rho}
    \varphi\left(\pi_\learner, \pi_\rolemodel\right) = \frac{1}{1\!+\! e^{- \!\beta\left(\pi_\rolemodel- \pi_\learner\right)}}.
\end{equation}
The selection strength parameter $\beta\!\ge\!0$ indicates how sensitive players are to payoff differences. 
For $\beta\!=\!0$, payoff differences are irrelevant, and the learner simply adopts the role model's strategy with probability one half. As the selection strength~$\beta$ increases, players are increasingly biased to imitate the role model only if it has the higher payoff. 
%In either case, we assume in the following that the role model's strategy can be identified.
%That is, if the learner decides to imitate the role model's strategy, the learner copies it exactly, as in previous works on direct reciprocity. 

%% MODEL: Perfect vs limited payoff memory

We deviate from previous models in how we interpret the payoffs $\pi_\learner$ and $\pi_\rolemodel$, which form the basis of the pairwise comparisons in Eq.~\eqref{Eq:rho}. 
In previous work, these payoffs are taken to be the respective players' expected payoffs. 
We interpret that setup as a model with perfect payoff memory. 
There, the payoffs  $\pi_\learner$ and $\pi_\rolemodel$ represent an average over all possible repeated games the two individuals have played with all population members (\FigBaseResults, upper left panel). 
The use of expected payoffs is mathematically convenient, because explicit formulas for these payoffs are available~\citep{hofbauer1998evolutionary}.
Herein, we compare this model of perfect payoff memory to a model with limited payoff memory. 
In that model, the players' payoffs $\pi_\learner$ and $\pi_\rolemodel$ are taken to be the payoffs that each player received in their very last round prior to making social comparisons. 
That is, players only consider the very last repeated game they participated in, and there they only take into account the outcome of the very last round (\FigBaseResults, lower left panel). 
This assumption could reflect, for example, a strong recency bias in how individuals evaluate payoffs.  
In addition to this extreme case of limited payoff memory, later on we also explore cases in which players take into account the outcome of two, three, four, or more recent rounds. 

%% MODEL: Summary and rare mutation assumption. 

Both in the case of perfect and limited memory, we iterate the elementary strategy updating step described above for many time steps. 
This gives rise to a stochastic process that describes which strategies players adopt over time. 
We explore the dynamics of this process mathematically and with computer simulations.
For the results presented in the following, we assume that mutations are rare (\(\mu\!\rightarrow\! 0\)). 
This assumption is fairly common in evolutionary game theory, because it makes some computations more efficient~\citep{fudenberg:JET:2006,wu:JMB:2012,mcavoy:jet:2015}, and because the results can be interpreted more easily.  
However, in Section~3 of the \esm{} we show that our main results continue to hold for strictly positive mutation rates.



%%%%%%%%%%
%% RESULTS  %%
%%%%%%%%%%

\section{Results}

%% 	RESULTS: Setup to get analytical results

\noindent
{\bf Stability of cooperative populations.}
To get some intuition for the differences between perfect and limited payoff memory, we first analyze when cooperation is stable in either scenario.
To this end, we consider a resident population in which all players but one adopt a strategy of Generous Tit-for-Tat, \gtft{} $=\!(1,1,q)$. 
The remaining mutant player adopts \alld. 
We say {\it cooperation is stochastically stable} if the single mutant is more likely to imitate the residents than vice versa. 
For simplicity, we consider a large population~($N\!\rightarrow\!\infty$) and strong selection~($\beta\!\rightarrow\!\infty$).
More general results are derived in the \esm. 

%% RESULTS: Analytical results for perfect memory

In the case of perfect payoff memory, it is straightforward to characterize when cooperation is stochastically stable. 
Here, we simply need to compute the players' expected payoffs. 
Because the population mostly consists of residents, and because residents mutually cooperate with each other, their expected payoff is $\pi_\gtft = b\!-\!c$. 
On the other hand, the defecting mutant only interacts with residents. 
Given the residents' strategy, the mutant receives a benefit in the first round, and in every subsequent round with probability $q$. 
As a result, the mutant's expected payoff is $\pi_\alld \!=\! (1\!-\!\delta\!+\!\delta q)b$. 
For perfect payoff memory, the requirement for cooperation to be stochastically stable reduces to the condition $\pi_\gtft > \pi_\alld$. 
This yields
\begin{equation} \label{Eq:PerfectMemory}
q < 1\!-\!\frac{c}{\delta  b}.
\end{equation}
In particular, we recover the previous observation that $q\!=\!1-c/(\delta b)$ is the maximum generosity that cooperators should have~\citep{molander:jcr:1985,Nowak1992tit,Schmid:NHB:2021}. 
Because $q\!\ge\!0$, we also conclude that cooperation can only be stable if $\delta \!>\! c/b$.
Again, this condition for the feasibility of direct reciprocity is the condition found in the literature~\citep{nowak:Science:2006}.

%% RESULTS: Analytical results for limited memory

The logic of the case with limited payoff memory is somewhat different. 
Here we need to compute how likely each player obtains one of the four possible payoffs $\{b\!-\!c, -c, b, 0\}$ in the very last round of a game, before they make social comparisons.
Because residents almost always interact with other residents, their last one-shot payoff is $\pi_\gtft = b\!-\!c$ almost surely. 
For the defecting mutant, there are two possibilities. 
({\it i})~ If the mutant's co-player happens to cooperate in the last round, the mutant receives $\pi_\alld\!=\!b$.
This case occurs with probability $1\!-\!\delta\!+\!\delta q$. 
({\it ii})~If the co-player defects in the last round, the mutant receives $\pi_\alld=\!0$. 
This occurs with the converse probability $\delta(1\!-\!q)$.
Because $b\!-\!c\!<\!b$, residents tend to imitate the mutant in the first case. 
Because $b\!-\!c\!>\!0$, mutants tend to imitate the resident in the second case. 
Cooperation is stochastically stable if the first case is less likely than the second. 
This yields the condition
\begin{equation} \label{Eq:LimitedMemory}
q < 1\!-\!\frac{1}{2 \delta}.
\end{equation}
Interestingly, this condition no longer depends on the exact payoff values $c$ and $b$. 
This independence arises because of our assumption of strong selection, in which case only the payoff ordering $b\!>\!c\!>\!0$ matters. Because $q$ is non-negative, condition~\eqref{Eq:LimitedMemory} can only be satisfied if $\delta\!>\!1/2$. That is, players need to interact in more than two rounds in expectation. 

%% RESULTS: Summary of analytical results

By comparing the two cases, we find that payoff memory affects whether a conditionally cooperative strategy ($1,1,q$) is viable. 
With perfect memory, the maximum generosity $q$ needs to satisfy Eq.~\eqref{Eq:PerfectMemory}.
In particular, this generosity can become arbitrarily large, provided the game's benefit-to-cost ratio~$b/c$ and the continuation probability~$\delta$ are sufficiently large. 
In contrast, with limited payoff memory, the maximum generosity is bounded by one half, and it is independent of the benefit-to-cost ratio.\\


%% RESULTS: Evolving strategies

\noindent
{\bf Evolutionary dynamics of reciprocity.}
To explore whether the previous static observations describe the dynamics of evolving populations, we turn to simulations.
We have run separate simulations for perfect and limited payoff memory. 
In each case, we consider both a low and a high benefit of cooperation ($b/c\!=\!3$ and $b/c\!=\!10$, respectively).  
For each simulation, we record which strategies ($y,p,q$) the players adopt over time.
\FigBaseResults{} depicts the conditional cooperation probabilities $p$ and $q$ (we omit the opening move \(y\) because we use a discount factor~\(\delta\) close to one, such that first-round behavior is largely irrelevant). 
In all simulations, we find that the players' strategies cluster in two regions of the strategy space. 
The first region corresponds to a neighborhood of \alld{} with $(p,q)\!\approx\!(0,0)$.
The second region corresponds to a thin strip of cooperative strategies with $(p,q)\!\approx\!(1,q)$. 
Within this strip, we observe that most strategies satisfy the constraints on $q$ imposed by the inequalities~\eqref{Eq:PerfectMemory} and~\eqref{Eq:LimitedMemory}. 
That is, with perfect memory, most evolving strategies have $q\!<\!1\!-\!c/b$, whereas with limited payoff memory, most strategies have $q\!<\!1/2$. 
In particular, for limited payoff memory, changes in the benefit parameter have no effect on the qualitative distribution of strategies. 

%% RESULTS: Evolving cooperation rates

In each case, the evolutionary dynamics follow a similar cyclic pattern~\citep[as described in Refs.][]{imhof2010stochastic, Nowak1992tit}:
Resident populations of defectors are most likely invaded by strategies close to \tft. 
Once the population adopts conditionally cooperative strategies  $(1,1,q)$, neutral drift may introduce larger values of generosity~$q$. 
If the resident's generosity~$q$ violates the conditions~\eqref{Eq:PerfectMemory} and~\eqref{Eq:LimitedMemory}, defectors can re-invade, and the cycle starts again. 
The relative time spent near \alld{} and near the strip of conditionally cooperative strategies depends on the considered memory setting (\FigInvasionAnalysis, depicting the case of high benefits). 
For perfect memory, we find that \alld{} is replaced relatively quickly by more cooperative strategies. 
Here, it takes on average 159 invading mutants until \alld{} is successfully replaced. 
In contrast, for limited memory, \alld{} is more robust, resisting on average 798 mutant strategies. 
This picture reverses when we consider an initial population that adopts \gtft. 
Such populations are much more robust under perfect memory than they are under limited memory. 
Overall, we find that the impact of memory on the population's average cooperation rate is substantial. 
For perfect memory, this rate is 52\% for low benefits, and 98\% for high benefits. 
For limited payoff memory, the evolving cooperation rates are smaller but still strictly positive, with 37\% cooperation for low benefits and 51\% cooperation for high benefits (\FigBaseResults). 

%% RESULTS: Evolving cooperation rates

To further investigate the influence of different parameters, we have systematically varied the benefit~$b$ and the selection strength~$\beta$ in~\FigDependenceParameters.
According to \FigDependenceParameters{\it a}, perfect memory consistently results in a higher cooperation rate, and this relative advantage further increases with an increasing benefit~$b$. 
Interestingly, for limited payoff memory, the cooperation rate remains stable at approximately 50\% once \(b \!\ge\! 5\).
This again reflects our earlier observation that the feasibility of cooperation in this scenario is largely independent of the exact values of $b$ and $c$, as described by Eq.~\eqref{Eq:LimitedMemory}. 
With respect to the effect of different selection strengths, \FigDependenceParameters{\it b} suggests that both perfect and limited payoff memory yields similar cooperation rates for weak selection \(\beta \!<\! 1\). 
Beyond weak selection, increasing selection has a positive effect under perfect payoff memory, but a negative effect under limited payoff memory.\\

%% RESULTS: Memory-1 strategies %%

\noindent
{\bf Beyond reactive strategies.} While the results presented in the main text focus on reactive strategies, the patterns we observe do not seem to depend on the considered strategy space. 
To illustrate this point in more detail, in the \esm{} we consider the dynamics among memory-1 strategies. 
Here, players take into account both their co-player's and their own last move, see Refs.~\citep{nowak:Nature:1993,imhof:JTB:2007}. 
Also in that case, we observe that perfect memory leads to systematically higher cooperation rates (Supplementary Figures~5,6).
Again, this advantage of perfect memory is particularly pronounced for strong selection, or when there is a high benefit of cooperation (\FigMemoryOneParameters).\\ 

%% RESULTS: Setup of higher-memory simulations

\noindent 
{\bf The effect of increasing individual payoff memory.}
So far, we have taken a rather extreme interpretation of limited payoff memory. 
In the respective scenario, we assumed that individuals update their strategies based on their experience in a single round of the prisoner's dilemma, against a single co-player. 
The limited payoff memory framework can be expanded in various ways. 
In particular, individuals may recall a larger number of rounds, they may recall their interactions with several co-players, or both. 
To gain further insights on the impact of payoff memory, we explore four additional scenarios. 
In the first scenario, players recall the payoffs they obtained in the last two rounds against a single co-player. 
In the second scenario, players recall their last-round payoffs against two co-players. 
In the third scenario, they recall the two last rounds against two co-players. 
Finally, in the last scenario, players update based on the average payoff they
receive over all rounds with a single co-player (further extensions are possible, but we do not explore them here).

%% RESULTS: Analytical results for higher memory

For most scenarios, we can again derive an analytical condition for when cooperation is stochastically stable. 
As before, we assume populations are large and that selection is strong. 
For simplicity, we also assume that the game continues almost certainly after each round (i.e., $\delta$ approaches one). 
The details of this analysis can be found in the \esm. 
In the first two scenarios, we interestingly find that for $b\!>\!2c$, cooperation is stochastically stable when $q\!<\! \frac{\sqrt{2}}{2}\approx 0.707$. 
Comparing this condition with the more stringent condition in Eq.~\eqref{Eq:LimitedMemory} suggests that there are now more conditionally cooperative strategies that can sustain cooperation. 
Hence, cooperation should evolve more easily.
In the last scenario, we find that cooperation is stochastically stable when \(q
< 1 - \frac{c}{b}\), which is the same condition as in
Eq.~\eqref{Eq:PerfectMemory}, even though only a single co-player is considered
instead of the whole population. 

%% RESULTS: Simulations for higher memory

We complement these analytical results with additional simulations, see \FigHigherMemory.
We observe that a minimal increase in the players' payoff memory (compared to the baseline case with a single round recalled) can promote cooperation considerably. 
Specifically, in all four scenarios with extended memory, we see similar cooperation rates, and they approach the rates observed under perfect memory. 
These results suggest that while it takes {\it some} payoff memory to sustain substantial cooperation rates, the requirements on memory seem to be rather modest. 
Already remembering a few interactions, either with the same co-player or across different co-players, may provide players with enough information to adopt reciprocal strategies. 



%%%%%%%%%%%%
%%  DISCUSSION %%
%%%%%%%%%%%%

\section{Discussion}\label{section:discussion}
%
In economics, if payoff is measured in terms of money and a decision is to be made at time $t$ in the future, then currency accumulated early on is weighted more in that decision because it has more time to accumulate interest. Such a model discounts the future relative to the past. It also does not necessarily require ``memory'' because rewards are accumulated into a factor used in decision-making; the specific time stamps of rewards do not themselves provide better information beyond their effects on total payoff. In a similar fashion, the probabilistic interpretation of discounting as a continuation probability \citep{axelrod1981evolution} also effectively discounts the future relative to the past. A foraging animal deciding between two behaviors might tend to choose the one that yields a moderate reward sooner relative to a larger reward later, since earlier rewards (e.g., food) contribute to immediate survival, and there is no guarantee that later rewards will happen at all \citep{stephens1986foraging}.

Within the context of a single repeated game, the model we consider here is, in some ways, dual to the classical model of temporally-discounted rewards in repeated games. Instead of making decisions based on expected rewards in the future, we consider individuals who make decisions based on actual rewards in the past. Thus, the ability to estimate the future payoff of a strategy is replaced by the memory of how this strategy previously fared against others. This involves two time scales: interaction partners and rounds within those interactions. As a result, we are dealing with a model that discounts the past rather than the future. Intriguingly, treating payoffs in this manner is reminiscent of the reward-smoothing technique of ``eligibility traces'' in reinforcement learning \citep{sutton:MIT:2018}, which uses past rewards (discounted appropriately) to shape present perceived payoff. There is a sound basis for this method in neuroscience, where rewards and (temporal-difference) learning are associated to dopaminergic neurons \citep{schultz:JN:1998} and spike-timing-dependent plasticity \citep{dan:Neuron:2004}. This suggests a more biologically-encoded interpretation of memory, which is equally applicable to models of direct reciprocity where rewards have a neurological basis.

Of course, the precise nature of ``memory'' also depends on what payoffs in a game represent, which should be taken into account when applying game-theoretic models. For example, a payoff stream of monetary currency might truly accumulate and not require memory on the parts of agents. Even in the context of money, however, not all of what was obtained in the past is necessarily available at the time a decision is made, which brings memory into play. The serial position effect in human psychology shows that in an ordered list of items (e.g., words), humans tend to have difficulty remembering the entirety of sequences, demonstrating moderate recall for those items coming earlier (primacy effect), substantial recall for those coming later (recency effect), and lower recall for those in between \citep{murdock:JEP:1962}. It is therefore reasonable that when presented with a stream of payoffs, whether on the timescale of pairing for repeated interactions or in a stream of one-shot games, players might be able to effectively incorporate only the most recent payoffs.

In fact, even beyond specific psychological considerations, a curious interpretation of payoffs arises from the formula commonly used for expected payoffs in repeated games. If $\delta\in\left[0,1\right)$ is the probability of continuing to another round in the game, then the expected payoff to an agent is $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$, where $u_{t}$ is the reward the agent receives in the stage game at time $t$. Here, additional stochasticity arises due to uncertainty in the game length, and an agent might not be able to compute his or her expected payoff for use in decision-making. The probability that the game terminates after the interaction at time $T$ is $\delta^{T}\left(1-\delta\right)$, in which case $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$ is exactly the expected payoff the agent receives at time $T$, i.e. \emph{in the last round}. As an unbiased estimator of this expectation, the agent might thus use $u_{T}$ as a proxy for ``success'' when evaluating his or her behavior. This gives a purely model-driven justification for why considering payoff in the last round of the game can result in more realistic extensions of traditional models.

We note that the expected \emph{total} payoff in the game, $u_{0}+u_{1}+\cdots +u_{T}$, is given by $\sum_{t=0}^{\infty}\delta^{t}u_{t}$. This version of ``expected payoff'' appears less common in the literature on direct reciprocity than its normalization, $\left(1-\delta\right)\sum_{t=0}^{\infty}\delta^{t}u_{t}$, likely owing to the fact that payoffs can grow arbitrarily large with sufficiently long time horizons ($\delta\rightarrow 1^{-}$). Non-normalized payoffs interfere with selection intensity ($\beta$) in models of social imitation, which is (presumably) why they appear less frequently in the literature. On that point, we note that many of the differences between realized and expected payoffs disappear in the limit of weak selection \citep{mcavoy:NHB:2020}. Non-weak selection can introduce substantial differences between models with realized and expected payoffs \citep{mcavoy:PLOSCB:2021}, which is especially important to understand in models of social systems with cultural transmission \citep{cavalli1981cultural}.

Our main contribution is an application of these ideas to direct reciprocity, which is one of the key mechanisms to explain why unrelated individuals might cooperate~\citep{nowak:Science:2006}. 
According to this mechanism, cooperation pays if it makes the interaction partner more cooperative in future. 
To describe which strategies are most effective, the previous theoretical literature assumes that the evolutionary dynamics are driven by the players' expected payoffs~\citep{brauchli:JTB:1999,brandt:JTB:2006,ohtsuki:JTB:2007b,szolnoki:pre:2009b,imhof2010stochastic,van-segbroeck:prl:2012,grujic:jtb:2012,Martinez2012,stewart:pnas:2013,pinheiro:PLoSCB:2014,stewart:games:2015,Baek2016,McAvoy:ProcA:2019,glynatsi:SCR:2020,Schmid:PlosCB:2022,Murase:SciRep:2022}.
To the extent that strategies are learned (not inherited), this assumption seems to impose rather stringent requirements on the individuals' cognitive abilities. 
In the most extreme case, this would require individuals to remember (or compute) their payoffs against all population members, for all possible ways in which their repeated games may unfold. 
This assumption introduces a curious inconsistency in how these models represent an individual's cognitive abilities. 
For playing their games, individuals are often assumed to only recall the outcome of the very last round. 
Yet to update their strategies, individuals are implicitly assumed to have a record of the outcome of all rounds, across all interaction partners. 

It is natural to ask, then, to what extent perfect payoff memory is in fact required for the evolution of reciprocity. 
To this end, we consider a model in which individuals only remember the payoff of their very last interaction, or the payoffs of the last few interactions. 
By only considering an individual's most recent experiences, the evolutionary process is subject to additional stochasticity. 
Strategies that perform well on average (across an entire repeated game and across many interaction partners) may still get replaced if the respective player happened to yield an inferior payoff in the very last round. 
A similar element of stochasticity has been previously explored in the context of one-shot (non-repeated) games~\citep{sanchez:JTB:2005,roca:PhysicalReview:2006,Traulsen:JTB:2007,Woelfing:JTB:2009,Hauert:PRE:2018}. 
This literature studies which strategies are selected for when individuals only interact with a finite sample of population members. 
In the respective models, individuals can only choose among two strategies. 
They can either cooperate or defect, and stochastic sampling affects which of these two strategies is favored. 
Instead, in repeated games, players have access to a large set of strategies~\citep[in our case, all reactive strategies;][]{nowak:APC:1989}. 
Here, stochastic sampling does not only affect whether cooperative or non-cooperative strategies are favored; it also affects {\it which} conditionally cooperative strategies are favored. 

To address these questions, we combine analytical methods and computer simulations. 
In the most extreme case, we consider individuals who update their strategies based on only one piece of information:  the last round of a single repeated game. 
For that case, we find that individuals are less generous, and they tend to be less cooperative overall~(\FigBaseResults). 
However, once individuals update their strategies based on two or more recent experiences, overall cooperation rates quickly approach the levels observed under perfect payoff memory~(\FigHigherMemory). 
These findings suggest that models based on expected payoffs can serve as a useful approximation to more realistic models with limited payoff memory. 
Our findings also contribute to a wider literature that explores which kinds of cognitive capacities are required for reciprocal altruism to be feasible~\citep[e.g.,][]{Stevens:fip:2011,Volstorf:PlosOne:2011}. 
While more payoff memory is always favorable, reciprocal cooperation can already be sustained if individuals have a record of two or three past outcomes. We believe that this kind of result, derived entirely within a theoretical model, is crucial for making model-informed deductions about reciprocity in natural systems.\\[0.5cm]

%%%%%%%%%%%
%% END NOTES %%
%%%%%%%%%%%

\noindent
{\bf Ethics.}
This work is purely theoretical and did not require ethical approval from a human subject or animal welfare committee.\\

\noindent
{\bf Data accessibility.}
All data and code used in this manuscript are openly available at\newline
\texttt{https://zenodo.org/records/10066227}.\\

\noindent
{\bf Declaration of AI use.}
We have not used AI-assisted technologies in creating this article.\\

\noindent
{\bf Authors' contributions.}
N.G.: conceptualization, formal analysis, investigation, methodology, visualization, writing - original draft, writing -- review and editing; 
A.M.: conceptualization, formal analysis, methodology, writing -- review and editing; 
C.H.: conceptualization, formal analysis, methodology, writing -- review and editing.\\

\noindent
{\bf Conflict of interest declaration.}
We declare we have no competing interests.\\

\noindent
{\bf Funding.}
N.G. and C.H. acknowledge generous support by the European Research Council Starting Grant 850529:
E-DIRECT, and by the Max Planck Society.\\



%%%%%%%%%%%%
%% REFERENCES %%
%%%%%%%%%%%%

{
{\setlength{\bibsep}{0\baselineskip}
\bibliographystyle{naturemag}
\bibliography{bibliography}
}



%%%%%%%%%%
%% FIGURES  %%
%%%%%%%%%%

\clearpage
\newpage

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{static/donation_expected_last_round_summary_results.pdf}
    \caption{{\bf Evolutionary dynamics under perfect and limited payoff memory.}
    The leftmost panels give a schematic overview of the two main scenarios we compare. 
    The two scenarios differ in how many past interactions individuals take into account when updating their strategy. 
    In the scenario with perfect payoff memory, individuals consider all their past interactions (against all population members, and taking every turn of each repeated game into account). 
    In the scenario with limited payoff memory, individuals only consider their very last interaction (against one specific population member, taking into account only one round of the repeated game). 
    The four panels  on the right side depict the outcome of evolutionary simulations for repeated games with either a low or a high benefit of cooperation. 
    Colors represent how often the respective region of the strategy space is visited over time. 
    In all four panels, two regions are visited particularly often. 
    One region corresponds to a neighborhood of \alld{} with $p\approx q\!\approx\!0$ (lower left corner). 
    The other region corresponds to a strip of conditionally cooperative strategies with $p\!\approx\! 1$ and $q$ satisfying the constraints \eqref{Eq:PerfectMemory} and \eqref{Eq:LimitedMemory}, respectively (lower right corner). 
    The resulting average cooperation rate depends on which of these two neighborhoods is visited more often. 
    Simulations are run for $T\!=\!10^7$ time steps, using a cost $c\!=\!1$, a continuation probability of $\delta\!=\!0.999$ and a selection strength of $\beta\!=\!1$, in a population of size $N\!=\!100$.}
\label{fig:expected_and_stochastic_for_donation}
\end{figure}


\clearpage
\newpage


\begin{figure}[t]
    \centering
    \includegraphics[width=.75\textwidth]{static/cooperation_rate_over_b_and_beta.pdf}
    \caption{{\bf Evolution of direct reciprocity for different parameter values.} 
    To explore the robustness of our results, we have run simulations for different benefit values (left panels, {\it a}), and for different selection strengths (right panels, {\it b}). 
    In each case, we record the resulting average cooperation rate over the entire simulation (upper panels). 
    In addition we record the individuals' average generosity. 
    Here, we only take into account those residents with $p\! \approx\! 1$ and we compute
    the average of their cooperation probability~$q$. 
    These simulations suggest that perfect payoff memory consistently leads to more cooperation and more generosity. 
    Unless explicitly varied, the parameters of the simulation are $N\!=\!100$, $b\!=\!3$, $c\!=\!1$, $\beta\!=\!1$, $\delta\!=\!0.99$. 
    Simulations are run for $T\!=\!5\times 10^7$ time steps for each parameter combination.}
    \label{fig:cooperation_rate_over_benefit_and_beta}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{static/more_memory_summary_results.pdf}
  \caption{{\bf Average cooperation rates for different payoff memories.}
  We vary how much information individuals take into account when updating their strategies. 
  From left to right, we consider the following cases.
  ({\it i}) Updating occurs based on expected payoffs (perfect memory), 
  ({\it ii}) it occurs based on the last round of one interaction (limited memory), 
  ({\it iii}) based on the last round of two interactions,
  ({\it iv}) based on the last two rounds of one interaction,
  ({\it v}) based on the last two rounds of two interactions, and
  ({\it vi}) based on the average payoff of one interaction.
  Again, simulations are run either for a comparably low benefit of cooperation~($b/c\!=\!3$), or for a high benefit~($b/c\!=\!10$). 
  We observe that perfect memory always yields the highest cooperation rate. 
  However, when individuals take into account at least two past interactions -- cases ({\it iii}) to ({\it vi}) -- evolving cooperation rates are close to this optimum. 
  Baseline parameters are the same as in \FigDependenceParameters.}
\label{fig:cooperation_rate_all_updating_payoffs}
\end{figure}

\end{document}